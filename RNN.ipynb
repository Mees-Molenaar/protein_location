{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN in Numpy\n",
    "\n",
    "Recurrent Neural Networks (RNN) are a class of Artificial Neural Networks that is extremely well equiped to process a sequence of inputs. Therefore, RNNs are usefull in time series. \n",
    "\n",
    "In this work, I created a RNN from scratch using Numpy. This RNN is based on Andrej Karpathy's char-rnn and will be the basis for a LSTM network. This network will be used to classify protein locations based on the amino acid sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import numpy and matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the training data and save some important variables. If you want to train on your own text, just change the .txt file in the data variable.\n",
    "#data = open('shakespeare.txt', 'r').read()\n",
    "data = open('nescio.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size = len(data)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set is length 201884\n",
      "Vocab set is length 88\n"
     ]
    }
   ],
   "source": [
    "print(f'Data set is length {data_size}')\n",
    "print(f'Vocab set is length {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple character embedding\n",
    "char_to_idx = {char:i for i, char in enumerate(chars)}\n",
    "idx_to_char = {i:char for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    r\"\"\" Simple recurrent neural network (RNN) class for an input sequence.\n",
    "    \n",
    "        This RNN initializes weight and gradients. And contains the forward\n",
    "        and backward pass. The network is optimized using Adagrad.\n",
    "        The train method is used to train the network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        seq_length : Number of layers connected to each others. \n",
    "        hidden_sz : The number of features in the hidden state h.\n",
    "        vocab_sz : The number of possible inputs and outputs.\n",
    "        \n",
    "        \n",
    "        Inputs (train)\n",
    "        --------------\n",
    "        data : Data used to train the network.\n",
    "        optimizer : The optimizer that is used to train the network.\n",
    "        lr : The learning rate used to train the network.\n",
    "        epochs : The number of epochs to train the network.\n",
    "        progress : If True, shows the progress of training the network.\n",
    "        \n",
    "        Inputs (predict)\n",
    "        ----------------\n",
    "        start : Start of a sentence that the network uses as initial sequence.\n",
    "        n : Length of the prediction.\n",
    "        \n",
    "        \n",
    "        Output (train)\n",
    "        --------------\n",
    "        smooth_loss : The loss of the current trained network.\n",
    "        Wxh, Whh, Why : Updated weights of the network due to training.\n",
    "        bh, by : Updated biases due to training.\n",
    "        \n",
    "        Output (predict)\n",
    "        ----------------\n",
    "        txt : A string that is predicted by the RNN.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seq_length, hidden_sz, vocab_sz):\n",
    "        self.hs = {} # Hidden states\n",
    "        self.sm_ps = {} # Softmax probabilities\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.vocab_sz = vocab_sz\n",
    "        \n",
    "        # Start with zero loss\n",
    "        self.loss = 0 \n",
    "        \n",
    "        # Initiate weight matrices\n",
    "        self.Wxh, self.Whh, self.Why, self.bh, self.by = self.init_weights()\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes weights and biases based on the inputs hidden_sz and vocab_sz\n",
    "        \"\"\"\n",
    "        Wxh = np.random.randn(self.hidden_sz, self.vocab_sz) * 0.01 #times 0.01 to avoid exploding gradients\n",
    "        Whh = np.random.randn(self.hidden_sz, self.hidden_sz) * 0.01\n",
    "        Why = np.random.randn(self.vocab_sz, self.hidden_sz) * 0.01\n",
    "        \n",
    "        # bias\n",
    "        bh = np.zeros((hidden_size, 1))\n",
    "        by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        return Wxh, Whh, Why, bh, by\n",
    "    \n",
    "    def init_gradients(self):\n",
    "        \"\"\"\n",
    "        Initializes gradients for biases and weights.\n",
    "        \"\"\"\n",
    "        self.dWxh, self.dWhh, self.dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        self.dby, self.dbh = np.zeros_like(self.by), np.zeros_like(self.bh)\n",
    "    \n",
    "    def forward(self, xs, targets):\n",
    "        \"\"\"\n",
    "        Forward pass of the RNN\n",
    "        \"\"\"\n",
    "        \n",
    "        y_preds = {}\n",
    "\n",
    "        self.loss = 0\n",
    "\n",
    "        for i in range(len(xs)):\n",
    "            x = xs[i]\n",
    "            x_vec = np.zeros((self.vocab_sz, 1)) # vectorize the input\n",
    "            x_vec[x] = 1\n",
    "\n",
    "            # Calculate the new hidden, which is based on the input and the previous hidden layer\n",
    "            self.hs[i] = np.tanh(np.dot(self.Wxh, x_vec) + np.dot(self.Whh, self.hs[i - 1]) + self.bh)\n",
    "            # Predict y\n",
    "            y_preds[i] = np.dot(self.Why, self.hs[i]) + self.by\n",
    "\n",
    "            self.sm_ps[i] = np.exp(y_preds[i]) / np.sum(np.exp(y_preds[i])) # Softmax probabilty\n",
    "            self.loss += -np.log(self.sm_ps[i][targets[i], 0]) #Negative loss likelyhood\n",
    "\n",
    "        self.hs[-1] = self.hs[len(xs) - 1]\n",
    "        \n",
    "    def backward(self, xs, targets):\n",
    "        \"\"\"\n",
    "        Backward pass of the RNN\n",
    "        \"\"\"\n",
    "        self.init_gradients()\n",
    "    \n",
    "        # Initialize empty next hidden layer for the first backprop\n",
    "        dhnext = np.zeros_like(self.hs[0])\n",
    "\n",
    "        for i in reversed(range(len(xs))):\n",
    "            # X to vector\n",
    "            x = xs[i]    \n",
    "            x_vec = np.zeros((vocab_size, 1))\n",
    "            x_vec[x] = 1\n",
    "\n",
    "            dy = np.copy(self.sm_ps[i])\n",
    "            dy[targets[i]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "\n",
    "            self.dby += dy   \n",
    "            self.dWhy += np.dot(dy, self.hs[i].T)\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext\n",
    "            dhraw = (1 - self.hs[i] * self.hs[i]) * dh  \n",
    "            self.dWxh += np.dot(dhraw, x_vec.T)\n",
    "            self.dWhh += np.dot(dhraw, self.hs[i-1].T)\n",
    "            self.dbh += dhraw\n",
    "            dhnext = np.dot(self.Whh.T, dhraw)\n",
    "\n",
    "        # Clip to prevent exploding gradients\n",
    "        for dparam in [self.dWhy, self.dWxh, self.dWhh, self.dbh, self.dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "            \n",
    "    def init_adagrad_memory(self):\n",
    "        \"\"\"\n",
    "        Initialize memory matrices needed for Adagrad.\n",
    "        \"\"\"\n",
    "        self.mWxh, self.mWhh, self.mWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        self.mbh, self.mby  = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "\n",
    "    def update_gradients(self, optimizer, lr):\n",
    "        \"\"\"\n",
    "        Update gradients based on the optimizer you have chosen.\n",
    "        \"\"\"\n",
    "        if optimizer == 'Adagrad':\n",
    "            if not hasattr(self, 'mWhh'):\n",
    "                self.init_adagrad_memory()\n",
    "                \n",
    "            # perform parameter update with Adagrad\n",
    "            for param, dparam, mem in zip([self.Wxh, self.Whh, self.Why, self.bh, self.by],\n",
    "                                  [self.dWxh, self.dWhh, self.dWhy, self.dbh, self.dby],\n",
    "                                  [self.mWxh, self.mWhh, self.mWhy, self.mbh, self.mby]):\n",
    "                mem += dparam * dparam\n",
    "                param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update\n",
    "                \n",
    "    def reset_hidden(self):\n",
    "        \"\"\"\n",
    "        Reset the hidden layer\n",
    "        \"\"\"\n",
    "        self.hs[-1] = np.zeros((self.hidden_sz, 1))\n",
    "        \n",
    "    def plot_losses(self):\n",
    "        \"\"\"\n",
    "        Plot the cross entropy loss against the number of sequences\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'losses'):\n",
    "            plt.plot(self.losses)\n",
    "            plt.xlabel('Number of sequences')\n",
    "            plt.ylabel('Cross entropy loss')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('Error: No losses recorded, train the model!')\n",
    "    \n",
    "    def train(self, data, optimizer, lr, epochs, progress=True):\n",
    "        \"\"\"\n",
    "        Train the model by chopping the data in sequences followed by performing\n",
    "        the forward pass, backward pass and update the gradients.\n",
    "        \"\"\"\n",
    "        self.losses = []\n",
    "        smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "        \n",
    "        # Loop over the amount of epochs\n",
    "        for epoch in range(epochs):\n",
    "            n = 0\n",
    "            \n",
    "            # Reset hidden state\n",
    "            self.reset_hidden()\n",
    "            \n",
    "            data_len = len(data)\n",
    "            \n",
    "            # Loop over the amount of sequences\n",
    "            sequences_amount = int(data_len // self.seq_length)\n",
    "            for j in range(sequences_amount):\n",
    "                \n",
    "                start_pos = self.seq_length * j\n",
    "                \n",
    "                # Embed the inputs and targets\n",
    "                xs = [char_to_idx[ch] for ch in data[start_pos:start_pos+self.seq_length]]\n",
    "                targets = [char_to_idx[ch] for ch in data[start_pos+1:start_pos+self.seq_length+1]]\n",
    "                \n",
    "                # Forward pass\n",
    "                self.forward(xs, targets)\n",
    "                \n",
    "                # Backward\n",
    "                self.backward(xs, targets)\n",
    "                \n",
    "                # Update weight matrices\n",
    "                self.update_gradients(optimizer, lr)\n",
    "                \n",
    "                smooth_loss = smooth_loss * 0.999 + self.loss * 0.001\n",
    "        \n",
    "                if progress and n % 1000 == 0:\n",
    "                    print(f'Epoch {epoch + 1}: {n} / {sequences_amount}: {smooth_loss}')\n",
    "                 \n",
    "                n += 1\n",
    "                self.losses.append(smooth_loss)\n",
    "    \n",
    "    def predict(self, start, n):\n",
    "        \"\"\"\n",
    "        Predict a sequence of text based on a starting string.\n",
    "        \"\"\"\n",
    "        seed_idx = char_to_idx[start[-1]]\n",
    "        x = np.zeros((self.vocab_sz, 1))\n",
    "        x[seed_idx] = 1\n",
    "        \n",
    "        txt = [ch for ch in start]\n",
    "        \n",
    "        idxes = []\n",
    "        \n",
    "        h = self.hs[-1]\n",
    "        \n",
    "        for i in range(n):\n",
    "            \n",
    "            # Calculate the hidden\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            # Calculate y\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "\n",
    "            sm_p = np.exp(y) / np.sum(np.exp(y)) # Softmax probabilty\n",
    "            # Determine character based on weighted probability (is using the softmax probability)\n",
    "            idx = np.random.choice(range(self.vocab_sz), p=sm_p.ravel())\n",
    "            idxes.append(idx)\n",
    "            \n",
    "            # Save X for next iteration\n",
    "            x = np.zeros((self.vocab_sz, 1))\n",
    "            x[idx] = 1\n",
    "            \n",
    "        prediction = [idx_to_char[idx] for idx in idxes]\n",
    "        \n",
    "        txt += prediction\n",
    "        \n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(seq_length, hidden_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0 / 8075: 111.93341315285379\n",
      "Epoch 1: 1000 / 8075: 86.75651624582633\n",
      "Epoch 1: 2000 / 8075: 69.6077455146045\n",
      "Epoch 1: 3000 / 8075: 61.320757951908575\n",
      "Epoch 1: 4000 / 8075: 58.17413484365942\n",
      "Epoch 1: 5000 / 8075: 56.88212065980858\n",
      "Epoch 1: 6000 / 8075: 55.36565541771713\n",
      "Epoch 1: 7000 / 8075: 53.875082177928824\n",
      "Epoch 1: 8000 / 8075: 53.368742731385225\n",
      "Epoch 2: 0 / 8075: 53.3489118736496\n",
      "Epoch 2: 1000 / 8075: 52.34603342673742\n",
      "Epoch 2: 2000 / 8075: 51.785163622937\n",
      "Epoch 2: 3000 / 8075: 51.128693548035464\n",
      "Epoch 2: 4000 / 8075: 51.260922841776654\n",
      "Epoch 2: 5000 / 8075: 51.411198284075766\n",
      "Epoch 2: 6000 / 8075: 50.929239689650636\n",
      "Epoch 2: 7000 / 8075: 50.08727453410787\n",
      "Epoch 2: 8000 / 8075: 50.11005786829514\n",
      "Epoch 3: 0 / 8075: 50.120579756652276\n",
      "Epoch 3: 1000 / 8075: 49.55466001319277\n",
      "Epoch 3: 2000 / 8075: 49.4425699467285\n",
      "Epoch 3: 3000 / 8075: 49.04565394720058\n",
      "Epoch 3: 4000 / 8075: 49.13694455244156\n",
      "Epoch 3: 5000 / 8075: 49.30521542272238\n",
      "Epoch 3: 6000 / 8075: 49.032354184019006\n",
      "Epoch 3: 7000 / 8075: 48.28571780191391\n",
      "Epoch 3: 8000 / 8075: 48.48716225125693\n",
      "Epoch 4: 0 / 8075: 48.525440599886174\n",
      "Epoch 4: 1000 / 8075: 48.18081009689441\n",
      "Epoch 4: 2000 / 8075: 48.20518437766289\n",
      "Epoch 4: 3000 / 8075: 47.90535667554824\n",
      "Epoch 4: 4000 / 8075: 47.937297579413574\n",
      "Epoch 4: 5000 / 8075: 48.09878080690355\n",
      "Epoch 4: 6000 / 8075: 47.857857657575074\n",
      "Epoch 4: 7000 / 8075: 47.17504089555748\n",
      "Epoch 4: 8000 / 8075: 47.44967709023066\n",
      "Epoch 5: 0 / 8075: 47.50826712781653\n",
      "Epoch 5: 1000 / 8075: 47.25126499878062\n",
      "Epoch 5: 2000 / 8075: 47.37489050313376\n",
      "Epoch 5: 3000 / 8075: 47.14867330697224\n",
      "Epoch 5: 4000 / 8075: 47.138031812088\n",
      "Epoch 5: 5000 / 8075: 47.315755616306696\n",
      "Epoch 5: 6000 / 8075: 47.08100838619764\n",
      "Epoch 5: 7000 / 8075: 46.437687512375014\n",
      "Epoch 5: 8000 / 8075: 46.770513519244005\n"
     ]
    }
   ],
   "source": [
    "model.train(data, 'Adagrad', learning_rate, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefeliciteerdreet di good er rat: \"Ne oog achten schooan van zi\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict('Gefeliciteerd', 50)\n",
    "print(''.join(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "* Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network \tElsevier \"Physica D: Nonlinear Phenomena\" journal, Volume 404, March 2020: Special Issue on Machine Learning and Dynamical Systems (DOI: \t10.1016/j.physd.2019.132306)\n",
    "* https://www.kdnuggets.com/2020/07/rnn-deep-learning-sequential-data.html\n",
    "* https://gist.github.com/karpathy/d4dee566867f8291f086"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
