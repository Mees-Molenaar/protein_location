{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN in Numpy\n",
    "\n",
    "Recurrent Neural Networks (RNN) are a class of Artificial Neural Networks that is extremely well equiped to process a sequence of inputs. Therefore, RNNs are usefull in time series. \n",
    "\n",
    "In this work, I created a RNN from scratch using Numpy. This RNN is based on Andrej Karpathy's char-rnn and will be the basis for a LSTM network. This network will be used to classify protein locations based on the amino acid sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import numpy and matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the training data and save some important variables. If you want to train on your own text, just change the .txt file in the data variable.\n",
    "#data = open('shakespeare.txt', 'r').read()\n",
    "data = open('nescio.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size = len(data)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set is length 201884\n",
      "Vocab set is length 88\n"
     ]
    }
   ],
   "source": [
    "print(f'Data set is length {data_size}')\n",
    "print(f'Vocab set is length {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple character embedding\n",
    "char_to_idx = {char:i for i, char in enumerate(chars)}\n",
    "idx_to_char = {i:char for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    r\"\"\" Simple recurrent neural network (RNN) class for an input sequence.\n",
    "    \n",
    "        This RNN initializes weight and gradients. And contains the forward\n",
    "        and backward pass. The network is optimized using Adagrad.\n",
    "        The train method is used to train the network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        seq_length : Number of layers connected to each others. \n",
    "        hidden_sz : The number of features in the hidden state h.\n",
    "        vocab_sz : The number of possible inputs and outputs.\n",
    "        \n",
    "        \n",
    "        Inputs (train)\n",
    "        --------------\n",
    "        data : Data used to train the network.\n",
    "        optimizer : The optimizer that is used to train the network.\n",
    "        lr : The learning rate used to train the network.\n",
    "        epochs : The number of epochs to train the network.\n",
    "        progress : If True, shows the progress of training the network.\n",
    "        \n",
    "        Inputs (predict)\n",
    "        ----------------\n",
    "        start : Start of a sentence that the network uses as initial sequence.\n",
    "        n : Length of the prediction.\n",
    "        \n",
    "        \n",
    "        Output (train)\n",
    "        --------------\n",
    "        smooth_loss : The loss of the current trained network.\n",
    "        Wxh, Whh, Why : Updated weights of the network due to training.\n",
    "        bh, by : Updated biases due to training.\n",
    "        \n",
    "        Output (predict)\n",
    "        ----------------\n",
    "        txt : A string that is predicted by the RNN.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seq_length, hidden_sz, vocab_sz):\n",
    "        self.hs = {} # Hidden states\n",
    "        self.sm_ps = {} # Softmax probabilities\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.vocab_sz = vocab_sz\n",
    "        \n",
    "        # Start with zero loss\n",
    "        self.loss = 0 \n",
    "        \n",
    "        # Initiate weight matrices\n",
    "        self.Wxh, self.Whh, self.Why, self.bh, self.by = self.init_weights()\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes weights and biases based on the inputs hidden_sz and vocab_sz\n",
    "        \"\"\"\n",
    "        Wxh = np.random.randn(self.hidden_sz, self.vocab_sz) * 0.01 #times 0.01 to avoid exploding gradients\n",
    "        Whh = np.random.randn(self.hidden_sz, self.hidden_sz) * 0.01\n",
    "        Why = np.random.randn(self.vocab_sz, self.hidden_sz) * 0.01\n",
    "        \n",
    "        # bias\n",
    "        bh = np.zeros((hidden_size, 1))\n",
    "        by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        return Wxh, Whh, Why, bh, by\n",
    "    \n",
    "    def init_gradients(self):\n",
    "        \"\"\"\n",
    "        Initializes gradients for biases and weights.\n",
    "        \"\"\"\n",
    "        self.dWxh, self.dWhh, self.dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        self.dby, self.dbh = np.zeros_like(self.by), np.zeros_like(self.bh)\n",
    "    \n",
    "    def forward(self, xs, targets):\n",
    "        \"\"\"\n",
    "        Forward pass of the RNN\n",
    "        \"\"\"\n",
    "        \n",
    "        y_preds = {}\n",
    "\n",
    "        self.loss = 0\n",
    "\n",
    "        for i in range(len(xs)):\n",
    "            x = xs[i]\n",
    "            x_vec = np.zeros((self.vocab_sz, 1)) # vectorize the input\n",
    "            x_vec[x] = 1\n",
    "\n",
    "            # Calculate the new hidden, which is based on the input and the previous hidden layer\n",
    "            self.hs[i] = np.tanh(np.dot(self.Wxh, x_vec) + np.dot(self.Whh, self.hs[i - 1]) + self.bh)\n",
    "            # Predict y\n",
    "            y_preds[i] = np.dot(self.Why, self.hs[i]) + self.by\n",
    "\n",
    "            self.sm_ps[i] = np.exp(y_preds[i]) / np.sum(np.exp(y_preds[i])) # Softmax probabilty\n",
    "            self.loss += -np.log(self.sm_ps[i][targets[i], 0]) #Negative loss likelyhood\n",
    "\n",
    "        self.hs[-1] = self.hs[len(xs) - 1]\n",
    "        \n",
    "    def backward(self, xs, targets):\n",
    "        \"\"\"\n",
    "        Backward pass of the RNN\n",
    "        \"\"\"\n",
    "        self.init_gradients()\n",
    "    \n",
    "        # Initialize empty next hidden layer for the first backprop\n",
    "        dhnext = np.zeros_like(self.hs[0])\n",
    "\n",
    "        for i in reversed(range(len(xs))):\n",
    "            # X to vector\n",
    "            x = xs[i]    \n",
    "            x_vec = np.zeros((vocab_size, 1))\n",
    "            x_vec[x] = 1\n",
    "\n",
    "            dy = np.copy(self.sm_ps[i])\n",
    "            dy[targets[i]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "\n",
    "            self.dby += dy   \n",
    "            self.dWhy += np.dot(dy, self.hs[i].T)\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext\n",
    "            dhraw = (1 - self.hs[i] * self.hs[i]) * dh  \n",
    "            self.dWxh += np.dot(dhraw, x_vec.T)\n",
    "            self.dWhh += np.dot(dhraw, self.hs[i-1].T)\n",
    "            self.dbh += dhraw\n",
    "            dhnext = np.dot(self.Whh.T, dhraw)\n",
    "\n",
    "        # Clip to prevent exploding gradients\n",
    "        for dparam in [self.dWhy, self.dWxh, self.dWhh, self.dbh, self.dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "            \n",
    "    def init_adagrad_memory(self):\n",
    "        \"\"\"\n",
    "        Initialize memory matrices needed for Adagrad.\n",
    "        \"\"\"\n",
    "        self.mWxh, self.mWhh, self.mWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        self.mbh, self.mby  = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "\n",
    "    def update_gradients(self, optimizer, lr):\n",
    "        \"\"\"\n",
    "        Update gradients based on the optimizer you have chosen.\n",
    "        \"\"\"\n",
    "        if optimizer == 'Adagrad':\n",
    "            if not hasattr(self, 'mWhh'):\n",
    "                self.init_adagrad_memory()\n",
    "                \n",
    "            # perform parameter update with Adagrad\n",
    "            for param, dparam, mem in zip([self.Wxh, self.Whh, self.Why, self.bh, self.by],\n",
    "                                  [self.dWxh, self.dWhh, self.dWhy, self.dbh, self.dby],\n",
    "                                  [self.mWxh, self.mWhh, self.mWhy, self.mbh, self.mby]):\n",
    "                mem += dparam * dparam\n",
    "                param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update\n",
    "                \n",
    "    def reset_hidden(self):\n",
    "        \"\"\"\n",
    "        Reset the hidden layer\n",
    "        \"\"\"\n",
    "        self.hs[-1] = np.zeros((self.hidden_sz, 1))\n",
    "        \n",
    "    def plot_losses(self):\n",
    "        \"\"\"\n",
    "        Plot the cross entropy loss against the number of sequences\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'losses'):\n",
    "            plt.plot(self.losses)\n",
    "            plt.xlabel('Number of sequences')\n",
    "            plt.ylabel('Cross entropy loss')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('Error: No losses recorded, train the model!')\n",
    "    \n",
    "    def train(self, data, optimizer, lr, epochs, progress=True):\n",
    "        \"\"\"\n",
    "        Train the model by chopping the data in sequences followed by performing\n",
    "        the forward pass, backward pass and update the gradients.\n",
    "        \"\"\"\n",
    "        self.losses = []\n",
    "        smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "        \n",
    "        # Loop over the amount of epochs\n",
    "        for epoch in range(epochs):\n",
    "            n = 0\n",
    "            \n",
    "            # Reset hidden state\n",
    "            self.reset_hidden()\n",
    "            \n",
    "            data_len = len(data)\n",
    "            \n",
    "            # Loop over the amount of sequences\n",
    "            sequences_amount = int(data_len // self.seq_length)\n",
    "            for j in range(sequences_amount):\n",
    "                \n",
    "                start_pos = self.seq_length * j\n",
    "                \n",
    "                # Embed the inputs and targets\n",
    "                xs = [char_to_idx[ch] for ch in data[start_pos:start_pos+self.seq_length]]\n",
    "                targets = [char_to_idx[ch] for ch in data[start_pos+1:start_pos+self.seq_length+1]]\n",
    "                \n",
    "                # Forward pass\n",
    "                self.forward(xs, targets)\n",
    "                \n",
    "                # Backward\n",
    "                self.backward(xs, targets)\n",
    "                \n",
    "                # Update weight matrices\n",
    "                self.update_gradients(optimizer, lr)\n",
    "                \n",
    "                smooth_loss = smooth_loss * 0.999 + self.loss * 0.001\n",
    "        \n",
    "                if progress and n % 1000 == 0:\n",
    "                    print(f'Epoch {epoch + 1}: {n} / {sequences_amount}: {smooth_loss}')\n",
    "                 \n",
    "                n += 1\n",
    "                self.losses.append(smooth_loss)\n",
    "    \n",
    "    def predict(self, start, n):\n",
    "        \"\"\"\n",
    "        Predict a sequence of text based on a starting string.\n",
    "        \"\"\"\n",
    "        seed_idx = char_to_idx[start[-1]]\n",
    "        x = np.zeros((self.vocab_sz, 1))\n",
    "        x[seed_idx] = 1\n",
    "        \n",
    "        txt = [ch for ch in start]\n",
    "        \n",
    "        idxes = []\n",
    "        \n",
    "        h = self.hs[-1]\n",
    "        \n",
    "        for i in range(n):\n",
    "            \n",
    "            # Calculate the hidden\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            # Calculate y\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "\n",
    "            sm_p = np.exp(y) / np.sum(np.exp(y)) # Softmax probabilty\n",
    "            # Determine character based on weighted probability (is using the softmax probability)\n",
    "            idx = np.random.choice(range(self.vocab_sz), p=sm_p.ravel())\n",
    "            idxes.append(idx)\n",
    "            \n",
    "            # Save X for next iteration\n",
    "            x = np.zeros((self.vocab_sz, 1))\n",
    "            x[idx] = 1\n",
    "            \n",
    "        prediction = [idx_to_char[idx] for idx in idxes]\n",
    "        \n",
    "        txt += prediction\n",
    "        \n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(seq_length, hidden_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0 / 8075: 111.93341970213005\n",
      "Epoch 1: 1000 / 8075: 87.02995426837006\n",
      "Epoch 1: 2000 / 8075: 70.74127613864813\n",
      "Epoch 1: 3000 / 8075: 62.46724790521041\n",
      "Epoch 1: 4000 / 8075: 59.23088827646622\n",
      "Epoch 1: 5000 / 8075: 58.133945241467885\n",
      "Epoch 1: 6000 / 8075: 56.48318487108943\n",
      "Epoch 1: 7000 / 8075: 55.10175836654876\n",
      "Epoch 1: 8000 / 8075: 54.62127981371386\n",
      "Epoch 2: 0 / 8075: 54.72435594750463\n",
      "Epoch 2: 1000 / 8075: 54.87372633996036\n",
      "Epoch 2: 2000 / 8075: 53.86373880121659\n",
      "Epoch 2: 3000 / 8075: 52.90014062136721\n",
      "Epoch 2: 4000 / 8075: 53.10728107211579\n",
      "Epoch 2: 5000 / 8075: 53.49776582202467\n",
      "Epoch 2: 6000 / 8075: 53.03214355608396\n",
      "Epoch 2: 7000 / 8075: 52.24203650030198\n",
      "Epoch 2: 8000 / 8075: 52.32931173453958\n",
      "Epoch 3: 0 / 8075: 52.351851903036334\n",
      "Epoch 3: 1000 / 8075: 52.53651386047602\n",
      "Epoch 3: 2000 / 8075: 52.09733525638235\n",
      "Epoch 3: 3000 / 8075: 51.333146877522495\n",
      "Epoch 3: 4000 / 8075: 51.44704702525801\n",
      "Epoch 3: 5000 / 8075: 51.893809114813614\n",
      "Epoch 3: 6000 / 8075: 51.45803778978202\n",
      "Epoch 3: 7000 / 8075: 50.77810790392852\n",
      "Epoch 3: 8000 / 8075: 50.94310070457553\n",
      "Epoch 4: 0 / 8075: 50.965440154479346\n",
      "Epoch 4: 1000 / 8075: 50.63675356548542\n",
      "Epoch 4: 2000 / 8075: 50.49071450847707\n",
      "Epoch 4: 3000 / 8075: 49.96896549942912\n",
      "Epoch 4: 4000 / 8075: 50.095498206100764\n",
      "Epoch 4: 5000 / 8075: 50.60220647478897\n",
      "Epoch 4: 6000 / 8075: 50.26852761968227\n",
      "Epoch 4: 7000 / 8075: 49.62914455466571\n",
      "Epoch 4: 8000 / 8075: 49.93290419595929\n",
      "Epoch 5: 0 / 8075: 49.96430192389499\n",
      "Epoch 5: 1000 / 8075: 49.681485600264644\n",
      "Epoch 5: 2000 / 8075: 49.58102914619874\n",
      "Epoch 5: 3000 / 8075: 49.10482218819324\n",
      "Epoch 5: 4000 / 8075: 49.2061003579184\n",
      "Epoch 5: 5000 / 8075: 49.70890625308643\n",
      "Epoch 5: 6000 / 8075: 49.39391138267696\n",
      "Epoch 5: 7000 / 8075: 48.77425643498918\n",
      "Epoch 5: 8000 / 8075: 49.14495960724535\n"
     ]
    }
   ],
   "source": [
    "model.train(data, 'Adagrad', learning_rate, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefeliciteerd nouds, jaar niuk. He meen\n",
      "geen reeterel\n",
      "heer neit\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict('Gefeliciteerd', 50)\n",
    "print(''.join(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "* Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network \tElsevier \"Physica D: Nonlinear Phenomena\" journal, Volume 404, March 2020: Special Issue on Machine Learning and Dynamical Systems (DOI: \t10.1016/j.physd.2019.132306)\n",
    "* https://www.kdnuggets.com/2020/07/rnn-deep-learning-sequential-data.html\n",
    "* https://gist.github.com/karpathy/d4dee566867f8291f086"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
