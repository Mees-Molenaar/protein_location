{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34dd7822",
   "metadata": {},
   "source": [
    "# Single Amino Acid Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3764c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "791134f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a11ff11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b84330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb82c0287f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d81c4544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "table, th, td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<style>\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a868da1",
   "metadata": {},
   "source": [
    "## Loading Language Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bea5f4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-83bf9c20638d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# If trained in colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'content/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# If trained in colab\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('content/', force_remount=True)\n",
    "\n",
    "file_paths = Path('/content/content/MyDrive/subcellular-location/v2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b525b57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If trained from home (linux)\n",
    "file_paths = Path('/home/mees/Desktop/Machine_Learning/subcellular_location/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5cfac4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Entry name</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P68307</td>\n",
       "      <td>NU3M_BALMU</td>\n",
       "      <td>MNLLLTLLTNTTLALLLVFIAFWLPQLNVYAEKTSPYECGFDPMGS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0CY61</td>\n",
       "      <td>O162_CONBU</td>\n",
       "      <td>MKLTCVLIIAVLFLTAITADDSRDKQVYRAVGLIDKMRRIRASEGC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q0VIL3</td>\n",
       "      <td>OTOMP_DANRE</td>\n",
       "      <td>MDLPGGHLAVVLFLFVLVSMSTENNIIRWCTVSDAEDQKCLDLAGN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1W9I4</td>\n",
       "      <td>NUSB_ACISJ</td>\n",
       "      <td>MTDSTHPTPSARPPRQPRTGTTGTGARKAGSKSGRSRAREFALQAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q8DBX0</td>\n",
       "      <td>OMPU_VIBVU</td>\n",
       "      <td>MKKTLIALSVSAAAVATGVNAAELYNQDGTSLDMGGRAEARLSMKD...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Entry   Entry name                                           Sequence\n",
       "0  P68307   NU3M_BALMU  MNLLLTLLTNTTLALLLVFIAFWLPQLNVYAEKTSPYECGFDPMGS...\n",
       "1  P0CY61   O162_CONBU  MKLTCVLIIAVLFLTAITADDSRDKQVYRAVGLIDKMRRIRASEGC...\n",
       "2  Q0VIL3  OTOMP_DANRE  MDLPGGHLAVVLFLFVLVSMSTENNIIRWCTVSDAEDQKCLDLAGN...\n",
       "3  A1W9I4   NUSB_ACISJ  MTDSTHPTPSARPPRQPRTGTTGTGARKAGSKSGRSRAREFALQAL...\n",
       "4  Q8DBX0   OMPU_VIBVU  MKKTLIALSVSAAAVATGVNAAELYNQDGTSLDMGGRAEARLSMKD..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data in a pandas dataframe\n",
    "data_file = file_paths / Path('data/raw/LM_data_2021-03-11.csv')\n",
    "df = pd.read_csv(data_file, sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "522b76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete entry and entry name columns since we do not need it for the language model\n",
    "df.drop(['Entry', 'Entry name'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d9699",
   "metadata": {},
   "source": [
    "## Tokenize the sequences\n",
    "\n",
    "Use a function to tokenize the sequences. In this case, we use a kmer size of 1, which are single amino acids. And we add the End of Sequence (EOS) tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b093e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(df, protein_seqs_column, kmer_sz, stride=1, eos_token=True):\n",
    "    kmers = set()\n",
    "        \n",
    "    # Map kmers for one-hot encoding\n",
    "    kmer_to_id = dict()\n",
    "    id_to_kmer = dict()\n",
    "\n",
    "    # Loop over the protein sequences\n",
    "    for protein_seq in df[protein_seqs_column]:\n",
    "        # Loop over the sequence and add the amino acid if it is not in kmers set.\n",
    "        seq_len = len(protein_seq)\n",
    "\n",
    "\n",
    "        for i in range(0, seq_len - (kmer_sz - 1), stride):\n",
    "\n",
    "            kmer = protein_seq[i: i + kmer_sz]\n",
    "\n",
    "            if kmer not in list(kmers):\n",
    "                ind = len(kmers)\n",
    "                kmers.add(kmer)\n",
    "\n",
    "                # Also create the dictionary\n",
    "                kmer_to_id[kmer] = ind\n",
    "                id_to_kmer[ind] = kmer\n",
    "\n",
    "    if eos_token:\n",
    "        token = '<EOS>'\n",
    "        ind = len(kmers)\n",
    "        \n",
    "        kmers.add(token)\n",
    "\n",
    "        # Also create the dictionary\n",
    "        kmer_to_id[token] = ind\n",
    "        id_to_kmer[ind] = token\n",
    "\n",
    "    vocab_sz = len(kmers)\n",
    "\n",
    "    assert vocab_sz == len(kmer_to_id.keys())\n",
    "    \n",
    "    return kmer_to_id, id_to_kmer, vocab_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2c67fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df, protein_seqs_column, kmer_sz, stride=1, eos_token=True, premade_vocab=False):\n",
    "    \n",
    "    \n",
    "    # Create the vocabulary\n",
    "    if not premade_vocab:\n",
    "        \n",
    "        kmer_to_id, id_to_kmer, vocab_sz = create_vocab(df, protein_seqs_column, kmer_sz, stride, eos_token)\n",
    "                \n",
    "    else:\n",
    "        kmer_to_id, id_to_kmer = premade_vocab\n",
    "        vocab_sz = len(kmer_to_id)\n",
    "            \n",
    "    # Tokenize the sequences in the DF\n",
    "\n",
    "    tokenized = []\n",
    "    for i, protein_seq in enumerate(df[protein_seqs_column], 0):\n",
    "        sequence = []\n",
    "        \n",
    "        # If the kmer can't be found these indexes should be deleted\n",
    "        remove_idxs = []\n",
    "        \n",
    "        # Loop over the protein sequence\n",
    "        for i in  range(len(protein_seq) - (kmer_sz -1)):\n",
    "            # Convert kmer to integer\n",
    "            kmer = protein_seq[i: i + kmer_sz]\n",
    "            \n",
    "            # For some reason, some kmers miss. Thus these sequences have to be removed\n",
    "            try:\n",
    "                sequence.append(kmer_to_id[kmer])\n",
    "            except:\n",
    "                remove_idxs.append(i)\n",
    "                \n",
    "        if eos_token:\n",
    "            sequence.append(kmer_to_id['<EOS>'])\n",
    "            \n",
    "        tokenized.append(sequence)\n",
    "            \n",
    "    df['tokenized_seqs'] = tokenized\n",
    "    \n",
    "    df.drop(remove_idxs, inplace=True)\n",
    "    \n",
    "    return df, vocab_sz, kmer_to_id, id_to_kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cbc09ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMER_SIZE = 1 # Single Amino Acids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43bc4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the protein sequence\n",
    "df, vocab_sz, kmer_to_id, id_to_kmer = tokenize(df, 'Sequence', KMER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13da2e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab_file = file_paths / Path('data/interim/AA_vocab.pkl')\n",
    "pickle.dump([kmer_to_id, id_to_kmer], open(save_vocab_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f6e6c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>tokenized_seqs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNLLLTLLTNTTLALLLVFIAFWLPQLNVYAEKTSPYECGFDPMGS...</td>\n",
       "      <td>[0, 1, 2, 2, 2, 3, 2, 2, 3, 1, 3, 3, 2, 4, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MKLTCVLIIAVLFLTAITADDSRDKQVYRAVGLIDKMRRIRASEGC...</td>\n",
       "      <td>[0, 13, 2, 3, 15, 5, 2, 7, 7, 4, 5, 2, 6, 2, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDLPGGHLAVVLFLFVLVSMSTENNIIRWCTVSDAEDQKCLDLAGN...</td>\n",
       "      <td>[0, 17, 2, 9, 16, 16, 19, 2, 4, 5, 5, 2, 6, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MTDSTHPTPSARPPRQPRTGTTGTGARKAGSKSGRSRAREFALQAL...</td>\n",
       "      <td>[0, 3, 17, 14, 3, 19, 9, 3, 9, 14, 4, 18, 9, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MKKTLIALSVSAAAVATGVNAAELYNQDGTSLDMGGRAEARLSMKD...</td>\n",
       "      <td>[0, 13, 13, 3, 2, 7, 4, 2, 14, 5, 14, 4, 4, 4,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence  \\\n",
       "0  MNLLLTLLTNTTLALLLVFIAFWLPQLNVYAEKTSPYECGFDPMGS...   \n",
       "1  MKLTCVLIIAVLFLTAITADDSRDKQVYRAVGLIDKMRRIRASEGC...   \n",
       "2  MDLPGGHLAVVLFLFVLVSMSTENNIIRWCTVSDAEDQKCLDLAGN...   \n",
       "3  MTDSTHPTPSARPPRQPRTGTTGTGARKAGSKSGRSRAREFALQAL...   \n",
       "4  MKKTLIALSVSAAAVATGVNAAELYNQDGTSLDMGGRAEARLSMKD...   \n",
       "\n",
       "                                      tokenized_seqs  \n",
       "0  [0, 1, 2, 2, 2, 3, 2, 2, 3, 1, 3, 3, 2, 4, 2, ...  \n",
       "1  [0, 13, 2, 3, 15, 5, 2, 7, 7, 4, 5, 2, 6, 2, 3...  \n",
       "2  [0, 17, 2, 9, 16, 16, 19, 2, 4, 5, 5, 2, 6, 2,...  \n",
       "3  [0, 3, 17, 14, 3, 19, 9, 3, 9, 14, 4, 18, 9, 9...  \n",
       "4  [0, 13, 13, 3, 2, 7, 4, 2, 14, 5, 14, 4, 4, 4,...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e56d1641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 2, 3, 2, 2, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "sequences = df['tokenized_seqs'].tolist()\n",
    "for seq in sequences:\n",
    "    for kmer in seq:\n",
    "        data.append(int(kmer))\n",
    "        \n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f8e9e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f05d756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AminoLMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, seq_len, stride=1):\n",
    "        self.data = torch.Tensor(data)\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        xs = torch.LongTensor(data[idx: idx + self.seq_len])\n",
    "        targets = data[idx + self.stride: idx + self.seq_len + self.stride]\n",
    "\n",
    "        ys = []\n",
    "\n",
    "        for target in targets:\n",
    "            y = torch.tensor(target)\n",
    "            ys.append(y)\n",
    "\n",
    "        ys = torch.stack(ys)\n",
    "\n",
    "        ys = ys.to(dev)\n",
    "        xs = xs.to(dev) \n",
    "    \n",
    "        return xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2412e8fd",
   "metadata": {},
   "source": [
    "## Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baa3703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(nn.Module):\n",
    "    \"Apply dropout to an Embedding with probability emp_p\"\n",
    "\n",
    "    def __init__(self, emb_p=0):\n",
    "        super(EmbeddingDropout, self).__init__()\n",
    "        \n",
    "        self.emb_p = emb_p\n",
    "\n",
    "    def forward(self, inp):\n",
    "       \n",
    "        drop = torch.nn.Dropout(self.emb_p)\n",
    "        placeholder = torch.ones((inp.size(1), 1)).to(dev)\n",
    "        mask = drop(placeholder)      \n",
    "        out = inp * mask\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ce45637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWD_LSTM(torch.nn.Module):\n",
    "    def __init__(self, num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz = 1):\n",
    "        super(AWD_LSTM, self).__init__()\n",
    "        \n",
    "        # Embedding with droput\n",
    "        self.encoder = nn.Embedding(vocab_sz, emb_dim)\n",
    "        self.emb_drop = EmbeddingDropout(emb_p=embed_p)\n",
    "\n",
    "        \n",
    "        # Dropouts on the inputs and the hidden layers\n",
    "        self.hid_dp = torch.nn.Dropout(p=hidden_p)\n",
    "        \n",
    "        self.lstms = nn.LSTM(emb_dim, hid_sz, num_layers, batch_first = True, dropout=input_p)\n",
    "\n",
    "        \n",
    "        # Save all variables        \n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_sz = hid_sz\n",
    "        self.hidden_p = hidden_p\n",
    "        self.embed_p = embed_p\n",
    "        self.input_p = input_p\n",
    "        self.weight_p = weight_p\n",
    "        self.batch_sz = batch_sz\n",
    "\n",
    "        # Initialize hidden layers        \n",
    "        self.reset_hidden()\n",
    "        self.last_hiddens = (self.hidden_state, self.cell_state)\n",
    "                \n",
    "    def forward(self, xs):\n",
    "        \"\"\"Forward pass AWD-LSTM\"\"\" \n",
    "        \n",
    "        bs, sl = xs.shape\n",
    "\n",
    "        ys = []\n",
    "        \n",
    "        hiddens = self.last_hiddens\n",
    "        \n",
    "        # Embed the inputs\n",
    "        embed = self.encoder(xs)\n",
    "        embed_dp = self.emb_drop(embed)\n",
    "        \n",
    "        inp = embed_dp.view(bs, sl, -1)\n",
    "        \n",
    "        # Dropout on hidden layers\n",
    "        hiddens_dp = []\n",
    "        for hidden_state in hiddens:\n",
    "            hiddens_dp.append(self.hid_dp(hidden_state))\n",
    "            \n",
    "        hiddens_dp = tuple(hiddens_dp)\n",
    "        \n",
    "        output, hidden_outs = self.lstms(embed_dp.view(sl, bs, -1), hiddens_dp)\n",
    "        \n",
    "        # Detach hidden layers\n",
    "        hidden_detached = []\n",
    "        \n",
    "        for hidden_out in hidden_outs:\n",
    "            hidden_detached.append(hidden_out.detach())\n",
    "            \n",
    "        \n",
    "        self.last_hiddens = hidden_detached\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def reset_hidden(self):\n",
    "        self.hidden_state = torch.zeros((self.num_layers, self.num_layers, self.hid_sz)).to(dev)\n",
    "        self.cell_state = torch.zeros((self.num_layers, self.num_layers, self.hid_sz)).to(dev)\n",
    "        self.last_hiddens = (self.hidden_state, self.cell_state)\n",
    "    \n",
    "    def freeze_to(self , n):\n",
    "        \n",
    "        params_to_freeze = n * 4 + 1 # Since each LSTM layer has 4 parameters plus 1 to also freeze the encoder\n",
    "        \n",
    "        total_params = len(list(self.parameters()))\n",
    "        \n",
    "        for i, parameter in enumerate(self.parameters()):\n",
    "            parameter.requires_grad = True\n",
    "            \n",
    "            if i < params_to_freeze:\n",
    "                parameter.requires_grad = False\n",
    "            \n",
    "            \n",
    "        for name, parameter in self.named_parameters():\n",
    "            print(name)\n",
    "            print(parameter.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61317515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AALM(nn.Module):\n",
    "    def __init__(self, num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz = 1):\n",
    "        super(AALM, self).__init__()\n",
    "        \n",
    "        self.encoder = AWD_LSTM(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, \n",
    "                                embed_p, input_p, weight_p, batch_sz=batch_sz)\n",
    "        self.decoder = nn.Linear(hid_sz, vocab_sz)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        encoded = self.encoder(inp)\n",
    "        \n",
    "        y = self.decoder(encoded)\n",
    "        \n",
    "        return y \n",
    "    \n",
    "    def freeze_to(self, n):\n",
    "        self.encoder.freeze_to(n)\n",
    "        \n",
    "    def reset_hidden(self):\n",
    "        self.encoder.reset_hidden()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14bcdff",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e5731cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "emb_dim = 10 # Embeddding dimension\n",
    "hid_sz = 1150 # Hidden size\n",
    "num_layers = 20 # Number of LSTM layers stacked together\n",
    "seq_len = num_layers\n",
    "bs = 8\n",
    "\n",
    "# Dropout parameters\n",
    "\n",
    "embed_p = 0.1 # Dropout probability on the embedding\n",
    "hidden_p = 0.3 # Dropout probability on hidden-to-hidden weight matrices\n",
    "# Dropout tussen de inputs van de LSTMs moet ik er nog in bouwen\n",
    "input_p = 0.3 # Dropout probablity on the LSTM input between LSTMS\n",
    "weight_p = 0.5 # Dropout probability on LSTM-to-LSTM weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0c7b66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AALM(\n",
       "  (encoder): AWD_LSTM(\n",
       "    (encoder): Embedding(26, 10)\n",
       "    (emb_drop): EmbeddingDropout()\n",
       "    (hid_dp): Dropout(p=0.3, inplace=False)\n",
       "    (lstms): LSTM(10, 1150, num_layers=20, batch_first=True, dropout=0.3)\n",
       "  )\n",
       "  (decoder): Linear(in_features=1150, out_features=26, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AALM(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz=bs)\n",
    "model = model.to(dev)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946a324",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e50f9a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = AminoLMDataset(data, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f512a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7244f7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7374044"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_train_len = len(training_loader)\n",
    "total_train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "623df5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamaters\n",
    "learning_rate = 0.001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d392870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Costfunction and optimize algorithm\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0f613c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\n",
      "torch.Size([8, 20])\n",
      "Labels:\n",
      "tensor([[ 1,  2,  2,  2,  3,  2,  2,  3,  1,  3,  3,  2,  4,  2,  2,  2,  5,  6,\n",
      "          7,  4],\n",
      "        [ 2,  2,  2,  3,  2,  2,  3,  1,  3,  3,  2,  4,  2,  2,  2,  5,  6,  7,\n",
      "          4,  6],\n",
      "        [ 2,  2,  3,  2,  2,  3,  1,  3,  3,  2,  4,  2,  2,  2,  5,  6,  7,  4,\n",
      "          6,  8],\n",
      "        [ 2,  3,  2,  2,  3,  1,  3,  3,  2,  4,  2,  2,  2,  5,  6,  7,  4,  6,\n",
      "          8,  2],\n",
      "        [ 3,  2,  2,  3,  1,  3,  3,  2,  4,  2,  2,  2,  5,  6,  7,  4,  6,  8,\n",
      "          2,  9],\n",
      "        [ 2,  2,  3,  1,  3,  3,  2,  4,  2,  2,  2,  5,  6,  7,  4,  6,  8,  2,\n",
      "          9, 10],\n",
      "        [ 2,  3,  1,  3,  3,  2,  4,  2,  2,  2,  5,  6,  7,  4,  6,  8,  2,  9,\n",
      "         10,  2],\n",
      "        [ 3,  1,  3,  3,  2,  4,  2,  2,  2,  5,  6,  7,  4,  6,  8,  2,  9, 10,\n",
      "          2,  1]])\n",
      "Output shape:\n",
      "torch.Size([160, 26])\n",
      "Label shape:\n",
      "torch.Size([160])\n",
      "tensor(3.2592, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Test for the real work\n",
    "for i, entry in enumerate(training_loader, 0):\n",
    "    xs, ys = entry[0], entry[1]\n",
    "\n",
    "    print('Input shape:')\n",
    "    print(xs.shape)\n",
    "\n",
    "    outputs = model(xs)\n",
    "    \n",
    "    bs, sl = outputs.shape[:2]\n",
    "    \n",
    "    # Flatten the output\n",
    "    outputs = outputs.view(bs * sl, -1)\n",
    "    \n",
    "    print('Labels:')  \n",
    "    print(ys)\n",
    "    \n",
    "    # Flatten the label\n",
    "    ys = ys.view(-1)\n",
    "    \n",
    "    print('Output shape:')\n",
    "    print(outputs.shape)\n",
    "    \n",
    "    print('Label shape:')\n",
    "    print(ys.shape)\n",
    "    \n",
    "    loss = criterion(outputs, ys)\n",
    "    print(loss)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d322e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <thead>\n",
       "          <tr>\n",
       "          <th>Epoch</th>\n",
       "          <th>Percentage</th>\n",
       "          <th>Loss</th>\n",
       "          <th>Accuracy</th>\n",
       "          <th>Time</th>\n",
       "          </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-99283cea5444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtrain_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-44974b051220>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "display(HTML(\n",
    "    \"\"\"<table>\n",
    "        <thead>\n",
    "          <tr>\n",
    "          <th>Epoch</th>\n",
    "          <th>Percentage</th>\n",
    "          <th>Loss</th>\n",
    "          <th>Accuracy</th>\n",
    "          <th>Time</th>\n",
    "          </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "        \"\"\"\n",
    "))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.reset_hidden()\n",
    "\n",
    "    # Initialize loss at 0\n",
    "    epoch_loss = 0.0\n",
    "    iteration_loss = 0.0\n",
    "    train_total = 0\n",
    "\n",
    "    for i, entry in enumerate(training_loader, 0):\n",
    "\n",
    "        if len(entry[0]) == bs:\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            xs, ys = entry[0], entry[1]\n",
    "\n",
    "            outputs = model(xs)\n",
    "\n",
    "            bs, sl = outputs.shape[:2]\n",
    "\n",
    "            # Flatten the output\n",
    "            outputs = outputs.view(bs * sl, -1)\n",
    "\n",
    "            # Flatten the label\n",
    "            ys = ys.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, ys)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += outputs.shape[0] * loss.item()\n",
    "            iteration_loss += outputs.shape[0] * loss.item()\n",
    "            \n",
    "            # Also calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            correct_pred += (predicted == ys).sum().item()\n",
    "            train_total += ys.size(0)\n",
    "\n",
    "\n",
    "            if i % 1e2 == 0:\n",
    "\n",
    "                round_time = time.time()\n",
    "                duration = round(((round_time - start_time) / 60), 0) # To convert to minutes\n",
    "                start_time = time.time()\n",
    "\n",
    "                perc = round((i / total_train_len * 100), 2)\n",
    "\n",
    "                iteration_loss = round((iteration_loss / 1e4), 2)\n",
    "                \n",
    "                accuracy = np.round((correct_pred / train_total * 100), 2)\n",
    "\n",
    "                display(HTML(\n",
    "                \"\"\"<tr>\n",
    "                <td>{}</td>\n",
    "                <td>{}</td>\n",
    "                <td>{}</td>\n",
    "                <td>{}</td>\n",
    "                </tr>\"\"\".format(str(epoch + 1), str(perc), str(iteration_loss), str(duration))\n",
    "                ))\n",
    "\n",
    "                iteration_loss = 0.0\n",
    "\n",
    "    loss_history.append(epoch_loss)\n",
    "\n",
    "    print(f'Epoch {str(epoch + 1)} Train loss: {str(epoch_loss)}.')\n",
    "\n",
    "display(HTML('</tbody></table>'))        \n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f3265e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
