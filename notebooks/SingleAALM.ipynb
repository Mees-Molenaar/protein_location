{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34dd7822",
   "metadata": {},
   "source": [
    "# Single Amino Acid Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3764c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791134f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a11ff11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98b84330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc5a801a950>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d81c4544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "table, th, td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<style>\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a868da1",
   "metadata": {},
   "source": [
    "## Loading Language Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bea5f4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-83bf9c20638d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# If trained in colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'content/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# If trained in colab\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('content/', force_remount=True)\n",
    "\n",
    "file_paths = Path('/content/content/MyDrive/subcellular-location/v2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b525b57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If trained from home (linux)\n",
    "file_paths = Path('/home/mees/Desktop/Machine_Learning/subcellular_location/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5cfac4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Entry name</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P68307</td>\n",
       "      <td>NU3M_BALMU</td>\n",
       "      <td>MNLLLTLLTNTTLALLLVFIAFWLPQLNVYAEKTSPYECGFDPMGS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0CY61</td>\n",
       "      <td>O162_CONBU</td>\n",
       "      <td>MKLTCVLIIAVLFLTAITADDSRDKQVYRAVGLIDKMRRIRASEGC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q0VIL3</td>\n",
       "      <td>OTOMP_DANRE</td>\n",
       "      <td>MDLPGGHLAVVLFLFVLVSMSTENNIIRWCTVSDAEDQKCLDLAGN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1W9I4</td>\n",
       "      <td>NUSB_ACISJ</td>\n",
       "      <td>MTDSTHPTPSARPPRQPRTGTTGTGARKAGSKSGRSRAREFALQAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q8DBX0</td>\n",
       "      <td>OMPU_VIBVU</td>\n",
       "      <td>MKKTLIALSVSAAAVATGVNAAELYNQDGTSLDMGGRAEARLSMKD...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Entry   Entry name                                           Sequence\n",
       "0  P68307   NU3M_BALMU  MNLLLTLLTNTTLALLLVFIAFWLPQLNVYAEKTSPYECGFDPMGS...\n",
       "1  P0CY61   O162_CONBU  MKLTCVLIIAVLFLTAITADDSRDKQVYRAVGLIDKMRRIRASEGC...\n",
       "2  Q0VIL3  OTOMP_DANRE  MDLPGGHLAVVLFLFVLVSMSTENNIIRWCTVSDAEDQKCLDLAGN...\n",
       "3  A1W9I4   NUSB_ACISJ  MTDSTHPTPSARPPRQPRTGTTGTGARKAGSKSGRSRAREFALQAL...\n",
       "4  Q8DBX0   OMPU_VIBVU  MKKTLIALSVSAAAVATGVNAAELYNQDGTSLDMGGRAEARLSMKD..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data in a pandas dataframe\n",
    "data_file = file_paths / Path('data/raw/LM_data_2021-03-11.csv')\n",
    "df = pd.read_csv(data_file, sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "522b76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete entry and entry name columns since we do not need it for the language model\n",
    "df.drop(['Entry', 'Entry name'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d9699",
   "metadata": {},
   "source": [
    "## Tokenize the sequences\n",
    "\n",
    "Use a function to tokenize the sequences. In this case, we use a kmer size of 1, which are single amino acids. And we add the End of Sequence (EOS) tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b093e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(df, protein_seqs_column, kmer_sz, stride=1, eos_token=True):\n",
    "    kmers = set()\n",
    "        \n",
    "    # Map kmers for one-hot encoding\n",
    "    kmer_to_id = dict()\n",
    "    id_to_kmer = dict()\n",
    "\n",
    "    # Loop over the protein sequences\n",
    "    for protein_seq in df[protein_seqs_column]:\n",
    "        # Loop over the sequence and add the amino acid if it is not in kmers set.\n",
    "        seq_len = len(protein_seq)\n",
    "\n",
    "\n",
    "        for i in range(0, seq_len - (kmer_sz - 1), stride):\n",
    "\n",
    "            kmer = protein_seq[i: i + kmer_sz]\n",
    "\n",
    "            if kmer not in list(kmers):\n",
    "                ind = len(kmers)\n",
    "                kmers.add(kmer)\n",
    "\n",
    "                # Also create the dictionary\n",
    "                kmer_to_id[kmer] = ind\n",
    "                id_to_kmer[ind] = kmer\n",
    "\n",
    "    if eos_token:\n",
    "        token = '<EOS>'\n",
    "        ind = len(kmers)\n",
    "        \n",
    "        kmers.add(token)\n",
    "\n",
    "        # Also create the dictionary\n",
    "        kmer_to_id[token] = ind\n",
    "        id_to_kmer[ind] = token\n",
    "\n",
    "    # Add padding token as the last token in the dictionary\n",
    "    token = '<PAD>'\n",
    "    ind = len(kmers)  \n",
    "    \n",
    "    kmers.add(token)\n",
    "\n",
    "    # Also create the dictionary\n",
    "    kmer_to_id[token] = ind\n",
    "    id_to_kmer[ind] = token\n",
    "\n",
    "    vocab_sz = len(kmers)\n",
    "\n",
    "    assert vocab_sz == len(kmer_to_id.keys())\n",
    "    \n",
    "    return kmer_to_id, id_to_kmer, vocab_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2c67fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df, protein_seqs_column, kmer_sz, stride=1, eos_token=True, premade_vocab=False):\n",
    "    \n",
    "    \n",
    "    # Create the vocabulary\n",
    "    if not premade_vocab:\n",
    "        \n",
    "        kmer_to_id, id_to_kmer, vocab_sz = create_vocab(df, protein_seqs_column, kmer_sz, stride, eos_token)\n",
    "                \n",
    "    else:\n",
    "        kmer_to_id, id_to_kmer = premade_vocab\n",
    "        vocab_sz = len(kmer_to_id)\n",
    "            \n",
    "    # Tokenize the sequences in the DF\n",
    "\n",
    "    tokenized = []\n",
    "    for i, protein_seq in enumerate(df[protein_seqs_column], 0):\n",
    "        sequence = []\n",
    "        \n",
    "        # If the kmer can't be found these indexes should be deleted\n",
    "        remove_idxs = []\n",
    "        \n",
    "        # Loop over the protein sequence\n",
    "        for i in  range(len(protein_seq) - (kmer_sz -1)):\n",
    "            # Convert kmer to integer\n",
    "            kmer = protein_seq[i: i + kmer_sz]\n",
    "            \n",
    "            # For some reason, some kmers miss. Thus these sequences have to be removed\n",
    "            try:\n",
    "                sequence.append(kmer_to_id[kmer])\n",
    "            except:\n",
    "                remove_idxs.append(i)\n",
    "                \n",
    "        if eos_token:\n",
    "            sequence.append(kmer_to_id['<EOS>'])\n",
    "            \n",
    "        tokenized.append(sequence)\n",
    "            \n",
    "    df['tokenized_seqs'] = tokenized\n",
    "    \n",
    "    df.drop(remove_idxs, inplace=True)\n",
    "    \n",
    "    return df, vocab_sz, kmer_to_id, id_to_kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cbc09ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMER_SIZE = 1 # Single Amino Acids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43bc4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the protein sequence\n",
    "df, vocab_sz, kmer_to_id, id_to_kmer = tokenize(df, 'Sequence', KMER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13da2e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab_file = file_paths / Path('data/interim/AA_vocab.pkl')\n",
    "pickle.dump([kmer_to_id, id_to_kmer], open(save_vocab_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f6e6c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>tokenized_seqs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNLLLTLLTNTTLALLLVFIAFWLPQLNVYAEKTSPYECGFDPMGS...</td>\n",
       "      <td>[0, 1, 2, 2, 2, 3, 2, 2, 3, 1, 3, 3, 2, 4, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MKLTCVLIIAVLFLTAITADDSRDKQVYRAVGLIDKMRRIRASEGC...</td>\n",
       "      <td>[0, 13, 2, 3, 15, 5, 2, 7, 7, 4, 5, 2, 6, 2, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDLPGGHLAVVLFLFVLVSMSTENNIIRWCTVSDAEDQKCLDLAGN...</td>\n",
       "      <td>[0, 17, 2, 9, 16, 16, 19, 2, 4, 5, 5, 2, 6, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MTDSTHPTPSARPPRQPRTGTTGTGARKAGSKSGRSRAREFALQAL...</td>\n",
       "      <td>[0, 3, 17, 14, 3, 19, 9, 3, 9, 14, 4, 18, 9, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MKKTLIALSVSAAAVATGVNAAELYNQDGTSLDMGGRAEARLSMKD...</td>\n",
       "      <td>[0, 13, 13, 3, 2, 7, 4, 2, 14, 5, 14, 4, 4, 4,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence  \\\n",
       "0  MNLLLTLLTNTTLALLLVFIAFWLPQLNVYAEKTSPYECGFDPMGS...   \n",
       "1  MKLTCVLIIAVLFLTAITADDSRDKQVYRAVGLIDKMRRIRASEGC...   \n",
       "2  MDLPGGHLAVVLFLFVLVSMSTENNIIRWCTVSDAEDQKCLDLAGN...   \n",
       "3  MTDSTHPTPSARPPRQPRTGTTGTGARKAGSKSGRSRAREFALQAL...   \n",
       "4  MKKTLIALSVSAAAVATGVNAAELYNQDGTSLDMGGRAEARLSMKD...   \n",
       "\n",
       "                                      tokenized_seqs  \n",
       "0  [0, 1, 2, 2, 2, 3, 2, 2, 3, 1, 3, 3, 2, 4, 2, ...  \n",
       "1  [0, 13, 2, 3, 15, 5, 2, 7, 7, 4, 5, 2, 6, 2, 3...  \n",
       "2  [0, 17, 2, 9, 16, 16, 19, 2, 4, 5, 5, 2, 6, 2,...  \n",
       "3  [0, 3, 17, 14, 3, 19, 9, 3, 9, 14, 4, 18, 9, 9...  \n",
       "4  [0, 13, 13, 3, 2, 7, 4, 2, 14, 5, 14, 4, 4, 4,...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e56d1641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 2, 3, 2, 2, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "sequences = df['tokenized_seqs'].tolist()\n",
    "for seq in sequences:\n",
    "    for kmer in seq:\n",
    "        data.append(int(kmer))\n",
    "        \n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f8e9e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f05d756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AminoLMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, seq_len, stride=1):\n",
    "        self.data = torch.Tensor(data)\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        xs = torch.LongTensor(data[idx: idx + self.seq_len])\n",
    "        targets = data[idx + self.stride: idx + self.seq_len + self.stride]\n",
    "\n",
    "        ys = []\n",
    "\n",
    "        for target in targets:\n",
    "            y = torch.tensor(target)\n",
    "            ys.append(y)\n",
    "\n",
    "        ys = torch.stack(ys)\n",
    "\n",
    "        ys = ys.to(dev)\n",
    "        xs = xs.to(dev) \n",
    "    \n",
    "        return xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2412e8fd",
   "metadata": {},
   "source": [
    "## Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "baa3703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(torch.nn.Module):\n",
    "    \"Apply dropout to an Embedding with probability emp_p\"\n",
    "\n",
    "    def __init__(self, emb_p=0):\n",
    "        super(EmbeddingDropout, self).__init__()\n",
    "        \n",
    "        self.emb_p = emb_p\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        bs, sl = inp.shape[:2]\n",
    "        \n",
    "        drop = torch.nn.Dropout(self.emb_p)\n",
    "        placeholder = torch.ones((bs, sl, 1)).to(dev)\n",
    "        mask = drop(placeholder)      \n",
    "        out = inp * mask\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce45637",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cbaf4aa8382d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAWD_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAWD_LSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Embedding with dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class AWD_LSTM(torch.nn.Module):\n",
    "    def __init__(self, num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz = 1, pad_token=False):\n",
    "        super(AWD_LSTM, self).__init__()\n",
    "        \n",
    "        # Embedding with dropout\n",
    "        if pad_token:\n",
    "            self.encoder = nn.Embedding(vocab_sz, emb_dim, padding_idx=pad_token)\n",
    "        else:\n",
    "            self.encoder = nn.Embedding(vocab_sz, emb_dim, padding_idx=pad_token)\n",
    "            \n",
    "        self.emb_drop = EmbeddingDropout(emb_p=embed_p)\n",
    "\n",
    "        \n",
    "        # Dropouts on the inputs and the hidden layers\n",
    "        self.hid_dp = torch.nn.Dropout(p=hidden_p)\n",
    "        \n",
    "        self.lstms = nn.LSTM(emb_dim, hid_sz, num_layers, batch_first = True, dropout=input_p)\n",
    "\n",
    "        \n",
    "        # Save all variables        \n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_sz = hid_sz\n",
    "        self.hidden_p = hidden_p\n",
    "        self.embed_p = embed_p\n",
    "        self.input_p = input_p\n",
    "        self.weight_p = weight_p\n",
    "        self.batch_sz = batch_sz\n",
    "\n",
    "        # Initialize hidden layers        \n",
    "        self.reset_hidden()\n",
    "        self.last_hiddens = (self.hidden_state, self.cell_state)\n",
    "                \n",
    "    def forward(self, xs):\n",
    "        \"\"\"Forward pass AWD-LSTM\"\"\" \n",
    "        \n",
    "        bs, sl = xs.shape\n",
    "        \n",
    "        # Because sequences consisting of only padding are removed from the mini-batch, the mini-batch alters\n",
    "        # Therefore we have to adjust the hidden state for that\n",
    "        if bs != self.last_hiddens[0].shape[1]:\n",
    "            self._change_bs_hidden(bs)\n",
    "        \n",
    "        hidden_states = []\n",
    "        \n",
    "        hiddens = self.last_hiddens\n",
    "\n",
    "        embed = self.encoder(xs)\n",
    "        embed_dp = self.emb_drop(embed)\n",
    "\n",
    "        \n",
    "        inp = embed_dp.view(bs, sl, -1)\n",
    "        \n",
    "        # Dropout on hidden layers\n",
    "        hiddens_dp = []\n",
    "        for hidden_state in hiddens:\n",
    "            hiddens_dp.append(self.hid_dp(hidden_state))\n",
    "            \n",
    "        hiddens_dp = tuple(hiddens_dp)\n",
    "        \n",
    "        output, (h, c) = self.lstms(embed_dp.view(bs, sl, -1), hiddens_dp)\n",
    "        \n",
    "        self.last_hiddens = (h.detach(), c.detach())\n",
    "        \n",
    "        return output, self.last_hiddens\n",
    "    \n",
    "    def reset_hidden(self):\n",
    "        self.hidden_state = torch.zeros((self.num_layers, self.batch_sz, self.hid_sz)).to(dev)\n",
    "        self.cell_state = torch.zeros((self.num_layers, self.batch_sz, self.hid_sz)).to(dev)\n",
    "        self.last_hiddens = (self.hidden_state, self.cell_state)\n",
    "    \n",
    "    def _change_bs_hidden(self, bs):\n",
    "        hidden_state = self.last_hiddens[0]\n",
    "        cell_state = self.last_hiddens[1]\n",
    "        \n",
    "        if bs > hidden_state.shape[1]:\n",
    "            self.batch_sz = bs\n",
    "            self.reset_hidden()\n",
    "        else:\n",
    "            corr_hidden_state = hidden_state[:,:bs,:]\n",
    "            corr_cell_state = cell_state[:,:bs,:]\n",
    "        \n",
    "            self.last_hiddens = (corr_hidden_state, corr_cell_state)\n",
    "    \n",
    "    def freeze_to(self , n):\n",
    "        \n",
    "        params_to_freeze = n * 4 + 1 # Since each LSTM layer has 4 parameters plus 1 to also freeze the encoder\n",
    "        \n",
    "        total_params = len(list(self.parameters()))\n",
    "        \n",
    "        for i, parameter in enumerate(self.parameters()):\n",
    "            parameter.requires_grad = True\n",
    "            \n",
    "            if i < params_to_freeze:\n",
    "                parameter.requires_grad = False\n",
    "            \n",
    "            \n",
    "        for name, parameter in self.named_parameters():\n",
    "            print(name)\n",
    "            print(parameter.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61317515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AALM(nn.Module):\n",
    "    def __init__(self, num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz = 1):\n",
    "        super(AALM, self).__init__()\n",
    "        \n",
    "        self.encoder = AWD_LSTM(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, \n",
    "                                embed_p, input_p, weight_p, batch_sz=batch_sz)\n",
    "        self.decoder = nn.Linear(hid_sz, vocab_sz)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        encoded = self.encoder(inp)\n",
    "        \n",
    "        y = self.decoder(encoded)\n",
    "        \n",
    "        return y \n",
    "    \n",
    "    def freeze_to(self, n):\n",
    "        self.encoder.freeze_to(n)\n",
    "        \n",
    "    def reset_hidden(self):\n",
    "        self.encoder.reset_hidden()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14bcdff",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0e5731cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "emb_dim = 10 # Embeddding dimension\n",
    "hid_sz = 400 # Hidden size\n",
    "num_layers = 20 # Number of LSTM layers stacked together\n",
    "seq_len = num_layers\n",
    "bs = 8\n",
    "\n",
    "# Dropout parameters\n",
    "\n",
    "embed_p = 0.1 # Dropout probability on the embedding\n",
    "hidden_p = 0.3 # Dropout probability on hidden-to-hidden weight matrices\n",
    "# Dropout tussen de inputs van de LSTMs moet ik er nog in bouwen\n",
    "input_p = 0.3 # Dropout probablity on the LSTM input between LSTMS\n",
    "weight_p = 0.5 # Dropout probability on LSTM-to-LSTM weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0c7b66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AALM(\n",
       "  (encoder): AWD_LSTM(\n",
       "    (encoder): Embedding(27, 10, padding_idx=26)\n",
       "    (emb_drop): EmbeddingDropout()\n",
       "    (hid_dp): Dropout(p=0.3, inplace=False)\n",
       "    (lstms): LSTM(10, 400, num_layers=20, batch_first=True, dropout=0.3)\n",
       "  )\n",
       "  (decoder): Linear(in_features=400, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AALM(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz=bs)\n",
    "model = model.to(dev)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946a324",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e50f9a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = AminoLMDataset(data, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f512a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7244f7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7374044"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_train_len = len(training_loader)\n",
    "total_train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "623df5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamaters\n",
    "learning_rate = 0.0001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d392870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Costfunction and optimize algorithm\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)\n",
    "# Make the learning rate cyclic\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=learning_rate, max_lr=0.01, cycle_momentum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c0f613c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\n",
      "torch.Size([8, 20])\n",
      "Labels:\n",
      "tensor([[ 1,  2,  2,  2,  3,  2,  2,  3,  1,  3,  3,  2,  4,  2,  2,  2,  5,  6,\n",
      "          7,  4],\n",
      "        [ 2,  2,  2,  3,  2,  2,  3,  1,  3,  3,  2,  4,  2,  2,  2,  5,  6,  7,\n",
      "          4,  6],\n",
      "        [ 2,  2,  3,  2,  2,  3,  1,  3,  3,  2,  4,  2,  2,  2,  5,  6,  7,  4,\n",
      "          6,  8],\n",
      "        [ 2,  3,  2,  2,  3,  1,  3,  3,  2,  4,  2,  2,  2,  5,  6,  7,  4,  6,\n",
      "          8,  2],\n",
      "        [ 3,  2,  2,  3,  1,  3,  3,  2,  4,  2,  2,  2,  5,  6,  7,  4,  6,  8,\n",
      "          2,  9],\n",
      "        [ 2,  2,  3,  1,  3,  3,  2,  4,  2,  2,  2,  5,  6,  7,  4,  6,  8,  2,\n",
      "          9, 10],\n",
      "        [ 2,  3,  1,  3,  3,  2,  4,  2,  2,  2,  5,  6,  7,  4,  6,  8,  2,  9,\n",
      "         10,  2],\n",
      "        [ 3,  1,  3,  3,  2,  4,  2,  2,  2,  5,  6,  7,  4,  6,  8,  2,  9, 10,\n",
      "          2,  1]])\n",
      "Output shape:\n",
      "torch.Size([160, 27])\n",
      "Label shape:\n",
      "torch.Size([160])\n",
      "tensor(3.2821, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Test for the real work\n",
    "for i, entry in enumerate(training_loader, 0):\n",
    "    xs, ys = entry[0], entry[1]\n",
    "\n",
    "    print('Input shape:')\n",
    "    print(xs.shape)\n",
    "\n",
    "    outputs = model(xs)\n",
    "    \n",
    "    bs, sl = outputs.shape[:2]\n",
    "    \n",
    "    # Flatten the output\n",
    "    outputs = outputs.view(bs * sl, -1)\n",
    "    \n",
    "    print('Labels:')  \n",
    "    print(ys)\n",
    "    \n",
    "    # Flatten the label\n",
    "    ys = ys.view(-1)\n",
    "    \n",
    "    print('Output shape:')\n",
    "    print(outputs.shape)\n",
    "    \n",
    "    print('Label shape:')\n",
    "    print(ys.shape)\n",
    "    \n",
    "    loss = criterion(outputs, ys)\n",
    "    print(loss)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4d322e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <thead>\n",
       "          <tr>\n",
       "          <th>Epoch</th>\n",
       "          <th>Percentage</th>\n",
       "          <th>Loss</th>\n",
       "          <th>Accuracy</th>\n",
       "          <th>Time</th>\n",
       "          </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mees/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'correct_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-324646fe533b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mcorrect_pred\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mtrain_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'correct_pred' is not defined"
     ]
    }
   ],
   "source": [
    "display(HTML(\n",
    "    \"\"\"<table>\n",
    "        <thead>\n",
    "          <tr>\n",
    "          <th>Epoch</th>\n",
    "          <th>Percentage</th>\n",
    "          <th>Loss</th>\n",
    "          <th>Accuracy</th>\n",
    "          <th>Time</th>\n",
    "          </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "        \"\"\"\n",
    "))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.reset_hidden()\n",
    "\n",
    "    # Initialize loss at 0\n",
    "    epoch_loss = 0.0\n",
    "    iteration_loss = 0.0\n",
    "    train_total = 0\n",
    "    correct_pred = 0\n",
    "\n",
    "    for i, entry in enumerate(training_loader, 0):\n",
    "\n",
    "        if len(entry[0]) == bs:\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            xs, ys = entry[0], entry[1]\n",
    "\n",
    "            outputs = model(xs)\n",
    "\n",
    "            bs, sl = outputs.shape[:2]\n",
    "\n",
    "            # Flatten the output\n",
    "            outputs = outputs.view(bs * sl, -1)\n",
    "\n",
    "            # Flatten the label\n",
    "            ys = ys.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, ys)\n",
    "            \n",
    "            \n",
    "            scheduler.step()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += outputs.shape[0] * loss.item()\n",
    "            iteration_loss += outputs.shape[0] * loss.item()\n",
    "            \n",
    "            # Also calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            correct_pred += (predicted == ys).sum().item()\n",
    "            train_total += ys.size(0)\n",
    "\n",
    "\n",
    "            if i % 1e4 == 0:\n",
    "\n",
    "                round_time = time.time()\n",
    "                duration = round(((round_time - start_time) / 60), 0) # To convert to minutes\n",
    "                start_time = time.time()\n",
    "\n",
    "                perc = round((i / total_train_len * 100), 2)\n",
    "\n",
    "                iteration_loss = round((iteration_loss / 1e4), 2)\n",
    "                \n",
    "                accuracy = np.round((correct_pred / train_total * 100), 2)\n",
    "\n",
    "                display(HTML(\n",
    "                \"\"\"<tr>\n",
    "                <td>{}</td>\n",
    "                <td>{}</td>\n",
    "                <td>{}</td>\n",
    "                <td>{}</td>\n",
    "                </tr>\"\"\".format(str(epoch + 1), str(perc), str(iteration_loss), str(duration))\n",
    "                ))\n",
    "\n",
    "                iteration_loss = 0.0\n",
    "\n",
    "    loss_history.append(epoch_loss)\n",
    "\n",
    "    print(f'Epoch {str(epoch + 1)} Train loss: {str(epoch_loss)}.')\n",
    "\n",
    "display(HTML('</tbody></table>'))        \n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f3265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to train it further later\n",
    "model_file = Path('models/0.5_percent_single_AA_v1.pt')\n",
    "model_file = file_paths / model_file\n",
    "torch.save(model.encoder, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
