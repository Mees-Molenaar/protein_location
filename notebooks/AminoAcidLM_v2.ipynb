{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S11CAvUpf85P"
   },
   "source": [
    "# Pytorch Amin Acid Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3EzLqPjqf6n_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pickle\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8IqrdfcCgPhm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mees/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "HQArlylGRmTL",
    "outputId": "271f5f96-7928-4ee9-c844-723389f28bcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tG_svVhvhRH0",
    "outputId": "5d245b51-0bf3-4c19-cf15-40ab7efdb4ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6c7013f1c8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0V7XsFdhpFrI"
   },
   "source": [
    "Add nice css for my table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "efGtVWAHpFG-",
    "outputId": "717967b3-78d0-4505-a703-7169d97dff45"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "table, th, td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<style>\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzMtjrTjjFG1"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "Rhajs1ZZjH-D",
    "outputId": "5d928282-33e2-4ccd-be5b-b8c68a43034c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Entry name</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P68307</td>\n",
       "      <td>NU3M_BALMU</td>\n",
       "      <td>MNLLLTLLTNTTLALLLVFIAFWLPQLNVYAEKTSPYECGFDPMGS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0CY61</td>\n",
       "      <td>O162_CONBU</td>\n",
       "      <td>MKLTCVLIIAVLFLTAITADDSRDKQVYRAVGLIDKMRRIRASEGC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q0VIL3</td>\n",
       "      <td>OTOMP_DANRE</td>\n",
       "      <td>MDLPGGHLAVVLFLFVLVSMSTENNIIRWCTVSDAEDQKCLDLAGN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1W9I4</td>\n",
       "      <td>NUSB_ACISJ</td>\n",
       "      <td>MTDSTHPTPSARPPRQPRTGTTGTGARKAGSKSGRSRAREFALQAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q8DBX0</td>\n",
       "      <td>OMPU_VIBVU</td>\n",
       "      <td>MKKTLIALSVSAAAVATGVNAAELYNQDGTSLDMGGRAEARLSMKD...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Entry   Entry name                                           Sequence\n",
       "0  P68307   NU3M_BALMU  MNLLLTLLTNTTLALLLVFIAFWLPQLNVYAEKTSPYECGFDPMGS...\n",
       "1  P0CY61   O162_CONBU  MKLTCVLIIAVLFLTAITADDSRDKQVYRAVGLIDKMRRIRASEGC...\n",
       "2  Q0VIL3  OTOMP_DANRE  MDLPGGHLAVVLFLFVLVSMSTENNIIRWCTVSDAEDQKCLDLAGN...\n",
       "3  A1W9I4   NUSB_ACISJ  MTDSTHPTPSARPPRQPRTGTTGTGARKAGSKSGRSRAREFALQAL...\n",
       "4  Q8DBX0   OMPU_VIBVU  MKKTLIALSVSAAAVATGVNAAELYNQDGTSLDMGGRAEARLSMKD..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = Path('/home/mees/Desktop/Machine_Learning/subcellular_location/data/raw/LM_data_2021-03-11.csv')\n",
    "df = pd.read_csv(data_file, sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "tBUVBC9foiCs",
    "outputId": "7477d3fe-6476-4de4-b1ed-dee9697f5a6c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNLLLTLLTNTTLALLLVFIAFWLPQLNVYAEKTSPYECGFDPMGS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MKLTCVLIIAVLFLTAITADDSRDKQVYRAVGLIDKMRRIRASEGC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDLPGGHLAVVLFLFVLVSMSTENNIIRWCTVSDAEDQKCLDLAGN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MTDSTHPTPSARPPRQPRTGTTGTGARKAGSKSGRSRAREFALQAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MKKTLIALSVSAAAVATGVNAAELYNQDGTSLDMGGRAEARLSMKD...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence\n",
       "0  MNLLLTLLTNTTLALLLVFIAFWLPQLNVYAEKTSPYECGFDPMGS...\n",
       "1  MKLTCVLIIAVLFLTAITADDSRDKQVYRAVGLIDKMRRIRASEGC...\n",
       "2  MDLPGGHLAVVLFLFVLVSMSTENNIIRWCTVSDAEDQKCLDLAGN...\n",
       "3  MTDSTHPTPSARPPRQPRTGTTGTGARKAGSKSGRSRAREFALQAL...\n",
       "4  MKKTLIALSVSAAAVATGVNAAELYNQDGTSLDMGGRAEARLSMKD..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['Entry', 'Entry name'], axis = 1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ny7DQRNqg3CE"
   },
   "source": [
    "## Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gQGGZRg_hJsn"
   },
   "outputs": [],
   "source": [
    "# Set-up numpy generator for random numbers\n",
    "random_number_generator = np.random.default_rng(seed=42)\n",
    "KMER_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TgddkaKhhP8D"
   },
   "outputs": [],
   "source": [
    "# Tokenize the protein sequence (or any sequence) in kmers.\n",
    "def tokenize(protein_seqs, kmer_sz):\n",
    "    kmers = set()\n",
    "    # Loop over protein sequences\n",
    "    for protein_seq in protein_seqs:\n",
    "        # Loop over the whole sequence\n",
    "        for i in range(len(protein_seq) - (kmer_sz - 1)):\n",
    "            # Add kmers to the set, thus only unique kmers will remain\n",
    "            kmers.add(protein_seq[i: i + kmer_sz])\n",
    "            \n",
    "    # Map kmers for one hot-encoding\n",
    "    kmer_to_id = dict()\n",
    "    id_to_kmer = dict()\n",
    "    \n",
    "    for ind, kmer in enumerate(kmers):\n",
    "        kmer_to_id[kmer] = ind\n",
    "        id_to_kmer[ind] = kmer\n",
    "        \n",
    "    vocab_sz = len(kmers)\n",
    "    \n",
    "    assert vocab_sz == len(kmer_to_id.keys())\n",
    "    \n",
    "    # Tokenize the protein sequence to integers\n",
    "    tokenized = []\n",
    "    for protein_seq in protein_seqs:\n",
    "        sequence = []\n",
    "        for i in  range(len(protein_seq) - (kmer_sz -1)):\n",
    "            # Convert kmer to integer\n",
    "            kmer = protein_seq[i: i + kmer_sz]\n",
    "            sequence.append(kmer_to_id[kmer])\n",
    "            \n",
    "        tokenized.append(sequence)\n",
    "            \n",
    "    \n",
    "    return tokenized, vocab_sz, kmer_to_id, id_to_kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "v6RSsp1yhoZk"
   },
   "outputs": [],
   "source": [
    "# Tokenize the protein sequence\n",
    "tokenized_seqs, vocab_sz, kmer_to_id, id_to_kmer = tokenize(df['Sequence'], KMER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oDknJFgyhsGe",
    "outputId": "ed52f6aa-c266-4de9-e01d-1b9f9847302d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9317"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0FGm4cBhtvA",
    "outputId": "5a049911-8153-4ea5-b479-6e0ba34f8c67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7409, 2315, 8973, 5157, 1725, 5878, 5157, 6447, 595, 6086]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_seqs[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1UQoexMBhweb"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for seq in tokenized_seqs:\n",
    "    for kmer in seq:\n",
    "        data.append(kmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bFDTL1rh0so"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KThomvMoh3YI"
   },
   "outputs": [],
   "source": [
    "class AminoLMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = torch.Tensor(data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        xs = torch.LongTensor(data[idx: idx + seq_len])\n",
    "        targets = data[idx + 1: idx + seq_len + 1]\n",
    "\n",
    "        ys = []\n",
    "\n",
    "        for target in targets:\n",
    "          y = torch.tensor(target)\n",
    "          ys.append(y)\n",
    "\n",
    "        ys = torch.stack(ys)\n",
    "\n",
    "        ys = ys.to(dev)\n",
    "        xs = xs.to(dev) \n",
    "    \n",
    "        return xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvWU-tRBim7d"
   },
   "source": [
    "## Building the LM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ql5EKTgciqZ5"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "emb_dim = 400 # Embeddding dimension\n",
    "hid_sz = 1150 # Hidden size\n",
    "num_layers = 3 # Number of LSTM layers stacked together\n",
    "seq_len = num_layers\n",
    "bs = 2\n",
    "\n",
    "# Dropout parameters\n",
    "\n",
    "embed_p = 0.1 # Dropout probability on the embedding\n",
    "hidden_p = 0.3 # Dropout probability on hidden-to-hidden weight matrices\n",
    "# Dropout tussen de inputs van de LSTMs moet ik er nog in bouwen\n",
    "input_p = 0.3 # Dropout probablity on the LSTM input between LSTMS\n",
    "weight_p = 0.5 # Dropout probability on LSTM-to-LSTM weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(torch.nn.Module):\n",
    "    \"Apply dropout to an Embedding with probability emp_p\"\n",
    "\n",
    "    def __init__(self, emb_p=0):\n",
    "        super(EmbeddingDropout, self).__init__()\n",
    "        \n",
    "        self.emb_p = emb_p\n",
    "\n",
    "    def forward(self, inp):\n",
    "       \n",
    "        drop = torch.nn.Dropout(self.emb_p)\n",
    "        placeholder = torch.ones((inp.size(0), 1))\n",
    "        mask = drop(placeholder)      \n",
    "        out = inp * mask\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "anf5XZoS6h16"
   },
   "outputs": [],
   "source": [
    "class WeightDropout(torch.nn.Module):\n",
    "  \"Apply dropout to LSTM's hidden-hidden weights\"\n",
    "    \n",
    "  def __init__(self, module, weight_p):\n",
    "    super(WeightDropout, self).__init__()\n",
    "    self.module = module\n",
    "    self.weight_p = weight_p\n",
    "\n",
    "    # Save the name of the layer weights in a list\n",
    "    num_layers = module.num_layers\n",
    "    layer_base_name = 'weight_hh_l'      \n",
    "    self.layer_weights = [layer_base_name + str(i) for i in range(num_layers)]\n",
    "\n",
    "    # Make a copy of the weights in weightname_raw\n",
    "    for weight in self.layer_weights:\n",
    "\n",
    "      w = getattr(self.module, weight)\n",
    "      del module._parameters[weight]\n",
    "      self.module.register_parameter(f'{weight}_raw', torch.nn.Parameter(w))\n",
    "\n",
    "  def _setweights(self):\n",
    "    \"Apply dropout to the raw weights\"\n",
    "    for weight in self.layer_weights:\n",
    "      raw_w = getattr(self.module, f'{weight}_raw')\n",
    "      if self.training:\n",
    "          w = torch.nn.functional.dropout(raw_w, p=self.weight_p)\n",
    "      else:\n",
    "          w = raw_w.clone()\n",
    "      setattr(self.module, weight, w)\n",
    "    \n",
    "  def forward(self, *args):\n",
    "    self._setweights()\n",
    "    return self.module(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "o9dLYlNPi53-"
   },
   "outputs": [],
   "source": [
    "class AWD_LSTM(torch.nn.Module):\n",
    "    def __init__(self, num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz = 1):\n",
    "        super(AWD_LSTM, self).__init__()\n",
    "        \n",
    "        # Embedding with droput\n",
    "        self.encoder = torch.nn.Embedding(vocab_sz, emb_dim)\n",
    "        self.emb_drop = EmbeddingDropout(emb_p=embed_p)\n",
    "\n",
    "        \n",
    "        # Dropouts on the inputs and the hidden layers\n",
    "        self.input_dp = torch.nn.Dropout(p=input_p)\n",
    "        self.hid_dp = torch.nn.Dropout(p=hidden_p)\n",
    "\n",
    "        # Create a list of lstm layers with wieghtdropout\n",
    "        self.lstms = []\n",
    "        for i in range(num_layers):\n",
    "            self.lstms.append(\n",
    "                WeightDropout(nn.LSTM(input_size=emb_dim, hidden_size=hid_sz, num_layers=1), weight_p))\n",
    "        self.lstms = nn.ModuleList(self.lstms)\n",
    "\n",
    "        # Save all variables        \n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_sz = hid_sz\n",
    "        self.hidden_p = hidden_p\n",
    "        self.embed_p = embed_p\n",
    "        self.input_p = input_p\n",
    "        self.weight_p = weight_p\n",
    "        self.batch_sz = batch_sz\n",
    "\n",
    "        # Initialize hidden layers        \n",
    "        self.reset_hidden()\n",
    "        self.last_hiddens = (self.hidden_state, self.cell_state)\n",
    "                \n",
    "    def forward(self, xs):\n",
    "        \"\"\"Forward pass AWD-LSTM\"\"\" \n",
    "        \n",
    "        bs, sl = xs.shape\n",
    "\n",
    "        ys = []\n",
    "        \n",
    "        hiddens = self.last_hiddens\n",
    "\n",
    "        hidden_states = [hiddens]\n",
    "\n",
    "        for i, lstm in enumerate(self.lstms):\n",
    "            \n",
    "          # Embed the input and add dropout to it  \n",
    "          x = xs[:, i]\n",
    "          embed = self.encoder(x)\n",
    "          embed_dp = self.emb_drop(embed)\n",
    "            \n",
    "          # Again add dropout, this feels like doing dropout on dropout, I dont know if it is worth\n",
    "          \n",
    "          input_dp = self.input_dp(embed_dp)\n",
    "\n",
    "          hiddens_dp = []\n",
    "\n",
    "          for hidden_state in hidden_states[i]:\n",
    "            hiddens_dp.append(self.hid_dp(hidden_state))\n",
    "\n",
    "          hiddens_dp = tuple(hiddens_dp)\n",
    "        \n",
    "          output, hiddens = lstm(input_dp.view(1, bs, -1), hiddens_dp) \n",
    "\n",
    "          det_hiddens = []\n",
    "\n",
    "          for hidden in hiddens:\n",
    "            det_hiddens.append(hidden.detach())\n",
    "\n",
    "          det_hiddens = tuple(det_hiddens)\n",
    "\n",
    "          hidden_states.append(det_hiddens)\n",
    "                 \n",
    "          y = output.view(bs, 1, -1)\n",
    "\n",
    "          # Outputs moeten squeezen om de loss toe te kunnen passen, maar moet nog even kijken of de juiste waardes dan wel worden mee genomen\n",
    "\n",
    "          ys.append(y)\n",
    "        \n",
    "\n",
    "        y = torch.stack(ys, dim=0)\n",
    "        \n",
    "        y = y.view(bs, sl, -1)\n",
    "        \n",
    "        self.last_hiddens = hidden_states[-1]\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def reset_hidden(self):\n",
    "        self.hidden_state = torch.zeros((1, self.batch_sz, self.hid_sz)).to(dev)\n",
    "        self.cell_state = torch.zeros((1, self.batch_sz, self.hid_sz)).to(dev)\n",
    "        self.last_hiddens = (self.hidden_state, self.cell_state)\n",
    "    \n",
    "    def freeze_to(self , n):\n",
    "        \n",
    "        params_to_freeze = n * 4 # Since each LSTM layer has 4 parameters\n",
    "        \n",
    "        total_params = len(list(self.parameters()))\n",
    "        \n",
    "        for i, parameter in enumerate(self.parameters()):\n",
    "            parameter.requires_grad = True\n",
    "            \n",
    "            if total_params - i <= params_to_freeze:\n",
    "                parameter.requires_grad = False\n",
    "            \n",
    "            \n",
    "        for name, parameter in self.named_parameters():\n",
    "            print(name)\n",
    "            print(parameter.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinLM(torch.nn.Module):\n",
    "    def __init__(self, num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz = 1):\n",
    "        super(ProteinLM, self).__init__()\n",
    "        \n",
    "        self.encoder = AWD_LSTM(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, \n",
    "                                embed_p, input_p, weight_p, batch_sz=batch_sz)\n",
    "        self.decoder = torch.nn.Linear(hid_sz, vocab_sz)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        encoded = self.encoder(inp)\n",
    "        \n",
    "        y = self.decoder(encoded)\n",
    "        \n",
    "        return y \n",
    "    \n",
    "    def freeze_to(self, n):\n",
    "        self.encoder.freeze_to(n)\n",
    "        \n",
    "    def reset_hidden(self):\n",
    "        self.encoder.reset_hidden()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6D6TH-7k8VV"
   },
   "source": [
    "## Create AWD_LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zDCjaBsKFZO3"
   },
   "outputs": [],
   "source": [
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5_KF4fBAjtyL",
    "outputId": "9cc99e4e-fcb9-4708-bcdb-c757b4e64836"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProteinLM(\n",
       "  (encoder): AWD_LSTM(\n",
       "    (encoder): Embedding(9317, 400)\n",
       "    (emb_drop): EmbeddingDropout()\n",
       "    (input_dp): Dropout(p=0.3, inplace=False)\n",
       "    (hid_dp): Dropout(p=0.3, inplace=False)\n",
       "    (lstms): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=1150, out_features=9317, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ProteinLM(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz=bs)\n",
    "model = model.to(dev)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8hx9Yj2jwvg"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_H4vVpkTj5Np"
   },
   "outputs": [],
   "source": [
    "training_set = AminoLMDataset(data, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "fnOwAHn_j8Wb"
   },
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEnPLliaj_Do",
    "outputId": "a8f7a6b1-3517-442e-8e41-54434f4fbdb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29230676"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_train_len = len(training_loader)\n",
    "total_train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "8rljV58FkC-Y"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "ivU0Dva6kJW-",
    "outputId": "3222d123-2ed6-4d04-a6b5-3ea540f9dcea"
   },
   "outputs": [],
   "source": [
    "# Costfunction and optimize algorithm\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "LbsURPMIqr9X",
    "outputId": "ba41921a-02aa-47ca-fc5c-83d9b85b8f02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\n",
      "torch.Size([2, 3])\n",
      "tensor([[7409, 2315, 8973],\n",
      "        [2315, 8973, 5157]])\n",
      "tensor([[2315, 8973, 5157],\n",
      "        [8973, 5157, 1725]])\n",
      "torch.Size([6, 9317])\n",
      "torch.Size([6])\n",
      "tensor([2315, 8973, 5157, 8973, 5157, 1725])\n",
      "tensor(9.1629, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Test for the real work\n",
    "for i, entry in enumerate(training_loader, 0):\n",
    "    xs, ys = entry[0], entry[1]\n",
    "\n",
    "    print('Input shape:')\n",
    "    print(xs.shape)\n",
    "    \n",
    "    print(xs)\n",
    "\n",
    "    outputs = model(xs)\n",
    "    \n",
    "    bs, sl = outputs.shape[:2]\n",
    "    \n",
    "    # Flatten the output\n",
    "    outputs = outputs.view(bs * sl, -1)\n",
    "    \n",
    "    print(ys)\n",
    "    \n",
    "    # Flatten the label\n",
    "    ys = ys.view(-1)\n",
    "\n",
    "    print(outputs.shape)\n",
    "    print(ys.shape)\n",
    "    \n",
    "    print(ys)\n",
    "    \n",
    "    loss = criterion(outputs, ys)\n",
    "    print(loss)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "29idEGfVkMH_",
    "outputId": "591d185a-8cb7-4dc2-db60-db2f571734cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <thead>\n",
       "          <tr>\n",
       "          <th>Epoch</th>\n",
       "          <th>Percentage</th>\n",
       "          <th>Loss</th>\n",
       "          <th>Time</th>\n",
       "          </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "              <td>1</td>\n",
       "              <td>0.0</td>\n",
       "              <td>0.0</td>\n",
       "              <td>0.0</td>\n",
       "              </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5598b8389454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m                         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshare_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "display(HTML(\n",
    "    \"\"\"<table>\n",
    "        <thead>\n",
    "          <tr>\n",
    "          <th>Epoch</th>\n",
    "          <th>Percentage</th>\n",
    "          <th>Loss</th>\n",
    "          <th>Time</th>\n",
    "          </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "        \"\"\"\n",
    "))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    model.reset_hidden()\n",
    "    \n",
    "    # Initialize loss at 0\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Iterations (in between epoch) loss\n",
    "    iteration_loss = 0.0\n",
    "\n",
    "    for i, entry in enumerate(training_loader, 0):\n",
    "        \n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        xs, ys = entry[0], entry[1]\n",
    "        \n",
    "        outputs = model(xs)\n",
    "        \n",
    "        bs, sl = outputs.shape[:2]\n",
    "    \n",
    "        # Flatten the output\n",
    "        outputs = outputs.view(bs * sl, -1)\n",
    "\n",
    "        # Flatten the label\n",
    "        ys = ys.view(-1)\n",
    "\n",
    "        loss = criterion(outputs, ys)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        iteration_loss += loss.item()\n",
    "        \n",
    "        if i % 1e4 == 0:\n",
    "\n",
    "            round_time = time.time()\n",
    "            duration = round(((round_time - start_time) / 60), 0) # To convert to minutes\n",
    "            \n",
    "            perc = round((i / total_train_len * 100), 2)\n",
    "\n",
    "            iteration_loss = round((iteration_loss / 1e4), 2)\n",
    "\n",
    "            display(HTML(\n",
    "            \"\"\"<tr>\n",
    "              <td>{}</td>\n",
    "              <td>{}</td>\n",
    "              <td>{}</td>\n",
    "              <td>{}</td>\n",
    "              </tr>\"\"\".format(str(epoch + 1), str(perc), str(iteration_loss), str(duration))\n",
    "            ))\n",
    "\n",
    "            iteration_loss = 0.0\n",
    "    \n",
    "    loss_history.append(epoch_loss)\n",
    "    \n",
    "    print(f'Epoch {str(epoch + 1)} Train loss: {str(epoch_loss)}.')\n",
    "\n",
    "display(HTML('</tbody></table>'))\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoxUmcl1q4ep"
   },
   "source": [
    "## Save Model for Training Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_fi4v03syH_",
    "outputId": "7f81c63b-6abf-4ab0-efcf-07b40b30c836"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at content/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "drive.mount('content/', force_remount=True)\n",
    "base = Path('/content/content/My Drive/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6fqe1X8s7fD",
    "outputId": "814deb80-708d-4654-e7ee-b84893bb9779"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/content/content/MyDrive/1_percent_AA_LM_v2.pt')"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '1_percent_AA_LM_v3.pt'\n",
    "file_dir = Path('/content/content/MyDrive/' + filename)\n",
    "file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zsq47uB5tU04"
   },
   "outputs": [],
   "source": [
    "torch.save(model, file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2olAPq3h4PX"
   },
   "source": [
    "## Load Model for Further Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB50rANmh9F5",
    "outputId": "2bc3b41a-71c9-4ba8-b757-904022444efe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at content/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "drive.mount('content/', force_remount=True)\n",
    "base = Path('/content/content/My Drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8P3I4zMiJkt",
    "outputId": "29d4d010-1fa0-4a87-803f-321016ad942d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AWD_LSTM(\n",
       "  (encoder): Embedding(9317, 400)\n",
       "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
       "  (input_dp): Dropout(p=0.3, inplace=False)\n",
       "  (hid_dp): Dropout(p=0.3, inplace=False)\n",
       "  (lstms): ModuleList(\n",
       "    (0): WeightDropout(\n",
       "      (module): LSTM(400, 1150)\n",
       "    )\n",
       "    (1): WeightDropout(\n",
       "      (module): LSTM(400, 1150)\n",
       "    )\n",
       "    (2): WeightDropout(\n",
       "      (module): LSTM(400, 1150)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=1150, out_features=9317, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = Path('/content/content/MyDrive/1_percent_AA_LM_v3.pt')\n",
    "model = torch.load(model_path)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the last LSTM layer\n",
    "model.freeze_to(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQY05bXyjFry"
   },
   "source": [
    "### Train Further with Data of which the location is known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZ8RRvSf4oaA"
   },
   "outputs": [],
   "source": [
    "# Tokenize the protein sequence (or any sequence) in kmers.\n",
    "def tokenize(df, protein_seqs_column, kmer_sz, premade_vocab=False):\n",
    "    \n",
    "    if not premade_vocab:\n",
    "        kmers = set()\n",
    "        # Loop over protein sequences\n",
    "        for protein_seq in df[protein_seqs_column]:\n",
    "            # Loop over the whole sequence\n",
    "            for i in range(len(protein_seq) - (kmer_sz - 1)):\n",
    "                # Add kmers to the set, thus only unique kmers will remain\n",
    "                kmers.add(protein_seq[i: i + kmer_sz])\n",
    "\n",
    "        # Map kmers for one hot-encoding\n",
    "        kmer_to_id = dict()\n",
    "        id_to_kmer = dict()\n",
    "\n",
    "        for ind, kmer in enumerate(kmers):\n",
    "            kmer_to_id[kmer] = ind\n",
    "            id_to_kmer[ind] = kmer\n",
    "\n",
    "        vocab_sz = len(kmers)\n",
    "\n",
    "        assert vocab_sz == len(kmer_to_id.keys())\n",
    "    \n",
    "    else:\n",
    "        kmer_to_id, id_to_kmer = premade_vocab\n",
    "        vocab_sz = len(kmer_to_id)\n",
    "    \n",
    "    # Tokenize the protein sequence to integers\n",
    "    tokenized = []\n",
    "    for i, protein_seq in enumerate(df[protein_seqs_column], 0):\n",
    "        sequence = []\n",
    "        \n",
    "        # If the kmer can't be found these indexes should be deleted\n",
    "        remove_idxs = []\n",
    "        \n",
    "        for i in  range(len(protein_seq) - (kmer_sz -1)):\n",
    "            # Convert kmer to integer\n",
    "            kmer = protein_seq[i: i + kmer_sz]\n",
    "            \n",
    "            # For some reason, some kmers miss. Thus these sequences have to be removed\n",
    "            try:\n",
    "                sequence.append(kmer_to_id[kmer])\n",
    "            except:\n",
    "                remove_idxs.append(i)\n",
    "            \n",
    "        tokenized.append(sequence)\n",
    "            \n",
    "    df['tokenized_seqs'] = tokenized\n",
    "    \n",
    "    df.drop(remove_idxs, inplace=True)\n",
    "    \n",
    "    return df, vocab_sz, kmer_to_id, id_to_kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "gFNPH-PHjLum",
    "outputId": "ac710c68-c691-4ab5-be24-7c5e1ee697f0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasmic vesicle, sec...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Early endosome {ECO:0000...</td>\n",
       "      <td>Endosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton,...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Mitochondrion {ECO:00003...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "      <td>Cell membrane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence  ...       Location\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...  ...      Cytoplasm\n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...  ...       Endosome\n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...  ...      Cytoplasm\n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  ...  Mitochondrion\n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  ...  Cell membrane\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = Path('/content/content/MyDrive/protein_data_2021-04-04.csv')\n",
    "df = pd.read_csv(data_file, sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6QeDY33rjQmk",
    "outputId": "7572b980-c03b-47dd-c3d0-ad69a41180e6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Endosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence       Location\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...      Cytoplasm\n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...       Endosome\n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...      Cytoplasm\n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  Mitochondrion\n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  Cell membrane"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['Subcellular location [CC]'], axis = 1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8Tv84KlmCSl",
    "outputId": "99cf492b-b422-4347-e247-fc0b39e594fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16614"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50R74aizj93G"
   },
   "source": [
    "Data should be tokenized with the same vocab as for the other vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgPJVIF8jl_1"
   },
   "outputs": [],
   "source": [
    "# Load the vocabolary from the Language Model\n",
    "vocab_save_file = '/content/content/MyDrive/LM_vocab.pkl'\n",
    "vocab = pickle.load(open(vocab_save_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcg-95gmmNv1"
   },
   "outputs": [],
   "source": [
    "# Tokenize the protein sequence\n",
    "df, vocab_sz, kmer_to_id, id_to_kmer = tokenize(df, 'Sequence', KMER_SIZE, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "PlGSEoqFjoik",
    "outputId": "ed22ebfc-6409-4c12-ee32-feb3d9e50ae5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Location</th>\n",
       "      <th>tokenized_seqs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[3884, 8570, 3840, 6832, 2277, 2221, 1020, 904...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Endosome</td>\n",
       "      <td>[8772, 7207, 1857, 1688, 5461, 3901, 4899, 424...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[1565, 3797, 2513, 516, 1428, 6558, 6568, 7337...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "      <td>[8939, 2538, 9262, 4438, 2547, 302, 60, 3064, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>[8939, 6897, 6013, 1021, 3034, 2863, 8501, 697...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence  ...                                     tokenized_seqs\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...  ...  [3884, 8570, 3840, 6832, 2277, 2221, 1020, 904...\n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...  ...  [8772, 7207, 1857, 1688, 5461, 3901, 4899, 424...\n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...  ...  [1565, 3797, 2513, 516, 1428, 6558, 6568, 7337...\n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  ...  [8939, 2538, 9262, 4438, 2547, 302, 60, 3064, ...\n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  ...  [8939, 6897, 6013, 1021, 3034, 2863, 8501, 697...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kGJRpioYWbTc",
    "outputId": "f8f6cce0-e2ff-4be4-f039-6cd064695bd1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16614"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXiEy8vumVw4"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for seq in df['tokenized_seqs']:\n",
    "    for kmer in seq:\n",
    "        data.append(kmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ox6DmpAZmaD9"
   },
   "source": [
    "### Train with the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iVIHGtEmeQI"
   },
   "outputs": [],
   "source": [
    "training_set = AminoLMDataset(data, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enzAH5b1moUI"
   },
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRmnqTD3ms4O",
    "outputId": "0ecb27e0-204a-4d57-e374-4c4946452d63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9616156"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_train_len = len(training_loader)\n",
    "total_train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jWzBj-Bm1zj"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tK5cGZhm228"
   },
   "outputs": [],
   "source": [
    "# Costfunction and optimize algorithm\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803
    },
    "id": "6yPlt-lhm7hi",
    "outputId": "a756401b-1cb2-4035-8329-14b95f8681ab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <thead>\n",
       "          <tr>\n",
       "          <th>Epoch</th>\n",
       "          <th>Percentage</th>\n",
       "          <th>Loss</th>\n",
       "          <th>Time</th>\n",
       "          </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:662: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>0.16</td>\n",
       "            <td>7.18</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>0.31</td>\n",
       "            <td>7.68</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>0.47</td>\n",
       "            <td>7.15</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>0.62</td>\n",
       "            <td>7.21</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>0.78</td>\n",
       "            <td>6.92</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>0.94</td>\n",
       "            <td>6.58</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>1.09</td>\n",
       "            <td>6.4</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>1.25</td>\n",
       "            <td>6.23</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>1.4</td>\n",
       "            <td>5.92</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>1.56</td>\n",
       "            <td>5.81</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>1.72</td>\n",
       "            <td>5.76</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>1.87</td>\n",
       "            <td>5.41</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>2.03</td>\n",
       "            <td>5.34</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>2.18</td>\n",
       "            <td>5.32</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>2.34</td>\n",
       "            <td>5.23</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>2.5</td>\n",
       "            <td>5.1</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>2.65</td>\n",
       "            <td>4.91</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>2.81</td>\n",
       "            <td>4.88</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>2.96</td>\n",
       "            <td>4.77</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>3.12</td>\n",
       "            <td>4.6</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>3.28</td>\n",
       "            <td>4.4</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>3.43</td>\n",
       "            <td>4.43</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>3.59</td>\n",
       "            <td>4.47</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>3.74</td>\n",
       "            <td>4.4</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>3.9</td>\n",
       "            <td>4.33</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>4.06</td>\n",
       "            <td>4.17</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "            <td>1</td>\n",
       "            <td>4.21</td>\n",
       "            <td>4.28</td>\n",
       "            <td>6.0</td>\n",
       "            </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-31914b1aafa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m       \u001b[0miteration_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "display(HTML(\n",
    "    \"\"\"<table>\n",
    "        <thead>\n",
    "          <tr>\n",
    "          <th>Epoch</th>\n",
    "          <th>Percentage</th>\n",
    "          <th>Loss</th>\n",
    "          <th>Time</th>\n",
    "          </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "        \"\"\"\n",
    "))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "  start_time = time.time()\n",
    "\n",
    "  model.reset_hidden()\n",
    "  \n",
    "  # Initialize loss at 0\n",
    "  epoch_loss = 0.0\n",
    "  iteration_loss = 0.0\n",
    "  \n",
    "  for i, entry in enumerate(training_loader, 0):\n",
    "      \n",
    "     \n",
    "\n",
    "      model.zero_grad()\n",
    "      \n",
    "      xs, ys = entry[0], entry[1]\n",
    "      \n",
    "      outputs = model(xs.squeeze(0))\n",
    "      loss = criterion(outputs, ys.squeeze(0))\n",
    "      \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      epoch_loss += loss.item()\n",
    "      iteration_loss += loss.item()\n",
    "      \n",
    "      if i % 1.5e4 == 0:\n",
    "          \n",
    "          round_time = time.time()\n",
    "          duration = round(((round_time - start_time) / 60), 0) # To convert to minutes\n",
    "          start_time = time.time()\n",
    "          \n",
    "          perc = round((i / total_train_len * 100), 2)\n",
    "\n",
    "          iteration_loss = round((iteration_loss / 1.5e4), 2)\n",
    "\n",
    "          display(HTML(\n",
    "          \"\"\"<tr>\n",
    "            <td>{}</td>\n",
    "            <td>{}</td>\n",
    "            <td>{}</td>\n",
    "            <td>{}</td>\n",
    "            </tr>\"\"\".format(str(epoch + 1), str(perc), str(iteration_loss), str(duration))\n",
    "          ))\n",
    "\n",
    "          iteration_loss = 0.0\n",
    "  \n",
    "  loss_history.append(epoch_loss)\n",
    "  \n",
    "  print(f'Epoch {str(epoch + 1)} Train loss: {str(epoch_loss)}.')\n",
    "\n",
    "display(HTML('</tbody></table>'))        \n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nFmbZV5ACxeo",
    "outputId": "04ed170d-515f-47b2-bca1-40f34b0ee9c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/content/content/MyDrive/AA_LM_v2.pt')"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'AA_LM_v3.pt'\n",
    "file_dir = Path('/content/content/MyDrive/' + filename)\n",
    "file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dPEOnY6C5R9"
   },
   "outputs": [],
   "source": [
    "torch.save(model, file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyM-TfgDvIFu"
   },
   "source": [
    "> https://arxiv.org/pdf/1801.06146.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nCPcZDGi1CP"
   },
   "source": [
    "## Testing AWD-LSTM output with FASTAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = AWD_LSTM(2, 100, 20, 10, 0.2, 0.02, 0.1, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 10])\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0, 100, (10,5)) # Die 10 en 5 zijn bs en sl\n",
    "x = torch.randint(0, 100, (1,3))\n",
    "r = tst(x)\n",
    "\n",
    "\n",
    "print(tst.last_hiddens[0].shape)\n",
    "\n",
    "tst.eval()\n",
    "tst.reset_hidden()\n",
    "tst(x);\n",
    "tst(x);\n",
    "\n",
    "\n",
    "print(r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\n",
      "torch.Size([1, 3])\n",
      "tensor([[3721,  850, 8386]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 400, got 160000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-10541f6c8332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-0e6c5f7c0fbc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0mhiddens_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiddens_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m           \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-ecde55d249b7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setweights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    178\u001b[0m             raise RuntimeError(\n\u001b[1;32m    179\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m--> 180\u001b[0;31m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 400, got 160000"
     ]
    }
   ],
   "source": [
    "# Test for the real work\n",
    "for i, entry in enumerate(training_loader, 0):\n",
    "    xs, ys = entry[0], entry[1]\n",
    "    \n",
    "    print('Input shape:')\n",
    "    print(xs.shape)\n",
    "    print(xs)\n",
    "\n",
    "    outputs = model(xs.squeeze(0))\n",
    "\n",
    "    print(outputs.shape)\n",
    "    print(ys.shape)\n",
    "\n",
    "    loss = criterion(outputs, ys.squeeze(0))\n",
    "    print(loss)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "teUy1azeki4H"
   ],
   "name": "AminoAcidLM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
