{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41971a24",
   "metadata": {},
   "source": [
    "# Single Amino Acid Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c40748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "from IPython.display import HTML, display\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f199422",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69a17ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc21227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1db411a8d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58872a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "table, th, td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<style>\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd82ad",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad214655",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-40604388b89b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'content/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "drive.mount('content/', force_remount=True)\n",
    "base = Path('/content/content/My Drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248b8f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google drive\n",
    "#file_paths = Path('/content/content/MyDrive/subcellular-location/v2/')\n",
    "\n",
    "# Linux path\n",
    "file_paths = Path('/home/mees/Desktop/Machine_Learning/subcellular_location/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b86ebe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasmic vesicle, sec...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Early endosome {ECO:0000...</td>\n",
       "      <td>Endosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton,...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Mitochondrion {ECO:00003...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "      <td>Cell membrane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence  \\\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...   \n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...   \n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...   \n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...   \n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...   \n",
       "\n",
       "                           Subcellular location [CC]       Location  \n",
       "0  SUBCELLULAR LOCATION: Cytoplasmic vesicle, sec...      Cytoplasm  \n",
       "1  SUBCELLULAR LOCATION: Early endosome {ECO:0000...       Endosome  \n",
       "2  SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton,...      Cytoplasm  \n",
       "3  SUBCELLULAR LOCATION: Mitochondrion {ECO:00003...  Mitochondrion  \n",
       "4  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  Cell membrane  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = Path('data/processed/protein_data_2021-04-04.csv')\n",
    "data_file = file_paths / data_file\n",
    "df = pd.read_csv(data_file, sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5abf8488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Endosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence       Location\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...      Cytoplasm\n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...       Endosome\n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...      Cytoplasm\n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  Mitochondrion\n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  Cell membrane"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['Subcellular location [CC]'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcb375f",
   "metadata": {},
   "source": [
    "### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1365e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(df, protein_seqs_column, kmer_sz, stride=1, eos_token=True):\n",
    "    kmers = set()\n",
    "        \n",
    "    # Map kmers for one-hot encoding\n",
    "    kmer_to_id = dict()\n",
    "    id_to_kmer = dict()\n",
    "\n",
    "    # Loop over the protein sequences\n",
    "    for protein_seq in df[protein_seqs_column]:\n",
    "        # Loop over the sequence and add the amino acid if it is not in kmers set.\n",
    "        seq_len = len(protein_seq)\n",
    "\n",
    "\n",
    "        for i in range(0, seq_len - (kmer_sz - 1), stride):\n",
    "\n",
    "            kmer = protein_seq[i: i + kmer_sz]\n",
    "\n",
    "            if kmer not in list(kmers):\n",
    "                ind = len(kmers)\n",
    "                kmers.add(kmer)\n",
    "\n",
    "                # Also create the dictionary\n",
    "                kmer_to_id[kmer] = ind\n",
    "                id_to_kmer[ind] = kmer\n",
    "\n",
    "    if eos_token:\n",
    "        token = '<EOS>'\n",
    "        ind = len(kmers)\n",
    "        \n",
    "        kmers.add(token)\n",
    "\n",
    "        # Also create the dictionary\n",
    "        kmer_to_id[token] = ind\n",
    "        id_to_kmer[ind] = token\n",
    "\n",
    "    vocab_sz = len(kmers)\n",
    "\n",
    "    assert vocab_sz == len(kmer_to_id.keys())\n",
    "    \n",
    "    return kmer_to_id, id_to_kmer, vocab_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "945d07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df, protein_seqs_column, kmer_sz, stride=1, eos_token=True, premade_vocab=False):\n",
    "    \n",
    "    \n",
    "    # Create the vocabulary\n",
    "    if not premade_vocab:\n",
    "        \n",
    "        kmer_to_id, id_to_kmer, vocab_sz = create_vocab(df, protein_seqs_column, kmer_sz, stride, eos_token)\n",
    "                \n",
    "    else:\n",
    "        kmer_to_id, id_to_kmer = premade_vocab\n",
    "        vocab_sz = len(kmer_to_id.keys())\n",
    "            \n",
    "    # Tokenize the sequences in the DF\n",
    "\n",
    "    tokenized = []\n",
    "    for i, protein_seq in enumerate(df[protein_seqs_column], 0):\n",
    "        sequence = []\n",
    "        \n",
    "        # If the kmer can't be found these indexes should be deleted\n",
    "        remove_idxs = []\n",
    "        \n",
    "        # Loop over the protein sequence\n",
    "        for i in  range(len(protein_seq) - (kmer_sz -1)):\n",
    "            # Convert kmer to integer\n",
    "            kmer = protein_seq[i: i + kmer_sz]\n",
    "            \n",
    "            # For some reason, some kmers miss. Thus these sequences have to be removed\n",
    "            try:\n",
    "                sequence.append(kmer_to_id[kmer])\n",
    "            except:\n",
    "                remove_idxs.append(i)\n",
    "                \n",
    "        if eos_token:\n",
    "            sequence.append(kmer_to_id['<EOS>'])\n",
    "            \n",
    "        tokenized.append(sequence)\n",
    "            \n",
    "    df['tokenized_seqs'] = tokenized\n",
    "    \n",
    "    df.drop(remove_idxs, inplace=True)\n",
    "    \n",
    "    return df, vocab_sz, kmer_to_id, id_to_kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "736ea2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMER_SIZE = 1 # Single Amino Acids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ab100e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabolary from the Language Model\n",
    "vocab_save_file = Path('data/interim/AA_vocab.pkl')\n",
    "vocab_save_file = file_paths / vocab_save_file\n",
    "vocab = pickle.load(open(vocab_save_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "003fa426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the protein sequence\n",
    "df, vocab_sz, kmer_to_id, id_to_kmer = tokenize(df, 'Sequence', KMER_SIZE, premade_vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1391aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Location</th>\n",
       "      <th>tokenized_seqs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[0, 3, 17, 3, 5, 6, 14, 1, 14, 14, 1, 18, 8, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Endosome</td>\n",
       "      <td>[0, 17, 3, 17, 14, 10, 18, 14, 19, 2, 14, 14, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[0, 12, 17, 14, 3, 14, 9, 13, 10, 12, 13, 12, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "      <td>[0, 4, 2, 9, 16, 4, 18, 4, 18, 16, 8, 4, 4, 4,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>[0, 4, 2, 2, 5, 17, 18, 5, 18, 16, 19, 8, 18, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence       Location  \\\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...      Cytoplasm   \n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...       Endosome   \n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...      Cytoplasm   \n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  Mitochondrion   \n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  Cell membrane   \n",
       "\n",
       "                                      tokenized_seqs  \n",
       "0  [0, 3, 17, 3, 5, 6, 14, 1, 14, 14, 1, 18, 8, 0...  \n",
       "1  [0, 17, 3, 17, 14, 10, 18, 14, 19, 2, 14, 14, ...  \n",
       "2  [0, 12, 17, 14, 3, 14, 9, 13, 10, 12, 13, 12, ...  \n",
       "3  [0, 4, 2, 9, 16, 4, 18, 4, 18, 16, 8, 4, 4, 4,...  \n",
       "4  [0, 4, 2, 2, 5, 17, 18, 5, 18, 16, 19, 8, 18, ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82b631d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16614"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b43a2",
   "metadata": {},
   "source": [
    "### Numericalize the labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8b3e8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16614"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some fields are NaN, remove these\n",
    "df.dropna(inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dabda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "\n",
    "for i, label in enumerate(df['Location'].unique(), 0):\n",
    "    label_dict[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eaa663d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalizeClass(df, class_column, label_dict, label_column='Label'):\n",
    "    df[label_column] = df[class_column].map(label_dict)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5f60822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Location</th>\n",
       "      <th>tokenized_seqs</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[0, 3, 17, 3, 5, 6, 14, 1, 14, 14, 1, 18, 8, 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Endosome</td>\n",
       "      <td>[0, 17, 3, 17, 14, 10, 18, 14, 19, 2, 14, 14, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[0, 12, 17, 14, 3, 14, 9, 13, 10, 12, 13, 12, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "      <td>[0, 4, 2, 9, 16, 4, 18, 4, 18, 16, 8, 4, 4, 4,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>[0, 4, 2, 2, 5, 17, 18, 5, 18, 16, 19, 8, 18, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence       Location  \\\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...      Cytoplasm   \n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...       Endosome   \n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...      Cytoplasm   \n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  Mitochondrion   \n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  Cell membrane   \n",
       "\n",
       "                                      tokenized_seqs  Label  \n",
       "0  [0, 3, 17, 3, 5, 6, 14, 1, 14, 14, 1, 18, 8, 0...      0  \n",
       "1  [0, 17, 3, 17, 14, 10, 18, 14, 19, 2, 14, 14, ...      1  \n",
       "2  [0, 12, 17, 14, 3, 14, 9, 13, 10, 12, 13, 12, ...      0  \n",
       "3  [0, 4, 2, 9, 16, 4, 18, 4, 18, 16, 8, 4, 4, 4,...      2  \n",
       "4  [0, 4, 2, 2, 5, 17, 18, 5, 18, 16, 19, 8, 18, ...      3  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = numericalizeClass(df, 'Location', label_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f141afcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cytoplasm': 0,\n",
       " 'Endosome': 1,\n",
       " 'Mitochondrion': 2,\n",
       " 'Cell membrane': 3,\n",
       " 'Nucleus': 4,\n",
       " 'Endoplasmic reticulum': 5,\n",
       " 'Secreted': 6,\n",
       " 'Golgi apparatus': 7,\n",
       " 'Extracellular': 8,\n",
       " 'Peroxisome': 9,\n",
       " 'Lysosome/ Vacuole': 10}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fbaf4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3647.,   95.,  858., 3993., 5462.,  651., 1079.,  316.,  375.,\n",
       "         138.]),\n",
       " array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9c2e850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.,  19.,  16.,   5.,  13.,  30.,  40.,  82.,  99., 163.]),\n",
       " array([  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90., 100.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Length'] = df['tokenized_seqs'].str.len()\n",
    "plt.hist(df['Length'], range=(0, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ee798c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16592"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['Length'] > 20]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eeb73a",
   "metadata": {},
   "source": [
    "This only removed about 60 entries.\n",
    "Now, to address class imbalance. Create a weighted sampler. However, the current weight labels are not correct. For each entry there should be a weight attached.\n",
    "\n",
    "Unfortunately, adding weight labeling to the cross entropy loss doesn't work. So, for each label we are selecteing 100 randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f591eda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3647, 1: 95, 2: 858, 3: 3972, 4: 5462, 5: 651, 6: 1078, 7: 316, 8: 375, 9: 53, 10: 85}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.5494927337537705,\n",
       " 174.6526315789474,\n",
       " 19.337995337995338,\n",
       " 4.177240684793555,\n",
       " 3.0377151226656904,\n",
       " 25.48694316436252,\n",
       " 15.391465677179962,\n",
       " 52.506329113924046,\n",
       " 44.245333333333335,\n",
       " 313.05660377358487,\n",
       " 195.20000000000002]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_of_labels = dict(df['Label'].value_counts(sort=False))\n",
    "\n",
    "weight_labels = []\n",
    "\n",
    "print(total_of_labels)\n",
    "\n",
    "for total_of_label in total_of_labels.values():\n",
    "    weight = 1 / (total_of_label / len(df))\n",
    "    weight_labels.append(weight)\n",
    "    \n",
    "weight_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a7e67",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67ead11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AminoClassifierDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, num_classes, bs=1):\n",
    "        self.df = df\n",
    "        self.num_classes = num_classes \n",
    "        self.batch_sz = bs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        x = torch.LongTensor(self.df.iloc[idx]['tokenized_seqs'])\n",
    "        x = x.to(dev) \n",
    "        \n",
    "        y = torch.LongTensor([self.df.iloc[idx]['Label']])\n",
    "        y = y.to(dev)\n",
    "    \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153e47c4",
   "metadata": {},
   "source": [
    "## The classifier\n",
    "\n",
    "Creating the complete classifier from its different parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2206013",
   "metadata": {},
   "source": [
    "### AWD-LSTM\n",
    "\n",
    "We start with the encoding, the AWD-LSTM as was used to train the language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79dca8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(torch.nn.Module):\n",
    "    \"Apply dropout to an Embedding with probability emp_p\"\n",
    "\n",
    "    def __init__(self, emb_p=0):\n",
    "        super(EmbeddingDropout, self).__init__()\n",
    "        \n",
    "        self.emb_p = emb_p\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        bs, sl = inp.shape[:2]\n",
    "        \n",
    "        drop = torch.nn.Dropout(self.emb_p)\n",
    "        placeholder = torch.ones((bs, sl, 1)).to(dev)\n",
    "        mask = drop(placeholder)      \n",
    "        out = inp * mask\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "edb5a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWD_LSTM(torch.nn.Module):\n",
    "    def __init__(self, num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz = 1, pad_token=False):\n",
    "        super(AWD_LSTM, self).__init__()\n",
    "        \n",
    "        if pad_token:\n",
    "            print(vocab_sz)\n",
    "            vocab_sz += 1\n",
    "        \n",
    "        # Embedding with dropout\n",
    "        self.encoder = nn.Embedding(vocab_sz, emb_dim)\n",
    "        self.emb_drop = EmbeddingDropout(emb_p=embed_p)\n",
    "\n",
    "        \n",
    "        # Dropouts on the inputs and the hidden layers\n",
    "        self.hid_dp = torch.nn.Dropout(p=hidden_p)\n",
    "        \n",
    "        self.lstms = nn.LSTM(emb_dim, hid_sz, num_layers, batch_first = True, dropout=input_p)\n",
    "\n",
    "        \n",
    "        # Save all variables        \n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_sz = hid_sz\n",
    "        self.hidden_p = hidden_p\n",
    "        self.embed_p = embed_p\n",
    "        self.input_p = input_p\n",
    "        self.weight_p = weight_p\n",
    "        self.batch_sz = batch_sz\n",
    "\n",
    "        # Initialize hidden layers        \n",
    "        self.reset_hidden()\n",
    "        self.last_hiddens = (self.hidden_state, self.cell_state)\n",
    "                \n",
    "    def forward(self, xs):\n",
    "        \"\"\"Forward pass AWD-LSTM\"\"\" \n",
    "        \n",
    "        bs, sl = xs.shape\n",
    "\n",
    "        ys = []\n",
    "        \n",
    "        hidden_states = []\n",
    "        \n",
    "        hiddens = self.last_hiddens\n",
    "        \n",
    "\n",
    "        embed = self.encoder(xs)\n",
    "        embed_dp = self.emb_drop(embed)\n",
    "\n",
    "        \n",
    "        inp = embed_dp.view(bs, sl, -1)\n",
    "        \n",
    "        # Dropout on hidden layers\n",
    "        hiddens_dp = []\n",
    "        for hidden_state in hiddens:\n",
    "            hiddens_dp.append(self.hid_dp(hidden_state))\n",
    "            \n",
    "        hiddens_dp = tuple(hiddens_dp)\n",
    "        \n",
    "        output, (h, c) = self.lstms(embed_dp.view(bs, sl, -1), hiddens_dp)\n",
    "        \n",
    "        self.last_hiddens = (h.detach(), c.detach())\n",
    "        \n",
    "        return output, self.last_hiddens\n",
    "    \n",
    "    def reset_hidden(self):\n",
    "        self.hidden_state = torch.zeros((self.num_layers, self.batch_sz, self.hid_sz)).to(dev)\n",
    "        self.cell_state = torch.zeros((self.num_layers, self.batch_sz, self.hid_sz)).to(dev)\n",
    "        self.last_hiddens = (self.hidden_state, self.cell_state)\n",
    "    \n",
    "    def freeze_to(self , n):\n",
    "        \n",
    "        params_to_freeze = n * 4 + 1 # Since each LSTM layer has 4 parameters plus 1 to also freeze the encoder\n",
    "        \n",
    "        total_params = len(list(self.parameters()))\n",
    "        \n",
    "        for i, parameter in enumerate(self.parameters()):\n",
    "            parameter.requires_grad = True\n",
    "            \n",
    "            if i < params_to_freeze:\n",
    "                parameter.requires_grad = False\n",
    "            \n",
    "            \n",
    "        for name, parameter in self.named_parameters():\n",
    "            print(name)\n",
    "            print(parameter.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7bbc90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AALM(nn.Module):\n",
    "    def __init__(self, num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz = 1, pad_token=False):\n",
    "        super(AALM, self).__init__()\n",
    "        \n",
    "        self.encoder = AWD_LSTM(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, \n",
    "                                embed_p, input_p, weight_p, batch_sz=batch_sz)\n",
    "        self.decoder = nn.Linear(hid_sz, vocab_sz)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        encoded = self.encoder(inp)\n",
    "        \n",
    "        y = self.decoder(encoded)\n",
    "        \n",
    "        return y \n",
    "    \n",
    "    def freeze_to(self, n):\n",
    "        self.encoder.freeze_to(n)\n",
    "        \n",
    "    def reset_hidden(self):\n",
    "        self.encoder.reset_hidden()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13abe7e",
   "metadata": {},
   "source": [
    "## SentenceEncoder\n",
    "\n",
    "This part encodes the whole sequence in seq_lengths using the pretrained AWD-LSTM language model.\n",
    "\n",
    "We use the Identity class to replace the decoder in the original AWD-LSTM. \n",
    "\n",
    "The AWD-LSTM should not be trained, therefore, it is in between torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "841b8b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0ab1ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, model, pad_token):\n",
    "        super(SentenceEncoder, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.model = model\n",
    "        self.pad_token = pad_token\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        with torch.no_grad():      \n",
    "            \n",
    "            # First element is batch size, second element is the sequence\n",
    "            inp_size = inp.shape[1]\n",
    "            bs = inp.shape[0]\n",
    "            \n",
    "            # Padded parts should not be taken into account and these padded sequences cannot go trough the embedding\n",
    "            padded = inp == self.pad_token\n",
    "            \n",
    "            # It is nicer to add padding\n",
    "            max_iterations = int(inp_size / self.seq_len)\n",
    "\n",
    "            hidden_state_outputs = []\n",
    "            cell_state_outputs = []\n",
    "            \n",
    "            # Save masked output for the pooling calculations\n",
    "            masked_outputs = []\n",
    "            corr_size = 0\n",
    "            \n",
    "            for i in range(0, self.seq_len * max_iterations, self.seq_len):\n",
    "\n",
    "                # Calculate the corrected batch size, meaning that a sequence only consisting of \n",
    "                # the padding_token will be not put in the LSTM\n",
    "                total_padded = padded[:,i : i + self.seq_len].sum(dim = 1) > 0\n",
    "                corr_bs = (total_padded == False).sum()\n",
    "                \n",
    "                _, hidden = self.model(inp[:corr_bs, i: i + self.seq_len])\n",
    "                \n",
    "                print(hidden[0].shape)\n",
    "                break\n",
    "                \n",
    "                for states in hidden:\n",
    "                    hidden_state_output = states[0]\n",
    "                    cell_state_output = states[1]\n",
    "\n",
    "                    # In order to concate the hidden outputs we have to correct for the decrease in batch size\n",
    "                    if corr_bs != bs:\n",
    "                        corr_size = bs - corr_bs\n",
    "                        corr_hidden = torch.zeros(1, corr_size, hidden_state_output.shape[2])\n",
    "                        corr_mask = torch.zeros(1, corr_size)\n",
    "                        \n",
    "                        hidden_state_output = torch.cat([hidden_state_output, corr_hidden], dim = 1)\n",
    "                        cell_state_output = torch.cat([cell_state_output, corr_hidden], dim = 1) \n",
    "                    \n",
    "                    \n",
    "                    # If no corrections for sequence length is made the mask will be one (there is no mask)\n",
    "                    if corr_size == 0:\n",
    "                        mask = torch.ones(1, bs)\n",
    "                    # If there are corrections made, mask will be a combination of ones for the sequence in the batch\n",
    "                    # that is not corrected and zeros for the sequence that is corrected \n",
    "                    else:\n",
    "                        unmasked = bs - corr_size\n",
    "                        mask = torch.ones(1, unmasked)\n",
    "                        \n",
    "                        mask = torch.cat([mask, corr_mask], dim = 1)\n",
    "                        \n",
    "                        \n",
    "                    masked_outputs.append(mask)\n",
    "                    hidden_state_outputs.append(hidden_state_output)\n",
    "                    cell_state_outputs.append(cell_state_output)\n",
    "                \n",
    "                   \n",
    "            hidden_state_outputs = torch.cat(hidden_state_outputs, dim = 0)\n",
    "            cell_state_outputs = torch.cat(cell_state_outputs, dim = 0)\n",
    "            masked_outputs = torch.cat(masked_outputs, dim = 0)\n",
    "\n",
    "            return (hidden_state_outputs, cell_state_outputs), masked_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f157ce",
   "metadata": {},
   "source": [
    "## PoolingLinearClassifier\n",
    "\n",
    "The encoded sequence is needed to be pooled, otherwise the model can not use the information for classification.\n",
    "\n",
    "Then, the data is normalized using batchnorm. Dropout is applied to prevent overfitting. And linear layers with a ReLU activiation are used to classify the pooled protein data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7c733ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_encoded_sequence(output, masked):\n",
    "    r\"\"\"Pool the encoded AA sequence and \n",
    "    return one vector with the max_pool and avg_pool concatenated\"\"\"\n",
    "    \n",
    "    hidden_states = output[0]\n",
    "    #cell_states = output[1]\n",
    "    \n",
    "    mask = masked == 0\n",
    "    \n",
    "    sl, bs = hidden_states.shape[:2]\n",
    "    \n",
    "    \n",
    "    lens = hidden_states.shape[0] - mask.long().sum(dim = 0)\n",
    "    last_lens = mask[-sl:,:].long().sum(dim = 0).max() - 1\n",
    "\n",
    "    last_hidden_state = hidden_states[-last_lens, :, :]\n",
    "    #last_cell_state = cell_states[-1, :, :]\n",
    "    \n",
    "    hidden_state_avg = hidden_states.masked_fill_(mask[:, :, None], 0).sum(dim = 0)\n",
    "    hidden_state_avg.div_(lens[:, None])\n",
    "    #cell_state_avg = cell_states.sum(dim=0) / sl\n",
    "    \n",
    "\n",
    "    hidden_state_max = hidden_states.masked_fill_(mask[:, :, None], -float('inf')).max(dim = 0)[0]\n",
    "    #cell_state_max = hidden_states.max(dim=0)[0]\n",
    "\n",
    "    #x = torch.cat([last_hidden_state, last_cell_state, hidden_state_avg, cell_state_avg, \\\n",
    "    #              hidden_state_max, cell_state_max], 0)\n",
    "\n",
    "    x = torch.cat([last_hidden_state, hidden_state_avg, hidden_state_max], 1)\n",
    "    \n",
    "    x = x.view(bs, -1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "37833b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingLinearClassifier(torch.nn.Module):\n",
    "    r\"\"\"Pool the outputs from the encoder and classify it.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, batch_sz):\n",
    "        super(PoolingLinearClassifier, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.batch_sz = batch_sz\n",
    "        \n",
    "        if batch_sz > 1:\n",
    "            \n",
    "            self.layers = nn.Sequential(\n",
    "                nn.BatchNorm1d(1150 * 3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Dropout(p=0.2, inplace=False),\n",
    "                nn.Linear(in_features=1150 * 3, out_features=50, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Dropout(p=0.1, inplace=False),\n",
    "                nn.Linear(in_features=50, out_features=num_classes, bias=True)\n",
    "            )\n",
    "        else:\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Dropout(p=0.2, inplace=False),\n",
    "                nn.Linear(in_features=1150 * 3, out_features=50, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=0.1, inplace=False),\n",
    "                nn.Linear(in_features=50, out_features=num_classes, bias=True)\n",
    "            )\n",
    "            \n",
    "        \n",
    "    \n",
    "    def forward(self, inp):\n",
    "        output_encoder, padded = inp\n",
    "        pooled_output = pool_encoded_sequence(output_encoder, padded)\n",
    "        y = self.layers(pooled_output)\n",
    "        \n",
    "        return y        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3a37729a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(label_dict)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4923d",
   "metadata": {},
   "source": [
    "## Combine everything in the protein classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0f167f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class proteinClassifier(torch.nn.Module):\n",
    "    r\"\"\"The complete protein classifier\"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, seq_len, num_classes, batch_size, pretrained_file=False):\n",
    "        super(proteinClassifier, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_sz = hid_sz\n",
    "        self.hidden_p = hidden_p\n",
    "        self.embed_p = embed_p\n",
    "        self.input_p = input_p\n",
    "        self.seq_len = seq_len\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        pad_token = vocab_sz\n",
    "        \n",
    "        if pad_token:\n",
    "        \n",
    "            language_model = AALM(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, \n",
    "                                  embed_p, input_p, weight_p, batch_sz=batch_size, pad_token=True)\n",
    "\n",
    "        else:\n",
    "            language_model = AALM(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, \n",
    "                                      embed_p, input_p, weight_p, batch_sz=batch_size)\n",
    "        \n",
    "        if pretrained_file:\n",
    "            language_mode = torch.load(pretrained_file, map_location=torch.device(dev))\n",
    "        \n",
    "        language_model.decoder = Identity()\n",
    "        \n",
    "        encoder = SentenceEncoder(seq_len, language_model, pad_token)\n",
    "        \n",
    "        classifier = PoolingLinearClassifier(num_classes, self.batch_size)\n",
    "        \n",
    "        self.layers = nn.Sequential(encoder, classifier)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        y = self.layers(inp)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99702db1",
   "metadata": {},
   "source": [
    "## Model hyperparameters and train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "063fe84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "emb_dim = 10 # Embeddding dimension\n",
    "hid_sz = 1150 # Hidden size\n",
    "num_layers = 20 # Number of LSTM layers stacked together\n",
    "seq_len = num_layers # Based on paper mentioned above\n",
    "batch_size = 2 \n",
    "\n",
    "# Dropout parameters\n",
    "\n",
    "embed_p = 0.1 # Dropout probability on the embedding\n",
    "hidden_p = 0.3 # Dropout probability on hidden-to-hidden weight matrices\n",
    "input_p = 0.3 # Dropout probablity on the LSTM input between LSTMS\n",
    "weight_p = 0.5 # Dropout probability on LSTM-to-LSTM weight matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b942d2c",
   "metadata": {},
   "source": [
    "## Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "57c38d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = Path('models/1.27_percent_single_AA_v2.pt')\n",
    "pretrained_model = file_paths / pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4cd1bdd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "proteinClassifier(\n",
       "  (layers): Sequential(\n",
       "    (0): SentenceEncoder(\n",
       "      (model): AALM(\n",
       "        (encoder): AWD_LSTM(\n",
       "          (encoder): Embedding(26, 10)\n",
       "          (emb_drop): EmbeddingDropout()\n",
       "          (hid_dp): Dropout(p=0.3, inplace=False)\n",
       "          (lstms): LSTM(10, 1150, num_layers=20, batch_first=True, dropout=0.3)\n",
       "        )\n",
       "        (decoder): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): PoolingLinearClassifier(\n",
       "      (layers): Sequential(\n",
       "        (0): BatchNorm1d(3450, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Dropout(p=0.2, inplace=False)\n",
       "        (2): Linear(in_features=3450, out_features=50, bias=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "        (6): Linear(in_features=50, out_features=11, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = proteinClassifier(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p,\n",
    "                         embed_p, input_p, weight_p, seq_len, num_classes, batch_size, pretrained_model)\n",
    "model.to(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77247dd3",
   "metadata": {},
   "source": [
    "## Learning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "767fb943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "adam_betas = (0.7, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a3764663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Costfunction and optimize algorithm\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.FloatTensor(weight_labels))\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=adam_betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9691d8d",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Since the data has different length, therefore I use the pack_sequence in the collate fn.\n",
    "https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9f71bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deze functie voegt padding toe zodat ik een batch kan runnen want een batch moet gelijke grote hebben.\n",
    "# En sorteert ook op grote\n",
    "\n",
    "def collate_fn_padd(batch):\n",
    "    \n",
    "    padding_value = vocab_sz\n",
    "\n",
    "    seqs = []\n",
    "    ys = []\n",
    "    \n",
    "    for seq in batch:\n",
    "        seqs.append(seq[0])\n",
    "        ys.append(seq[1])\n",
    "\n",
    "    # Add padding to the sequences\n",
    "    padded = pad_sequence(seqs, batch_first=True, padding_value=padding_value)\n",
    "    \n",
    "    ys = torch.stack(ys, dim=0)\n",
    "    \n",
    "    \n",
    "    # Calculate the real size so that the padded sequences can be sorted\n",
    "    masked = padded == padding_value    \n",
    "    real_seq_size = padded.size(1) - masked.sum(dim=1)\n",
    "    _, indices = torch.sort(real_seq_size, descending=True)\n",
    "    \n",
    "    sorted_masked = torch.zeros_like(masked)\n",
    "    sorted_padded_sequences = torch.zeros_like(padded)\n",
    "    \n",
    "    for i, ind in enumerate(indices, 0):\n",
    "        sorted_padded_sequences[i] = padded[ind]\n",
    "        sorted_masked[i] = masked[ind]\n",
    "    \n",
    "    \"\"\" \n",
    "    # It is nicer to add padding\n",
    "    max_iterations = int(sorted_padded_sequences.size(1) / 50)\n",
    "    \n",
    "    \n",
    "    # Testen hoe ik uit kan vogelen hoe die batch sizes kunnen worden aangepast\n",
    "    for i in range(0, max_iterations * 50, 50):\n",
    "        print('Possible input')\n",
    "        inp = sorted_padded_sequences[:, i:i + 50]\n",
    "        total_mask = sorted_masked[:, i:i+50].sum(dim=1) == 50\n",
    "        true_bs = (total_mask == False).sum()\n",
    "        print(total_mask)\n",
    "        print(true_bs)\n",
    "    \"\"\" \n",
    "    \n",
    "    return sorted_padded_sequences, ys   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4da82c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data in the DataSet\n",
    "AADataset = AminoClassifierDataset(df, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4870a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in an 80/20% split for training and testing\n",
    "data_len = len(AADataset)\n",
    "train_part = int(0.8 * data_len)\n",
    "test_part = data_len - train_part\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(AADataset, [train_part, test_part])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "abbc511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_padd)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_padd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9813ab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 808])\n",
      "torch.Size([2, 1])\n",
      "Batch size is:\n",
      "2\n",
      "torch.Size([20, 2, 1150])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::_cat.  This usually means that this function requires a non-empty list of Tensors.  Available functions are [CPU, QuantizedCPU, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradNestedTensor, UNKNOWN_TENSOR_TYPE_ID, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:5925 [kernel]\nQuantizedCPU: registered at /pytorch/build/aten/src/ATen/RegisterQuantizedCPU.cpp:641 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nAutogradOther: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nAutogradCPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nAutogradCUDA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nAutogradXLA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nAutogradNestedTensor: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nAutogradPrivateUse1: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nAutogradPrivateUse2: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nAutogradPrivateUse3: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nTracer: registered at /pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:10525 [kernel]\nAutocast: registered at /pytorch/aten/src/ATen/autocast_mode.cpp:254 [kernel]\nBatched: registered at /pytorch/aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-9a3c86f9c1c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Flatten the label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-143-4a79cc3d0dfd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-139-9974412ae138>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mhidden_state_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mcell_state_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_state_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mmasked_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::_cat.  This usually means that this function requires a non-empty list of Tensors.  Available functions are [CPU, QuantizedCPU, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradNestedTensor, UNKNOWN_TENSOR_TYPE_ID, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:5925 [kernel]\nQuantizedCPU: registered at /pytorch/build/aten/src/ATen/RegisterQuantizedCPU.cpp:641 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nAutogradOther: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nAutogradCPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nAutogradCUDA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nAutogradXLA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nAutogradNestedTensor: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nAutogradPrivateUse1: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nAutogradPrivateUse2: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nAutogradPrivateUse3: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:9122 [autograd kernel]\nTracer: registered at /pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:10525 [kernel]\nAutocast: registered at /pytorch/aten/src/ATen/autocast_mode.cpp:254 [kernel]\nBatched: registered at /pytorch/aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "# Test for the real work\n",
    "for i, entry in enumerate(train_loader, 0):\n",
    "    xs, ys = entry[0], entry[1]\n",
    "    \n",
    "    print(xs.shape)\n",
    "    print(ys.shape)\n",
    "    \n",
    "    outputs = model(xs)\n",
    "    \n",
    "    # Flatten the label\n",
    "    ys = ys.view(-1)\n",
    "\n",
    "    print(outputs.shape)\n",
    "    print(outputs)\n",
    "    print(ys.shape)\n",
    "    \n",
    "    print(ys)\n",
    "    \n",
    "    loss = criterion(outputs, ys)\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63983a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc7bf8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
