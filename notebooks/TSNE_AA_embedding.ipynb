{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fd8ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86e03f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cdbd61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasmic vesicle, sec...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Early endosome {ECO:0000...</td>\n",
       "      <td>Endosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton,...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Mitochondrion {ECO:00003...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "      <td>Cell membrane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence  \\\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...   \n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...   \n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...   \n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...   \n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...   \n",
       "\n",
       "                           Subcellular location [CC]       Location  \n",
       "0  SUBCELLULAR LOCATION: Cytoplasmic vesicle, sec...      Cytoplasm  \n",
       "1  SUBCELLULAR LOCATION: Early endosome {ECO:0000...       Endosome  \n",
       "2  SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton,...      Cytoplasm  \n",
       "3  SUBCELLULAR LOCATION: Mitochondrion {ECO:00003...  Mitochondrion  \n",
       "4  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  Cell membrane  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = Path('data/processed/protein_data_2021-04-04.csv')\n",
    "data_file = file_paths / data_file\n",
    "df = pd.read_csv(data_file, sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c01e28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Endosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence       Location\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...      Cytoplasm\n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...       Endosome\n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...      Cytoplasm\n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  Mitochondrion\n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  Cell membrane"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['Subcellular location [CC]'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ddcd90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(df, protein_seqs_column, kmer_sz, stride=1, eos_token=True):\n",
    "    kmers = set()\n",
    "        \n",
    "    # Map kmers for one-hot encoding\n",
    "    kmer_to_id = dict()\n",
    "    id_to_kmer = dict()\n",
    "\n",
    "    # Loop over the protein sequences\n",
    "    for protein_seq in df[protein_seqs_column]:\n",
    "        # Loop over the sequence and add the amino acid if it is not in kmers set.\n",
    "        seq_len = len(protein_seq)\n",
    "\n",
    "\n",
    "        for i in range(0, seq_len - (kmer_sz - 1), stride):\n",
    "\n",
    "            kmer = protein_seq[i: i + kmer_sz]\n",
    "\n",
    "            if kmer not in list(kmers):\n",
    "                ind = len(kmers)\n",
    "                kmers.add(kmer)\n",
    "\n",
    "                # Also create the dictionary\n",
    "                kmer_to_id[kmer] = ind\n",
    "                id_to_kmer[ind] = kmer\n",
    "\n",
    "    if eos_token:\n",
    "        token = '<EOS>'\n",
    "        ind = len(kmers)\n",
    "        \n",
    "        kmers.add(token)\n",
    "\n",
    "        # Also create the dictionary\n",
    "        kmer_to_id[token] = ind\n",
    "        id_to_kmer[ind] = token\n",
    "\n",
    "    vocab_sz = len(kmers)\n",
    "\n",
    "    assert vocab_sz == len(kmer_to_id.keys())\n",
    "    \n",
    "    return kmer_to_id, id_to_kmer, vocab_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9573b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df, protein_seqs_column, kmer_sz, stride=1, eos_token=True, premade_vocab=False):\n",
    "    \n",
    "    \n",
    "    # Create the vocabulary\n",
    "    if not premade_vocab:\n",
    "        \n",
    "        kmer_to_id, id_to_kmer, vocab_sz = create_vocab(df, protein_seqs_column, kmer_sz, stride, eos_token)\n",
    "                \n",
    "    else:\n",
    "        kmer_to_id, id_to_kmer = premade_vocab\n",
    "        vocab_sz = len(kmer_to_id.keys())\n",
    "            \n",
    "    # Tokenize the sequences in the DF\n",
    "\n",
    "    tokenized = []\n",
    "    for i, protein_seq in enumerate(df[protein_seqs_column], 0):\n",
    "        sequence = []\n",
    "        \n",
    "        # If the kmer can't be found these indexes should be deleted\n",
    "        remove_idxs = []\n",
    "        \n",
    "        # Loop over the protein sequence\n",
    "        for i in  range(len(protein_seq) - (kmer_sz -1)):\n",
    "            # Convert kmer to integer\n",
    "            kmer = protein_seq[i: i + kmer_sz]\n",
    "            \n",
    "            # For some reason, some kmers miss. Thus these sequences have to be removed\n",
    "            try:\n",
    "                sequence.append(kmer_to_id[kmer])\n",
    "            except:\n",
    "                remove_idxs.append(i)\n",
    "                \n",
    "        if eos_token:\n",
    "            sequence.append(kmer_to_id['<EOS>'])\n",
    "            \n",
    "        tokenized.append(sequence)\n",
    "            \n",
    "    df['tokenized_seqs'] = tokenized\n",
    "    \n",
    "    df.drop(remove_idxs, inplace=True)\n",
    "    \n",
    "    return df, vocab_sz, kmer_to_id, id_to_kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "117a91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabolary from the Language Model\n",
    "vocab_save_file = Path('data/interim/AA_vocab.pkl')\n",
    "vocab_save_file = file_paths / vocab_save_file\n",
    "vocab = pickle.load(open(vocab_save_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ea8f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMER_SIZE = 1 # Single Amino Acids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7e81319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the protein sequence\n",
    "df, vocab_sz, kmer_to_id, id_to_kmer = tokenize(df, 'Sequence', KMER_SIZE, premade_vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcb5fbf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Location</th>\n",
       "      <th>tokenized_seqs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[0, 3, 17, 3, 5, 6, 14, 1, 14, 14, 1, 18, 8, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Endosome</td>\n",
       "      <td>[0, 17, 3, 17, 14, 10, 18, 14, 19, 2, 14, 14, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[0, 12, 17, 14, 3, 14, 9, 13, 10, 12, 13, 12, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "      <td>[0, 4, 2, 9, 16, 4, 18, 4, 18, 16, 8, 4, 4, 4,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>[0, 4, 2, 2, 5, 17, 18, 5, 18, 16, 19, 8, 18, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence       Location  \\\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...      Cytoplasm   \n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...       Endosome   \n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...      Cytoplasm   \n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  Mitochondrion   \n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  Cell membrane   \n",
       "\n",
       "                                      tokenized_seqs  \n",
       "0  [0, 3, 17, 3, 5, 6, 14, 1, 14, 14, 1, 18, 8, 0...  \n",
       "1  [0, 17, 3, 17, 14, 10, 18, 14, 19, 2, 14, 14, ...  \n",
       "2  [0, 12, 17, 14, 3, 14, 9, 13, 10, 12, 13, 12, ...  \n",
       "3  [0, 4, 2, 9, 16, 4, 18, 4, 18, 16, 8, 4, 4, 4,...  \n",
       "4  [0, 4, 2, 2, 5, 17, 18, 5, 18, 16, 19, 8, 18, ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25891e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(torch.nn.Module):\n",
    "    \"Apply dropout to an Embedding with probability emp_p\"\n",
    "\n",
    "    def __init__(self, emb_p=0):\n",
    "        super(EmbeddingDropout, self).__init__()\n",
    "        \n",
    "        self.emb_p = emb_p\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        bs, sl = inp.shape[:2]\n",
    "        \n",
    "        drop = torch.nn.Dropout(self.emb_p)\n",
    "        placeholder = torch.ones((bs, sl, 1)).to(dev)\n",
    "        mask = drop(placeholder)      \n",
    "        out = inp * mask\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0b6d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWD_LSTM(torch.nn.Module):\n",
    "    def __init__(self, num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz = 1, pad_token=False):\n",
    "        super(AWD_LSTM, self).__init__()\n",
    "        \n",
    "        # Embedding with dropout\n",
    "        if pad_token:\n",
    "            self.encoder = nn.Embedding(vocab_sz, emb_dim, padding_idx=pad_token)\n",
    "        else:\n",
    "            self.encoder = nn.Embedding(vocab_sz, emb_dim, padding_idx=pad_token)\n",
    "            \n",
    "        self.emb_drop = EmbeddingDropout(emb_p=embed_p)\n",
    "\n",
    "        \n",
    "        # Dropouts on the inputs and the hidden layers\n",
    "        self.hid_dp = torch.nn.Dropout(p=hidden_p)\n",
    "        \n",
    "        self.lstms = nn.LSTM(emb_dim, hid_sz, num_layers, batch_first = True, dropout=input_p)\n",
    "\n",
    "        \n",
    "        # Save all variables        \n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_sz = hid_sz\n",
    "        self.hidden_p = hidden_p\n",
    "        self.embed_p = embed_p\n",
    "        self.input_p = input_p\n",
    "        self.weight_p = weight_p\n",
    "        self.batch_sz = batch_sz\n",
    "\n",
    "        # Initialize hidden layers        \n",
    "        self.reset_hidden()\n",
    "        self.last_hiddens = (self.hidden_state, self.cell_state)\n",
    "                \n",
    "    def forward(self, xs):\n",
    "        \"\"\"Forward pass AWD-LSTM\"\"\" \n",
    "        \n",
    "        bs, sl = xs.shape\n",
    "        \n",
    "        # Because sequences consisting of only padding are removed from the mini-batch, the mini-batch alters\n",
    "        # Therefore we have to adjust the hidden state for that\n",
    "        if bs != self.last_hiddens[0].shape[1]:\n",
    "            self._change_bs_hidden(bs)\n",
    "        \n",
    "        hidden_states = []\n",
    "        \n",
    "        hiddens = self.last_hiddens\n",
    "\n",
    "        embed = self.encoder(xs)\n",
    "        embed_dp = self.emb_drop(embed)\n",
    "\n",
    "        \n",
    "        inp = embed_dp.view(bs, sl, -1)\n",
    "        \n",
    "        # Dropout on hidden layers\n",
    "        hiddens_dp = []\n",
    "        for hidden_state in hiddens:\n",
    "            hiddens_dp.append(self.hid_dp(hidden_state))\n",
    "            \n",
    "        hiddens_dp = tuple(hiddens_dp)\n",
    "        \n",
    "        output, (h, c) = self.lstms(embed_dp.view(bs, sl, -1), hiddens_dp)\n",
    "        \n",
    "        self.last_hiddens = (h.detach(), c.detach())\n",
    "        \n",
    "        return output, self.last_hiddens\n",
    "    \n",
    "    def reset_hidden(self):\n",
    "        self.hidden_state = torch.zeros((self.num_layers, self.batch_sz, self.hid_sz)).to(dev)\n",
    "        self.cell_state = torch.zeros((self.num_layers, self.batch_sz, self.hid_sz)).to(dev)\n",
    "        self.last_hiddens = (self.hidden_state, self.cell_state)\n",
    "    \n",
    "    def _change_bs_hidden(self, bs):\n",
    "        hidden_state = self.last_hiddens[0]\n",
    "        cell_state = self.last_hiddens[1]\n",
    "        \n",
    "        if bs > hidden_state.shape[1]:\n",
    "            self.batch_sz = bs\n",
    "            self.reset_hidden()\n",
    "        else:\n",
    "            corr_hidden_state = hidden_state[:,:bs,:]\n",
    "            corr_cell_state = cell_state[:,:bs,:]\n",
    "        \n",
    "            self.last_hiddens = (corr_hidden_state, corr_cell_state)\n",
    "    \n",
    "    def freeze_to(self , n):\n",
    "        \n",
    "        params_to_freeze = n * 4 + 1 # Since each LSTM layer has 4 parameters plus 1 to also freeze the encoder\n",
    "        \n",
    "        total_params = len(list(self.parameters()))\n",
    "        \n",
    "        for i, parameter in enumerate(self.parameters()):\n",
    "            parameter.requires_grad = True\n",
    "            \n",
    "            if i < params_to_freeze:\n",
    "                parameter.requires_grad = False\n",
    "            \n",
    "            \n",
    "        for name, parameter in self.named_parameters():\n",
    "            print(name)\n",
    "            print(parameter.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3369ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "emb_dim = 10 # Embeddding dimension\n",
    "hid_sz = 400 # Hidden size\n",
    "num_layers = 20 # Number of LSTM layers stacked together\n",
    "seq_len = num_layers\n",
    "bs = 8\n",
    "\n",
    "# Dropout parameters\n",
    "\n",
    "embed_p = 0.1 # Dropout probability on the embedding\n",
    "hidden_p = 0.3 # Dropout probability on hidden-to-hidden weight matrices\n",
    "# Dropout tussen de inputs van de LSTMs moet ik er nog in bouwen\n",
    "input_p = 0.3 # Dropout probablity on the LSTM input between LSTMS\n",
    "weight_p = 0.5 # Dropout probability on LSTM-to-LSTM weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62114efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = Path('/home/mees/Desktop/Machine_Learning/subcellular_location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e8d2463",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = file_paths / Path('models/3_percent_single_AA_v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac98f88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/mees/Desktop/Machine_Learning/subcellular_location/models/3_percent_single_AA_v1.pt')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3680a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = vocab_sz - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3e3772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWD_LSTM = AWD_LSTM(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, \n",
    "                                embed_p, input_p, weight_p, batch_sz=bs, pad_token=pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ac1621b",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "NEWOBJ class argument isn't a type object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-1334bf5cb14e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAWD_LSTM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: NEWOBJ class argument isn't a type object"
     ]
    }
   ],
   "source": [
    "AWD_LSTM = torch.load(pretrained_model, map_location=torch.device(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134fa6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
