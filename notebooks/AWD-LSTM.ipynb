{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWD-LSTM in Pytorch\n",
    "\n",
    "Creating a AWD-LSTM neural network using PyTorch. The network is based on https://arxiv.org/pdf/1708.02182.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the PyTorch docs: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "\n",
    "We can see that the hidden-to-gidden weight are saved in this manner:\n",
    ">~LSTM.weight_hh_l[k] â€“ the learnable hidden-hidden weights of the kth\\text{k}^{th}kth layer (W_hi|W_hf|W_hg|W_ho), of shape (4*hidden_size, hidden_size)\n",
    "\n",
    "We can use this to apply WeightDropout or DropConnect to the LSTM layers.\n",
    "Also using https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/nn/weight_drop.html and FastAI docs https://github.com/fastai/fastai/blob/45376f13df04ddf72749be25ae8a6dff35859f68/fastai/text/models/awdlstm.py as inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightDropout(torch.nn.Module):\n",
    "    \"Apply dropout to LSTM's hidden-hidden weights\"\n",
    "    \n",
    "    def __init__(self, module, weight_p):\n",
    "        super(WeightDropout, self).__init__()\n",
    "        self.module = module\n",
    "        self.weight_p = weight_p\n",
    "        \n",
    "        # Save the name of the layer weights in a list\n",
    "        num_layers = module.num_layers\n",
    "        layer_base_name = 'weight_hh_l'      \n",
    "        self.layer_weights = [layer_base_name + str(i) for i in range(num_layers)]\n",
    "        \n",
    "        # Make a copy of the weights in weightname_raw\n",
    "        for weight in self.layer_weights:\n",
    "            w = getattr(self.module, weight)\n",
    "            del module._parameters[weight]\n",
    "            self.module.register_parameter(f'{weight}_raw', torch.nn.Parameter(w))\n",
    "            \n",
    "        def _setweights(self):\n",
    "            \"Apply dropout to the raw weights\"\n",
    "            for weight in self.layer_weights:\n",
    "                raw_w = getattr(self, f'{weight}_raw')\n",
    "                if self.training:\n",
    "                    w = torch.nn.F(raw_w, p=self.weight_p)\n",
    "                else:\n",
    "                    w = raw_w.clone()\n",
    "                setattr(self.module, weight, w)\n",
    "                \n",
    "        def forward(self, *args):\n",
    "            self._setweights()\n",
    "            return self.module(*args)   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sz = 10\n",
    "emb_dim = 400 # Embeddding dimension\n",
    "hid_sz = 1150 # Hidden size\n",
    "num_layers = 3 # Number of LSTM layers stacked together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_p = 0.1 # Dropout probability on the embedding\n",
    "hidden_p = 0.3 # Dropout probability on hidden-to-hidden weight matrices\n",
    "input_p = 0.3 # Dropout probablity on the LSTM input between LSTMS\n",
    "\n",
    "# This one still has to be implemented\n",
    "#weight_p = 0.5 # Dropout probability on LSTM-to-LSTM weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Embedding(vocab_sz, emb_dim),\n",
    "    torch.nn.Dropout(p=0.1),\n",
    "    WeightDropout(\n",
    "        torch.nn.LSTM(input_size = emb_dim, hidden_size = hid_sz, num_layers = num_layers, dropout=input_p),\n",
    "        hidden_p\n",
    "    ),\n",
    "    torch.nn.Linear(emb_dim, vocab_sz)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Embedding(10, 10)\n",
       "  (1): Dropout(p=0.1, inplace=False)\n",
       "  (2): WeightDropout(\n",
       "    (module): LSTM(10, 10, num_layers=3, dropout=0.3)\n",
       "  )\n",
       "  (3): Linear(in_features=10, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open data in pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasmic vesicle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Early endosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Membrane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence Subcellular location [CC]\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...       Cytoplasmic vesicle\n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...            Early endosome\n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...                 Cytoplasm\n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...             Mitochondrion\n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...                  Membrane"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = Path('/home/mees/Desktop/Machine_Learning/subcellular_location/data/processed/protein_data_2021-02-08.csv')\n",
    "df = pd.read_csv(data_file, sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = set(df['Subcellular location [CC]'])\n",
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
