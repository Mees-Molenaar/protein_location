{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVoWWP2TkfpH"
   },
   "source": [
    "# Protein Classifier using AWD-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "05m_tDaZkfpQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import sklearn.metrics as metrics\n",
    "from IPython.display import HTML, display\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mp69KYdNkfpS"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "56HVWDq5kfpU",
    "outputId": "034e4862-f298-4b3f-e3be-b653ffaac8e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0frf8WW_kfpW",
    "outputId": "e8496ecc-dc07-43da-a355-0b2fd1a85b0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa8a40377d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Q-6XVNYTkfpX",
    "outputId": "d567d9d9-1fb1-4dd3-8565-feeb354188de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "table, th, td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<style>\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qq9C8202kfpZ"
   },
   "source": [
    "## Usefull Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "V7-BmN0zkfpb"
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation loss using matplotlib\n",
    "def show_losses(training_loss, validation_loss):\n",
    "    r\"\"\"Plot graphs with on x-axis epochs and y-axis the loss.\n",
    "\n",
    "        This graph contains both the `training_loss` and `validation_loss`\n",
    "        for each epoch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        training_loss : array\n",
    "            Containing the training loss for each epoch.\n",
    "        validation_loss : array\n",
    "            Containing the validation loss for each epoch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Plot of the two graphs.\n",
    "\n",
    "    \"\"\"\n",
    "    np_loss = np.asarray(training_loss)\n",
    "    np_val_loss = np.asarray(validation_loss)\n",
    "    plt.plot(np_loss, label='Train loss')\n",
    "    plt.plot(np_val_loss, label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Cross Entropy Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dX9RNmgTkfpZ"
   },
   "outputs": [],
   "source": [
    "def train_model(epochs, model, train_loader, test_loader, optimizer, criterion):\n",
    "    r\"\"\"Train the resnet18 model.\n",
    "    \n",
    "    Train the resnet18 model for `epochs`. Using the\n",
    "    `optimizer` and the loss function in `criterion`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : int\n",
    "        Number of epochs to train the model.\n",
    "    model : torch Model\n",
    "        Model that is being trained.\n",
    "    train_loader : torch Dataloader\n",
    "        Dataloader containing the train data to train the model.\n",
    "    test_loader : torch Dataloader\n",
    "        Dataloader containing the test data to validate the model.\n",
    "    optimizer : torch Optimizer\n",
    "        Optimizer used to optimize the model.\n",
    "    criterion : torch.nn Lossfunction\n",
    "        Loss function used. This loss function has to be minimilized by the model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch Model\n",
    "        The trained model.\n",
    "    loss_history : array\n",
    "        Training loss for each epoch.\n",
    "    val_loss_history : array\n",
    "        Validation loss for each epoch.\n",
    "    \n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Inititiate loss variables\n",
    "        epoch_loss = 0.0\n",
    "        epoch_val_loss = 0.0\n",
    "\n",
    "        print(f'Epoch: {str(epoch + 1)}')\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Alterate between train and validaton phase\n",
    "        for phase in ['train', 'val']:\n",
    "\n",
    "          if phase == 'train':\n",
    "            model.train(True) # Set model to training mode\n",
    "            data_loader = train_loader\n",
    "          else:\n",
    "            model.train(False) # Set model to evaluate mode\n",
    "            data_loader = test_loader\n",
    "\n",
    "\n",
    "          # Loop over the data in batch sizes.\n",
    "          for i, data in enumerate(data_loader, 0):\n",
    "            # get the inputs; data is a one input (batch size), and y\n",
    "\n",
    "            x, y = data\n",
    "            x = x.squeeze(0) # Squeeze x in the correct shape\n",
    "            y = y.squeeze(0) # Squeeze y in the correct shape\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            output = model(x)   \n",
    "            output = output.unsqueeze(0) # For the correct shape\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            # Add loss to each epoch\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss = output.shape[0] * loss.item()\n",
    "            else:\n",
    "                epoch_val_loss += output.shape[0] * loss.item()\n",
    "\n",
    "        epoch_loss /= len(train_set)\n",
    "        epoch_val_loss /= len(test_set)\n",
    "\n",
    "        loss_history.append(epoch_loss)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "\n",
    "        print(f'Epoch {str(epoch)} MSE Train Loss: {str(epoch_loss)} ; Validation Loss: {str(epoch_val_loss)}.')\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time_minutes = (end_time - start_time) / 60\n",
    "        \n",
    "        print(f'Epoch duration {str(epoch_time_minutes)} minutes.')\n",
    "        \n",
    "    print('Finished Training')\n",
    "    return model, loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dObmbgbvkfpc"
   },
   "outputs": [],
   "source": [
    "# Tokenize the protein sequence (or any sequence) in kmers.\n",
    "def tokenize(df, protein_seqs_column, kmer_sz, premade_vocab=False):\n",
    "    \n",
    "    if not premade_vocab:\n",
    "        kmers = set()\n",
    "        # Loop over protein sequences\n",
    "        for protein_seq in df[protein_seqs_column]:\n",
    "            # Loop over the whole sequence\n",
    "            for i in range(len(protein_seq) - (kmer_sz - 1)):\n",
    "                # Add kmers to the set, thus only unique kmers will remain\n",
    "                kmers.add(protein_seq[i: i + kmer_sz])\n",
    "\n",
    "        # Map kmers for one hot-encoding\n",
    "        kmer_to_id = dict()\n",
    "        id_to_kmer = dict()\n",
    "\n",
    "        for ind, kmer in enumerate(kmers):\n",
    "            kmer_to_id[kmer] = ind\n",
    "            id_to_kmer[ind] = kmer\n",
    "\n",
    "        vocab_sz = len(kmers)\n",
    "\n",
    "        assert vocab_sz == len(kmer_to_id.keys())\n",
    "    \n",
    "    else:\n",
    "        kmer_to_id, id_to_kmer = premade_vocab\n",
    "        vocab_sz = len(kmer_to_id)\n",
    "    \n",
    "    # Tokenize the protein sequence to integers\n",
    "    tokenized = []\n",
    "    for i, protein_seq in enumerate(df[protein_seqs_column], 0):\n",
    "        sequence = []\n",
    "        \n",
    "        # If the kmer can't be found these indexes should be deleted\n",
    "        remove_idxs = []\n",
    "        \n",
    "        for i in  range(len(protein_seq) - (kmer_sz -1)):\n",
    "            # Convert kmer to integer\n",
    "            kmer = protein_seq[i: i + kmer_sz]\n",
    "            \n",
    "            # For some reason, some kmers miss. Thus these sequences have to be removed\n",
    "            try:\n",
    "                sequence.append(kmer_to_id[kmer])\n",
    "            except:\n",
    "                remove_idxs.append(i)\n",
    "            \n",
    "        tokenized.append(sequence)\n",
    "            \n",
    "    df['tokenized_seqs'] = tokenized\n",
    "    \n",
    "    df.drop(remove_idxs, inplace=True)\n",
    "    \n",
    "    return df, vocab_sz, kmer_to_id, id_to_kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "b0Sufgyqkfpc"
   },
   "outputs": [],
   "source": [
    "# Function to show the accuracy of the model.\n",
    "def accuracy(model, data_loader):\n",
    "    r\"\"\"Calculate accuracy of the model.\n",
    "    \n",
    "    Calculates the accuracy of the trained `model`\n",
    "    for the `data_loader` in the input.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch Model\n",
    "        The trained model.\n",
    "    data_loader : torch Dataloader\n",
    "        Torch dataloader containing the samples you want to test the accuracy for.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Prints the accurucy of the `model` for the samples in the `data_loader`\n",
    "    \n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for ind, data in enumerate(data_loader, 0):\n",
    "            x, y = data\n",
    "            x = x.squeeze(0) # Squeeze x in the correct shape\n",
    "            y = y.squeeze(0) # Squeeze y in the correct shape\n",
    "            \n",
    "            output = model(x)\n",
    "            output = output.unsqueeze(0)\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "\n",
    "        accuracy = np.round((correct / total * 100), 2)\n",
    "        print(f'Accuracy of the network is {str(accuracy)}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcW0LHbzkfpe"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "tbRFWsBukfpe",
    "outputId": "2290f72f-7005-49f3-f695-ea688643ffb6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasmic vesicle, sec...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Early endosome {ECO:0000...</td>\n",
       "      <td>Endosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton,...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Mitochondrion {ECO:00003...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "      <td>Cell membrane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence  \\\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...   \n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...   \n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...   \n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...   \n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...   \n",
       "\n",
       "                           Subcellular location [CC]       Location  \n",
       "0  SUBCELLULAR LOCATION: Cytoplasmic vesicle, sec...      Cytoplasm  \n",
       "1  SUBCELLULAR LOCATION: Early endosome {ECO:0000...       Endosome  \n",
       "2  SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton,...      Cytoplasm  \n",
       "3  SUBCELLULAR LOCATION: Mitochondrion {ECO:00003...  Mitochondrion  \n",
       "4  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  Cell membrane  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = Path('/home/mees/Desktop/Machine_Learning/subcellular_location/data/processed/protein_data_2021-04-04.csv')\n",
    "df = pd.read_csv(data_file, sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "sNGEB5i4kfpf",
    "outputId": "4a0f1948-d4c9-4891-df86-74d7100fa941"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Endosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence       Location\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...      Cytoplasm\n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...       Endosome\n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...      Cytoplasm\n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  Mitochondrion\n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  Cell membrane"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['Subcellular location [CC]'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNktYsKWkfpf"
   },
   "source": [
    "### Tokenize the Data\n",
    "\n",
    "The data has to be tokenized according to the Language Model trained before, therefore, we have to load in that dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vJwNg0L9kfpg"
   },
   "outputs": [],
   "source": [
    "# Set-up numpy generator for random numbers\n",
    "random_number_generator = np.random.default_rng(seed=42)\n",
    "KMER_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "j3QzdCOfkfpg"
   },
   "outputs": [],
   "source": [
    "# Load the vocabolary from the Language Model\n",
    "vocab_save_file = '/home/mees/Desktop/Machine_Learning/subcellular_location/data/interim/LM_vocab.pkl'\n",
    "vocab = pickle.load(open(vocab_save_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AtZA6bQ5kfpg"
   },
   "outputs": [],
   "source": [
    "# Tokenize the protein sequence\n",
    "df, vocab_sz, kmer_to_id, id_to_kmer = tokenize(df, 'Sequence', KMER_SIZE, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "v-GIN4-rkfph",
    "outputId": "30a21a4d-c729-406c-9eb4-feecbdf60810"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Location</th>\n",
       "      <th>tokenized_seqs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[3884, 8570, 3840, 6832, 2277, 2221, 1020, 904...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Endosome</td>\n",
       "      <td>[8772, 7207, 1857, 1688, 5461, 3901, 4899, 424...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[1565, 3797, 2513, 516, 1428, 6558, 6568, 7337...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "      <td>[8939, 2538, 9262, 4438, 2547, 302, 60, 3064, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>[8939, 6897, 6013, 1021, 3034, 2863, 8501, 697...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence       Location  \\\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...      Cytoplasm   \n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...       Endosome   \n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...      Cytoplasm   \n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  Mitochondrion   \n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  Cell membrane   \n",
       "\n",
       "                                      tokenized_seqs  \n",
       "0  [3884, 8570, 3840, 6832, 2277, 2221, 1020, 904...  \n",
       "1  [8772, 7207, 1857, 1688, 5461, 3901, 4899, 424...  \n",
       "2  [1565, 3797, 2513, 516, 1428, 6558, 6568, 7337...  \n",
       "3  [8939, 2538, 9262, 4438, 2547, 302, 60, 3064, ...  \n",
       "4  [8939, 6897, 6013, 1021, 3034, 2863, 8501, 697...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZGZZKjMkfph",
    "outputId": "c32523a2-881e-42f9-a52e-50f143d7c48f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16614"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_s83p4ekfpi"
   },
   "source": [
    "### Numericalize the label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6RnA7Ahwkfpi",
    "outputId": "21fbf819-505f-455f-dbd5-49004a725221"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16614"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some fields are NaN, remove these\n",
    "df.dropna(inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqwWz9y7kfpi"
   },
   "source": [
    "Create a dictionary to numericalize the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "rVTVFS09kfpj"
   },
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "\n",
    "for i, label in enumerate(df['Location'].unique(), 0):\n",
    "    label_dict[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "GSU8mia8kfpj"
   },
   "outputs": [],
   "source": [
    "def numericalizeClass(df, class_column, label_dict, label_column='Label'):\n",
    "    df[label_column] = df[class_column].map(label_dict)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "q-gQqK4Zkfpk",
    "outputId": "888c57aa-9f2f-4a3c-c3ea-65d897189647"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Location</th>\n",
       "      <th>tokenized_seqs</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[3884, 8570, 3840, 6832, 2277, 2221, 1020, 904...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Endosome</td>\n",
       "      <td>[8772, 7207, 1857, 1688, 5461, 3901, 4899, 424...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[1565, 3797, 2513, 516, 1428, 6558, 6568, 7337...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "      <td>[8939, 2538, 9262, 4438, 2547, 302, 60, 3064, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>[8939, 6897, 6013, 1021, 3034, 2863, 8501, 697...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence       Location  \\\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...      Cytoplasm   \n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...       Endosome   \n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...      Cytoplasm   \n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  Mitochondrion   \n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  Cell membrane   \n",
       "\n",
       "                                      tokenized_seqs  Label  \n",
       "0  [3884, 8570, 3840, 6832, 2277, 2221, 1020, 904...      0  \n",
       "1  [8772, 7207, 1857, 1688, 5461, 3901, 4899, 424...      1  \n",
       "2  [1565, 3797, 2513, 516, 1428, 6558, 6568, 7337...      0  \n",
       "3  [8939, 2538, 9262, 4438, 2547, 302, 60, 3064, ...      2  \n",
       "4  [8939, 6897, 6013, 1021, 3034, 2863, 8501, 697...      3  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = numericalizeClass(df, 'Location', label_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JGmFcAjJkfpk",
    "outputId": "c8a3c476-176a-44a9-edb3-40dbd8b5fecb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cytoplasm': 0,\n",
       " 'Endosome': 1,\n",
       " 'Mitochondrion': 2,\n",
       " 'Cell membrane': 3,\n",
       " 'Nucleus': 4,\n",
       " 'Endoplasmic reticulum': 5,\n",
       " 'Secreted': 6,\n",
       " 'Golgi apparatus': 7,\n",
       " 'Extracellular': 8,\n",
       " 'Peroxisome': 9,\n",
       " 'Lysosome/ Vacuole': 10}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "wlpBWvdWkfpl",
    "outputId": "30c5f808-392a-43fd-e799-742ca4ef9fa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3647.,   95.,  858., 3993., 5462.,  651., 1079.,  316.,  375.,\n",
       "         138.]),\n",
       " array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAnUfQNPkfpl"
   },
   "source": [
    "! It might help to rebalance the classes since classes 0, 3 and 4 are now over represented. Therefore, the model can just default to class 4. Since, if the model predicts class 4 for every sequence it will have an accuracy of around 34% already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jSGjtNVkfpm"
   },
   "source": [
    "An other point is that the sequence should have some information to work with, according to this paper:\n",
    "> https://www.nature.com/articles/s41598-019-38746-w\n",
    "\n",
    "A motif can be in between 3-20 amino acids, thus 1-6/7 kmers. I chose 5 kmers. However training time took way longer in that way. Therefore, I know choose another seqeunce length to reduce training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "hV_YQwZCkfpm"
   },
   "outputs": [],
   "source": [
    "df['Length'] = df['tokenized_seqs'].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "M1Uk4Cffkfpm",
    "outputId": "45aa26d8-3bba-4630-dafb-b4318e382baa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.,  21.,  15.,   5.,  18.,  31.,  57.,  93., 104., 163.]),\n",
       " array([  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90., 100.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['Length'], range=(0, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoMVJdmwkfpn"
   },
   "source": [
    "Based on this, and reduce training time. I now choose a sequence length of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2SGZXjgkfpn",
    "outputId": "2c59f273-d341-40e0-8242-a8f76a04d6ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16552"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['Length'] >= 50]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Se0r2uHnkfpo"
   },
   "source": [
    "This only removed about 60 entries.\n",
    "Now, to address class imbalance. Create a weighted sampler. However, the current weight labels are not correct. For each entry there should be a weight attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3628, 1: 95, 2: 854, 3: 3966, 4: 5459, 5: 649, 6: 1072, 7: 316, 8: 375, 9: 53, 10: 85}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2191880135331078,\n",
       " 0.005739487675205413,\n",
       " 0.05159497341710972,\n",
       " 0.23960850652489124,\n",
       " 0.3298090865152247,\n",
       " 0.03920976317061382,\n",
       " 0.06476558724021267,\n",
       " 0.019091348477525374,\n",
       " 0.02265587240212663,\n",
       " 0.0032020299661672308,\n",
       " 0.0051353310778153695]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_of_labels = dict(df['Label'].value_counts(sort=False))\n",
    "\n",
    "weight_labels = []\n",
    "\n",
    "print(total_of_labels)\n",
    "\n",
    "for total_of_label in total_of_labels.values():\n",
    "    weight = total_of_label / len(df)\n",
    "    weight_labels.append(weight)\n",
    "    \n",
    "weight_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aMErvL7kfpo"
   },
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "ZdainIjGkfpp"
   },
   "outputs": [],
   "source": [
    "class AminoClassifierDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, num_classes, bs=1):\n",
    "        self.df = df\n",
    "        self.num_classes = num_classes \n",
    "        self.batch_sz = bs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        x = torch.LongTensor(self.df.iloc[idx]['tokenized_seqs'])\n",
    "        x = x.to(dev) \n",
    "        \n",
    "        y = torch.LongTensor([self.df.iloc[idx]['Label']])\n",
    "        y = y.to(dev)\n",
    "    \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTJPhWeQkfpp"
   },
   "source": [
    "## The Protein Classifier\n",
    "\n",
    "Creating the complete protein from its parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tS3F6Jmkkfpp"
   },
   "source": [
    "### AWD-LSTM\n",
    "Start with the AWD-LSTM, which encodes the protein sequence and is already trained.\n",
    "\n",
    "I can add more dropout and I should add including the last hidden layers (cell and hidden states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(torch.nn.Module):\n",
    "    \"Apply dropout to an Embedding with probability emp_p\"\n",
    "\n",
    "    def __init__(self, emb_p=0):\n",
    "        super(EmbeddingDropout, self).__init__()\n",
    "        \n",
    "        self.emb_p = emb_p\n",
    "\n",
    "    def forward(self, inp):\n",
    "       \n",
    "        drop = torch.nn.Dropout(self.emb_p)\n",
    "        placeholder = torch.ones((inp.size(0), 1)).to(dev)\n",
    "        mask = drop(placeholder)      \n",
    "        out = inp * mask\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "8-QAxw1zkfpp"
   },
   "outputs": [],
   "source": [
    "class WeightDropout(torch.nn.Module):\n",
    "    \"Apply dropout to LSTM's hidden-hidden weights\"\n",
    "\n",
    "    def __init__(self, module, weight_p):\n",
    "        super(WeightDropout, self).__init__()\n",
    "        self.module = module\n",
    "        self.weight_p = weight_p\n",
    "\n",
    "        # Save the name of the layer weights in a list\n",
    "        num_layers = module.num_layers\n",
    "        layer_base_name = 'weight_hh_l'      \n",
    "        self.layer_weights = [layer_base_name + str(i) for i in range(num_layers)]\n",
    "\n",
    "        # Make a copy of the weights in weightname_raw\n",
    "        for weight in self.layer_weights:\n",
    "\n",
    "            w = getattr(self.module, weight)\n",
    "            del module._parameters[weight]\n",
    "            self.module.register_parameter(f'{weight}_raw', torch.nn.Parameter(w))\n",
    "\n",
    "    def _setweights(self):\n",
    "        \"Apply dropout to the raw weights\"\n",
    "        for weight in self.layer_weights:\n",
    "            raw_w = getattr(self.module, f'{weight}_raw')\n",
    "            if self.training:\n",
    "                w = torch.nn.functional.dropout(raw_w, p=self.weight_p)\n",
    "            else:\n",
    "                w = raw_w.clone()\n",
    "            setattr(self.module, weight, w)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        return self.module(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "id": "l91JM4j3kfpq"
   },
   "outputs": [],
   "source": [
    "class AWD_LSTM(torch.nn.Module):\n",
    "    def __init__(self, num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz = 1):\n",
    "        super(AWD_LSTM, self).__init__()\n",
    "\n",
    "        # Embedding with droput\n",
    "        self.encoder = torch.nn.Embedding(vocab_sz, emb_dim)\n",
    "        self.emb_drop = EmbeddingDropout(emb_p=embed_p)\n",
    "\n",
    "\n",
    "        # Dropouts on the inputs and the hidden layers\n",
    "        self.input_dp = torch.nn.Dropout(p=input_p)\n",
    "        self.hid_dp = torch.nn.Dropout(p=hidden_p)\n",
    "\n",
    "        # Create a list of lstm layers with wieghtdropout\n",
    "        self.lstms = []\n",
    "        for i in range(num_layers):\n",
    "            self.lstms.append(\n",
    "                WeightDropout(nn.LSTM(input_size=emb_dim, hidden_size=hid_sz, num_layers=1), weight_p))\n",
    "        self.lstms = nn.ModuleList(self.lstms)\n",
    "\n",
    "        # Save all variables        \n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_sz = hid_sz\n",
    "        self.hidden_p = hidden_p\n",
    "        self.embed_p = embed_p\n",
    "        self.input_p = input_p\n",
    "        self.weight_p = weight_p\n",
    "        self.batch_sz = batch_sz\n",
    "\n",
    "        # Initialize hidden layers        \n",
    "        self.reset_hidden()\n",
    "        self.last_hiddens = (self.hidden_state, self.cell_state)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        \"\"\"Forward pass AWD-LSTM\"\"\" \n",
    "\n",
    "        bs, sl = xs.shape\n",
    "\n",
    "        ys = []\n",
    "\n",
    "        hiddens = self.last_hiddens\n",
    "\n",
    "        hidden_states = [hiddens]\n",
    "\n",
    "        for i, lstm in enumerate(self.lstms):\n",
    "\n",
    "            # Embed the input and add dropout to it  \n",
    "            x = xs[:, i]\n",
    "            embed = self.encoder(x)\n",
    "            embed_dp = self.emb_drop(embed)\n",
    "            \n",
    "            # Again add dropout, this feels like doing dropout on dropout, I dont know if it is worth\n",
    "          \n",
    "            input_dp = self.input_dp(embed_dp)\n",
    "\n",
    "            # Dropout on the hidden states\n",
    "            hiddens_dp = []\n",
    "\n",
    "            for hidden_state in hidden_states[i]:\n",
    "                hiddens_dp.append(self.hid_dp(hidden_state))\n",
    "\n",
    "            hiddens_dp = tuple(hiddens_dp)\n",
    "            \n",
    "            # Go trough one LSTM layer\n",
    "            output, hiddens = lstm(input_dp.view(1, bs, -1), hiddens_dp) \n",
    "\n",
    "            # Detach hidden states\n",
    "            det_hiddens = []\n",
    "\n",
    "            for hidden in hiddens:\n",
    "                det_hiddens.append(hidden.detach())\n",
    "\n",
    "            det_hiddens = tuple(det_hiddens)\n",
    "\n",
    "            hidden_states.append(det_hiddens)\n",
    "\n",
    "            y = output.view(bs, 1, -1)\n",
    "            \n",
    "            ys.append(y)\n",
    "\n",
    "        y = torch.stack(ys, dim=0)\n",
    "        \n",
    "        y = y.view(bs, sl, -1)\n",
    "\n",
    "        self.last_hiddens = hidden_states[-1]\n",
    "\n",
    "        return y, hidden_states\n",
    "\n",
    "    def reset_hidden(self):\n",
    "                   \n",
    "        self.hidden_state = torch.zeros((1, self.batch_sz, self.hid_sz)).to(dev)\n",
    "        self.cell_state = torch.zeros((1, self.batch_sz, self.hid_sz)).to(dev)\n",
    "        self.last_hiddens = (self.hidden_state, self.cell_state)\n",
    "                                  \n",
    "    def freeze_to(self , n):\n",
    "        \n",
    "        params_to_freeze = n * 4 + 1 # Since each LSTM layer has 4 parameters plus 1 to also freeze the encoder\n",
    "        \n",
    "        total_params = len(list(self.parameters()))\n",
    "        \n",
    "        for i, parameter in enumerate(self.parameters()):\n",
    "            parameter.requires_grad = True\n",
    "            \n",
    "            if i < params_to_freeze:\n",
    "                parameter.requires_grad = False\n",
    "            \n",
    "            \n",
    "        for name, parameter in self.named_parameters():\n",
    "            print(name)\n",
    "            print(parameter.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gr8AWMzkfpq"
   },
   "source": [
    "### SentenceEncoder\n",
    "\n",
    "This part encodes the whole sequences in seq_lenghts using the pretrained AWD-LSTM language model.\n",
    "\n",
    "We use the Identity class to replace the decoder in the original AWD-LSTM. \n",
    "\n",
    "Finally, the model should not be updated. Therefore, the forward pass is in torch.no_grad()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "id": "jlFVmQtqkfpr"
   },
   "outputs": [],
   "source": [
    "class Identity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "id": "cA2cTY_Ekfpr"
   },
   "outputs": [],
   "source": [
    "class SentenceEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, model):\n",
    "        super(SentenceEncoder, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            \n",
    "            # First element is batch size, second element is the sequence\n",
    "            inp_size = inp.shape[1]\n",
    "\n",
    "            # It is nicer to add padding\n",
    "            max_iterations = int(inp_size / self.seq_len)\n",
    "\n",
    "            hidden_state_outputs = []\n",
    "            cell_state_outputs = []\n",
    "            \n",
    "            for i in range(0, self.seq_len * max_iterations, self.seq_len):\n",
    "                _, hidden = self.model(inp[:, i: i + self.seq_len])\n",
    "                \n",
    "                for states in hidden:\n",
    "                    hidden_state_outputs.append(states[0])\n",
    "                    cell_state_outputs.append(states[1])\n",
    "                        \n",
    "            hidden_state_outputs = torch.cat(hidden_state_outputs, dim = 0)\n",
    "            cell_state_outputs = torch.cat(cell_state_outputs, dim = 0)\n",
    "\n",
    "            return (hidden_state_outputs, cell_state_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeO_z7m-kfpr"
   },
   "source": [
    "### PoolingLinearClassifier\n",
    "\n",
    "The encoded sequence is needed to be pooled, otherwise the model can not use the information for classification.\n",
    "\n",
    "Then, the data is normalized using batchnorm.\n",
    "Dropout is applied to prevent overfitting.\n",
    "And linear layers with a ReLU activiation are used to classify the pooled protein data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "id": "w28cdQdpkfpr"
   },
   "outputs": [],
   "source": [
    "def pool_encoded_sequence(output):\n",
    "    r\"\"\"Pool the encoded AA sequence and \n",
    "    return one vector with the max_pool and avg_pool concatenated\"\"\"\n",
    "    \n",
    "    hidden_states = output[0]\n",
    "    cell_states = output[1]\n",
    "    \n",
    "    sl, bs = hidden_states.shape[:2]\n",
    "\n",
    "    last_hidden_state = hidden_states[-1, :, :]\n",
    "    last_cell_state = cell_states[-1, :, :]\n",
    "    \n",
    "\n",
    "    hidden_state_avg = hidden_states.sum(dim=0) / sl\n",
    "    cell_state_avg = cell_states.sum(dim=0) / sl\n",
    "    \n",
    "\n",
    "    hidden_state_max = hidden_states.max(dim=0)[0]\n",
    "    cell_state_max = hidden_states.max(dim=0)[0]\n",
    "\n",
    "    x = torch.cat([last_hidden_state, last_cell_state, hidden_state_avg, cell_state_avg, \\\n",
    "                  hidden_state_max, cell_state_max], 0)\n",
    "    \n",
    "    x = x.view(bs, -1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "id": "nCOiXkDUkfpr"
   },
   "outputs": [],
   "source": [
    "class PoolingLinearClassifier(torch.nn.Module):\n",
    "    r\"\"\"Pool the outputs from the encoder and classify it.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, batch_sz):\n",
    "        super(PoolingLinearClassifier, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.batch_sz = batch_sz\n",
    "        \n",
    "        if batch_sz > 1:\n",
    "            \n",
    "            self.layers = nn.Sequential(\n",
    "                nn.BatchNorm1d(1150 * 6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Dropout(p=0.2, inplace=False),\n",
    "                nn.Linear(in_features=1150 * 6, out_features=1150, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm1d(1150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Dropout(p=0.1, inplace=False),\n",
    "                nn.Linear(in_features=1150, out_features=50, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Dropout(p=0.1, inplace=False),\n",
    "                nn.Linear(in_features=50, out_features=num_classes, bias=True)\n",
    "            )\n",
    "        else:\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Dropout(p=0.2, inplace=False),\n",
    "                nn.Linear(in_features=1150 * 6, out_features=1150, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=0.1, inplace=False),\n",
    "                nn.Linear(in_features=1150, out_features=50, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=0.1, inplace=False),\n",
    "                nn.Linear(in_features=50, out_features=num_classes, bias=True)\n",
    "            )\n",
    "            \n",
    "        \n",
    "    \n",
    "    def forward(self, inp):\n",
    "        output_encoder = inp\n",
    "        pooled_output = pool_encoded_sequence(output_encoder)\n",
    "        y = self.layers(pooled_output)\n",
    "        \n",
    "        return y        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZmt9xEekfpr"
   },
   "source": [
    "### Combine everything in the protein classifier\n",
    "\n",
    "Combine every class and part in the protein classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "id": "KPF-qx-lkfpr"
   },
   "outputs": [],
   "source": [
    "class proteinClassifier(torch.nn.Module):\n",
    "    r\"\"\"The complete protein classifier\"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, seq_len, num_classes, batch_size, pretrained_file=False):\n",
    "        super(proteinClassifier, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_sz = hid_sz\n",
    "        self.hidden_p = hidden_p\n",
    "        self.embed_p = embed_p\n",
    "        self.input_p = input_p\n",
    "        self.seq_len = seq_len\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        language_model = AWD_LSTM(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, \n",
    "                                embed_p, input_p, weight_p, batch_sz=batch_size)\n",
    "        \n",
    "        if pretrained_file:\n",
    "            language_mode = torch.load(pretrained_file, map_location=torch.device(dev))\n",
    "        \n",
    "        encoder = SentenceEncoder(seq_len, language_model)\n",
    "        \n",
    "        classifier = PoolingLinearClassifier(num_classes, self.batch_size)\n",
    "        \n",
    "        self.layers = nn.Sequential(encoder, classifier)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        y = self.layers(inp)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSUp6mZHkfps"
   },
   "source": [
    "## Model hyperparameters and train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "id": "Rf1MiKM_kfps"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "emb_dim = 400 # Embeddding dimension\n",
    "hid_sz = 1150 # Hidden size\n",
    "num_layers = 3 # Number of LSTM layers stacked together\n",
    "seq_len = 50 # Based on paper mentioned above\n",
    "batch_size = 8 \n",
    "\n",
    "# Dropout parameters\n",
    "\n",
    "embed_p = 0.1 # Dropout probability on the embedding\n",
    "hidden_p = 0.3 # Dropout probability on hidden-to-hidden weight matrices\n",
    "input_p = 0.3 # Dropout probablity on the LSTM input between LSTMS\n",
    "weight_p = 0.5 # Dropout probability on LSTM-to-LSTM weight matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMaUgLbpkfpu"
   },
   "source": [
    "### Load in the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "id": "-RhParSjkfpu"
   },
   "outputs": [],
   "source": [
    "pretrained_model = '/home/mees/Desktop/Machine_Learning/subcellular_location/models/AA_LM_v3_ph2.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aW7z7yI-kfpu",
    "outputId": "62562bb1-aa62-444f-e180-8d93ff377aaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(label_dict)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJuNCOxskfpu",
    "outputId": "1a6f8ffc-4452-4d83-f3cc-d21bf831e1c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "proteinClassifier(\n",
       "  (layers): Sequential(\n",
       "    (0): SentenceEncoder(\n",
       "      (model): AWD_LSTM(\n",
       "        (encoder): Embedding(9317, 400)\n",
       "        (emb_drop): EmbeddingDropout()\n",
       "        (input_dp): Dropout(p=0.3, inplace=False)\n",
       "        (hid_dp): Dropout(p=0.3, inplace=False)\n",
       "        (lstms): ModuleList(\n",
       "          (0): WeightDropout(\n",
       "            (module): LSTM(400, 1150)\n",
       "          )\n",
       "          (1): WeightDropout(\n",
       "            (module): LSTM(400, 1150)\n",
       "          )\n",
       "          (2): WeightDropout(\n",
       "            (module): LSTM(400, 1150)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): PoolingLinearClassifier(\n",
       "      (layers): Sequential(\n",
       "        (0): BatchNorm1d(6900, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Dropout(p=0.2, inplace=False)\n",
       "        (2): Linear(in_features=6900, out_features=1150, bias=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): BatchNorm1d(1150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "        (6): Linear(in_features=1150, out_features=50, bias=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "        (8): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (9): Dropout(p=0.1, inplace=False)\n",
       "        (10): Linear(in_features=50, out_features=11, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = proteinClassifier(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p,\n",
    "                         embed_p, input_p, weight_p, seq_len, num_classes, batch_size, pretrained_model)\n",
    "model.to(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYwstDEKkfpv"
   },
   "source": [
    "## Learning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "id": "FmSpwoR7kfpv"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "adam_betas = (0.7, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "id": "Ajfvm-krkfpv"
   },
   "outputs": [],
   "source": [
    "# Costfunction and optimize algorithm\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.FloatTensor(weight_labels))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=adam_betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXkpsc7hkfpv"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data has different length, therefore I use the pack_sequence in the collate fn.\n",
    "https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_padd(batch):\n",
    "    \n",
    "    seqs = []\n",
    "    ys = []\n",
    "    \n",
    "    for seq in batch:\n",
    "        seqs.append(seq[0])\n",
    "        ys.append(seq[1])\n",
    "        \n",
    "    padded = pad_sequence(seqs, batch_first=True)\n",
    "\n",
    "    \n",
    "    ys = torch.stack(ys, dim=0)\n",
    "    \n",
    "    return padded, ys   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "id": "uDa2Eq40kfpv"
   },
   "outputs": [],
   "source": [
    "# Load the data in the DataSet\n",
    "AADataset = AminoClassifierDataset(df, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "id": "GHShLf7Lkfpw"
   },
   "outputs": [],
   "source": [
    "# Split the data in an 80/20% split for training and testing\n",
    "data_len = len(AADataset)\n",
    "train_part = int(0.8 * data_len)\n",
    "test_part = data_len - train_part\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(AADataset, [train_part, test_part])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "id": "fbs-M47qkfpw"
   },
   "outputs": [],
   "source": [
    "# Load the data into data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, collate_fn=collate_fn_padd)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_padd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3955])\n",
      "torch.Size([8, 1])\n",
      "torch.Size([8, 11])\n",
      "torch.Size([8])\n",
      "tensor([0, 5, 3, 4, 0, 4, 4, 0])\n",
      "tensor(2.4719, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Test for the real work\n",
    "for i, entry in enumerate(train_loader, 0):\n",
    "    xs, ys = entry[0], entry[1]\n",
    "    \n",
    "    print(xs.shape)\n",
    "    print(ys.shape)\n",
    "    \n",
    "    outputs = model(xs)\n",
    "    \n",
    "    # Flatten the label\n",
    "    ys = ys.view(-1)\n",
    "\n",
    "    print(outputs.shape)\n",
    "    print(ys.shape)\n",
    "    \n",
    "    print(ys)\n",
    "    \n",
    "    loss = criterion(outputs, ys)\n",
    "    print(loss)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKsYU03Nkfpw",
    "outputId": "a6006639-7874-4394-ccc1-b58cd88af89d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1145] at entry 0 and [548] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-10682932e135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-bf0909efd8b0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epochs, model, train_loader, test_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0;31m# Loop over the data in batch sizes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0;31m# get the inputs; data is a one input (batch size), and y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1145] at entry 0 and [548] at entry 1"
     ]
    }
   ],
   "source": [
    "trained_model, loss_history, val_loss_history = train_model(epochs, model, train_loader, test_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "keinNjOmkfpx",
    "outputId": "76b14142-0506-4857-80dc-1227d8c617fd"
   },
   "outputs": [],
   "source": [
    "show_losses(loss_history, val_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99DY1ETOkfpx"
   },
   "outputs": [],
   "source": [
    "save_model_file = '/home/mees/Desktop/Machine_Learning/subcellular_location/models/trained_protein_classifier_part2.pt'\n",
    "torch.save(model.state_dict, save_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ik ga de outputs controleren van de verschillende layers of die hetzelfde zijn als Fastai outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(test_loader, 0):\n",
    "    # get the inputs; data is a one input (batch size), and y\n",
    "\n",
    "    x, y = data\n",
    "    x = x.squeeze(0) # Squeeze x in the correct shape\n",
    "    y = y.squeeze(0) # Squeeze y in the correct shape\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    output = model(x)   \n",
    "    output = output.unsqueeze(0) # For the correct shape\n",
    "    loss = criterion(output, y)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLlLv3BVkfpx",
    "outputId": "a9d34cdd-4d40-4753-afb2-3b8f7c60df15"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-59b6bd33a9dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Show the accuracy on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-128-0dd93e7cab89>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Squeeze y in the correct shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-8b020a65e3ff>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-eedab9e09650>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-a3ac69f1cf6a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mhiddens_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiddens_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-6ae087298e53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setweights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-6ae087298e53>\u001b[0m in \u001b[0;36m_setweights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mraw_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{weight}_raw'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    981\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[1;32m    982\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Show the accuracy on test data\n",
    "accuracy(trained_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THbPwNEKkfpy",
    "outputId": "6929400f-45b9-4ada-f680-e9f7ceb5a985"
   },
   "outputs": [],
   "source": [
    "# Show the accuracy on train data\n",
    "accuracy(trained_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FbCIF6Vekfpy",
    "outputId": "7771899f-5e6e-460b-bd3a-8c14098aae56"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    all_predicted = []\n",
    "    ys = []\n",
    "    \n",
    "    for ind, data in enumerate(test_loader, 0):\n",
    "        x, y = data\n",
    "        x = x.squeeze(0) # Squeeze x in the correct shape\n",
    "        y = y.squeeze(0) # Squeeze y in the correct shape\n",
    "\n",
    "        output = model(x)\n",
    "        output = output.unsqueeze(0)\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        all_predicted.append(predicted.item())\n",
    "        ys.append(y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dyS-kzfdkfpy",
    "outputId": "b3a6ad8d-b851-43d8-ea25-d4e925779f5f"
   },
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(ys, all_predicted)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix, display_labels=list(label_dict.values()))\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "disp.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from this confusion matrix everything is mapped to class 4, this class is over represented ~5000 out of 16000 examples. So the cost is lowest to predict that one. Therefore, we have to use a weighted random sampler. And probably also batches to train faster. So first, implement training in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssrnJCMGkfpz",
    "outputId": "da80c4cc-f824-47f4-da19-7394882254d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Membrane': 0, 'Endosome': 1, 'Cytoplasm': 2, 'Mitochondrion': 3, 'pass membrane protein': 4, 'Single': 5, 'Nucleus': 6, 'Cell membrane': 7, 'Secreted': 8, 'Golgi apparatus': 9, 'Cell junction': 10, 'Peroxisome': 11, 'Cell projection': 12, 'Peripheral membrane protein': 13, 'Cytoplasmic vesicle': 14, 'Lysosome': 15, 'Chromosome': 16, 'anchor': 17, 'Endoplasmic reticulum membrane': 18, 'Cytoplasmic granule': 19, 'Cell surface': 20, 'Endoplasmic reticulum': 21, 'Endoplasmic reticulum lumen': 22, 'Melanosome': 23, 'Virion': 24, 'Vesicle': 25, 'Isoform 1': 26, 'Peroxisome matrix': 27, 'Sarcoplasmic reticulum': 28, 'Endomembrane system': 29, 'Membrane raft': 30, 'Midbody': 31, 'Lipid droplet': 32}\n"
     ]
    }
   ],
   "source": [
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testen met Fastai als referentie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.0000, -0.0000,  0.2283, -0.0674,  0.0000,  0.0000],\n",
       "        [-0.8458,  6.0831,  0.8827, -1.7225,  1.2066,  1.5383, -0.6647],\n",
       "        [-0.4909, -1.9670,  2.3101, -0.0000, -2.5325,  4.1651, -0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.2306, -0.0000,  1.5061,  0.0000,  0.0000,  0.0360, -0.0000],\n",
       "        [-0.0000,  0.0000,  0.0902,  1.2305,  1.1262,  0.0000, -0.0000],\n",
       "        [-0.0000, -2.6635,  1.5061,  1.3771,  0.0000,  0.0000, -0.0000],\n",
       "        [-0.0000, -0.0000,  2.3669, -0.7374,  1.4977, -1.2494,  0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding dropout\n",
    "encoder = torch.nn.Embedding(10, 7, padding_idx=1)\n",
    "emb_drop = torch.nn.Dropout(p=0.5)\n",
    "tst_inp = torch.randint(0,10,(8,))\n",
    "tst_out = emb_drop(encoder(tst_inp))\n",
    "tst_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(torch.nn.Module):\n",
    "    \"Apply dropout to an Embedding with probability emp_p\"\n",
    "\n",
    "    def __init__(self, emb_p=0):\n",
    "        super(EmbeddingDropout, self).__init__()\n",
    "        \n",
    "        self.emb_p = emb_p\n",
    "\n",
    "    def forward(self, inp):\n",
    "       \n",
    "        drop = torch.nn.Dropout(self.emb_p)\n",
    "        placeholder = torch.ones((inp.size(0), 1))\n",
    "        mask = drop(placeholder)      \n",
    "        out = inp * mask\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "tensor([5])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 7])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d454925e4c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtst_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtst_inp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "tst_inp = torch.randint(0,10,(1,))\n",
    "print(tst_inp.shape)\n",
    "print(tst_inp)\n",
    "encoder = torch.nn.Embedding(10, 7, padding_idx=1)\n",
    "encoded = encoder(tst_inp)\n",
    "print(encoded.shape)\n",
    "emb_drop = EmbeddingDropout(emb_p=0.5)\n",
    "tst_out = emb_drop(encoded)\n",
    "print(tst_out.shape)\n",
    "for i in range(8):\n",
    "    assert (tst_out[i]==0).all() or torch.allclose(tst_out[i], 2*encoder.weight[tst_inp[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8539,  2.2006, -0.0222,  0.4224,  1.2578, -0.5769, -0.7109],\n",
      "        [ 1.2339,  0.9903, -1.4282,  0.4318, -1.2289,  0.8093, -0.5293],\n",
      "        [ 2.0111, -0.3286,  0.6406,  1.2247,  0.5189,  0.4388, -0.2033],\n",
      "        [-0.7689, -1.3048,  0.5878,  0.1067,  0.1493, -0.9369,  0.1112],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-1.0289,  0.4262, -1.3303, -0.7120,  1.0064,  0.7244, -0.7196],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "torch.Size([8, 7])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.]])\n",
      "torch.Size([8, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
       "        [ 4.0222, -0.6571,  1.2812,  2.4494,  1.0377,  0.8776, -0.4066],\n",
       "        [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-2.0579,  0.8524, -2.6606, -1.4240,  2.0127,  1.4488, -1.4393],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_inp = torch.randint(0,10,(8,))\n",
    "drop = torch.nn.Dropout(0.5)\n",
    "encoder = torch.nn.Embedding(10, 7, padding_idx=1)\n",
    "encoded = encoder(tst_inp)\n",
    "print(encoded)\n",
    "print(encoded.shape)\n",
    "\n",
    "placeholder = torch.ones((tst_inp.size(0), 1))\n",
    "mask = drop(placeholder)\n",
    "print(mask)\n",
    "print(mask.shape)\n",
    "\n",
    "encoded * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 1, 6, 9, 3, 7, 1])\n",
      "torch.Size([8])\n",
      "tensor([[-0.9948, -1.3262, -1.4499,  0.1425,  0.3206, -0.5852,  0.4061],\n",
      "        [-1.8594,  0.1413,  1.6815,  0.9142, -0.5407, -0.8299,  0.1996],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.5156,  0.3203,  1.5523,  1.3523,  1.1633, -0.5602, -0.1511],\n",
      "        [ 0.9568, -0.6867,  1.0243,  1.7308, -0.3247,  0.3144, -0.0669],\n",
      "        [ 1.0880, -1.1082,  2.1468, -0.7218,  1.9135, -0.3273,  0.4820],\n",
      "        [-0.5337,  0.5620, -2.7735,  1.1568, -0.2488,  0.5844, -2.2863],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9948, -1.3262, -1.4499,  0.1425,  0.3206, -0.5852,  0.4061],\n",
       "        [-1.8594,  0.1413,  1.6815,  0.9142, -0.5407, -0.8299,  0.1996],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.5156,  0.3203,  1.5523,  1.3523,  1.1633, -0.5602, -0.1511],\n",
       "        [ 0.9568, -0.6867,  1.0243,  1.7308, -0.3247,  0.3144, -0.0669],\n",
       "        [ 1.0880, -1.1082,  2.1468, -0.7218,  1.9135, -0.3273,  0.4820],\n",
       "        [-0.5337,  0.5620, -2.7735,  1.1568, -0.2488,  0.5844, -2.2863],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding dropout\n",
    "encoder = torch.nn.Embedding(10, 7, padding_idx=1)\n",
    "emb_drop = EmbeddingDropout(emb_p=0.5)\n",
    "tst_inp = torch.randint(0,10,(8,))\n",
    "print(tst_inp)\n",
    "print(tst_inp.shape)\n",
    "encoded = encoder(tst_inp)\n",
    "print(encoded)\n",
    "tst_out = emb_drop(encoded)\n",
    "tst_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-197-d05f4a6a9746>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-197-d05f4a6a9746>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    (tst_out==).sum()\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "(tst_out==).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tst_out==1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 20, got 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-9c2712273055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-d575e1e6e46e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# Go trough one LSTM layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# Detach hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-6ae087298e53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setweights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    178\u001b[0m             raise RuntimeError(\n\u001b[1;32m    179\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m--> 180\u001b[0;31m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 20, got 100"
     ]
    }
   ],
   "source": [
    "tst = AWD_LSTM(2, 100, 20, 10, 0.2, 0.02, 0.1, 0.2)\n",
    "x = torch.randint(0, 100, (1,5))\n",
    "bs,sl = x.shape[:2]\n",
    "print(bs)\n",
    "print(sl)\n",
    "r = tst(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "proteinClassifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
