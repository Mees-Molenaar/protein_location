{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVoWWP2TkfpH"
   },
   "source": [
    "# Protein Classifier using AWD-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "05m_tDaZkfpQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import sklearn.metrics as metrics\n",
    "from IPython.display import HTML, display\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mp69KYdNkfpS"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "56HVWDq5kfpU",
    "outputId": "034e4862-f298-4b3f-e3be-b653ffaac8e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0frf8WW_kfpW",
    "outputId": "e8496ecc-dc07-43da-a355-0b2fd1a85b0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4c98851810>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Q-6XVNYTkfpX",
    "outputId": "d567d9d9-1fb1-4dd3-8565-feeb354188de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "table, th, td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<style>\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qq9C8202kfpZ"
   },
   "source": [
    "## Usefull Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "V7-BmN0zkfpb"
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation loss using matplotlib\n",
    "def show_losses(training_loss, validation_loss):\n",
    "    r\"\"\"Plot graphs with on x-axis epochs and y-axis the loss.\n",
    "\n",
    "        This graph contains both the `training_loss` and `validation_loss`\n",
    "        for each epoch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        training_loss : array\n",
    "            Containing the training loss for each epoch.\n",
    "        validation_loss : array\n",
    "            Containing the validation loss for each epoch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Plot of the two graphs.\n",
    "\n",
    "    \"\"\"\n",
    "    np_loss = np.asarray(training_loss)\n",
    "    np_val_loss = np.asarray(validation_loss)\n",
    "    plt.plot(np_loss, label='Train loss')\n",
    "    plt.plot(np_val_loss, label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Cross Entropy Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dX9RNmgTkfpZ"
   },
   "outputs": [],
   "source": [
    "def train_model(epochs, model, train_loader, test_loader, optimizer, criterion):\n",
    "    r\"\"\"Train the resnet18 model.\n",
    "    \n",
    "    Train the resnet18 model for `epochs`. Using the\n",
    "    `optimizer` and the loss function in `criterion`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : int\n",
    "        Number of epochs to train the model.\n",
    "    model : torch Model\n",
    "        Model that is being trained.\n",
    "    train_loader : torch Dataloader\n",
    "        Dataloader containing the train data to train the model.\n",
    "    test_loader : torch Dataloader\n",
    "        Dataloader containing the test data to validate the model.\n",
    "    optimizer : torch Optimizer\n",
    "        Optimizer used to optimize the model.\n",
    "    criterion : torch.nn Lossfunction\n",
    "        Loss function used. This loss function has to be minimilized by the model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch Model\n",
    "        The trained model.\n",
    "    loss_history : array\n",
    "        Training loss for each epoch.\n",
    "    val_loss_history : array\n",
    "        Validation loss for each epoch.\n",
    "    \n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Inititiate loss variables\n",
    "        epoch_loss = 0.0\n",
    "        epoch_val_loss = 0.0\n",
    "        train_correct_pred = 0\n",
    "        train_total = 0\n",
    "        test_correct_pred = 0\n",
    "        test_total = 0\n",
    "\n",
    "        print(f'Epoch: {str(epoch + 1)}')\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Alterate between train and validaton phase\n",
    "        for phase in ['train', 'val']:\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train(True) # Set model to training mode\n",
    "                data_loader = train_loader\n",
    "            else:\n",
    "                model.train(False) # Set model to evaluate mode\n",
    "                data_loader = test_loader\n",
    "\n",
    "\n",
    "        # Loop over the data in batch sizes.\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "        # get the inputs; data is a one input (batch size), and y\n",
    "\n",
    "            xs, ys = data[0], data[1]\n",
    "\n",
    "            if ys.size(0) == batch_size:\n",
    "\n",
    "                outputs = model(xs) \n",
    "\n",
    "                # Flatten the label\n",
    "                ys = ys.view(-1)             \n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                loss = criterion(outputs, ys)\n",
    "                total = ys.size(0)\n",
    "\n",
    "                # Add loss to each epoch\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "\n",
    "                    train_total += total\n",
    "                    train_correct_pred += (predicted == ys).sum().item()  \n",
    "\n",
    "                else:\n",
    "                    epoch_val_loss += loss.item()\n",
    "\n",
    "                    test_total += total\n",
    "                    test_correct_pred += (predicted == ys).sum().item()\n",
    "\n",
    "        train_accuracy = np.round((train_correct_pred / train_total * 100), 2)\n",
    "        test_accuracy = np.round((test_correct_pred / test_total * 100), 2)\n",
    "\n",
    "        epoch_loss /= len(train_loader)\n",
    "        epoch_val_loss /= len(test_loader)\n",
    "\n",
    "        loss_history.append(epoch_loss)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "\n",
    "        print(f'Epoch {str(epoch)} Cross entropy Train Loss: {str(epoch_loss)} ; Validation Loss: {str(epoch_val_loss)}.; Train Accuracy: {str(train_accuracy)}%; Test Accuracy: {str(test_accuracy)}% ')\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time_minutes = (end_time - start_time) / 60\n",
    "\n",
    "        print(f'Epoch duration {str(epoch_time_minutes)} minutes.')\n",
    "\n",
    "    print('Finished Training')\n",
    "    return model, loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dObmbgbvkfpc"
   },
   "outputs": [],
   "source": [
    "# Tokenize the protein sequence (or any sequence) in kmers.\n",
    "def tokenize(df, protein_seqs_column, kmer_sz, premade_vocab=False):\n",
    "    \n",
    "    if not premade_vocab:\n",
    "        kmers = set()\n",
    "        # Loop over protein sequences\n",
    "        for protein_seq in df[protein_seqs_column]:\n",
    "            # Loop over the whole sequence\n",
    "            for i in range(len(protein_seq) - (kmer_sz - 1)):\n",
    "                # Add kmers to the set, thus only unique kmers will remain\n",
    "                kmers.add(protein_seq[i: i + kmer_sz])\n",
    "\n",
    "        # Map kmers for one hot-encoding\n",
    "        kmer_to_id = dict()\n",
    "        id_to_kmer = dict()\n",
    "\n",
    "        for ind, kmer in enumerate(kmers):\n",
    "            kmer_to_id[kmer] = ind\n",
    "            id_to_kmer[ind] = kmer\n",
    "\n",
    "        vocab_sz = len(kmers)\n",
    "\n",
    "        assert vocab_sz == len(kmer_to_id.keys())\n",
    "    \n",
    "    else:\n",
    "        kmer_to_id, id_to_kmer = premade_vocab\n",
    "        vocab_sz = len(kmer_to_id)\n",
    "    \n",
    "    # Tokenize the protein sequence to integers\n",
    "    tokenized = []\n",
    "    for i, protein_seq in enumerate(df[protein_seqs_column], 0):\n",
    "        sequence = []\n",
    "        \n",
    "        # If the kmer can't be found these indexes should be deleted\n",
    "        remove_idxs = []\n",
    "        \n",
    "        for i in  range(len(protein_seq) - (kmer_sz -1)):\n",
    "            # Convert kmer to integer\n",
    "            kmer = protein_seq[i: i + kmer_sz]\n",
    "            \n",
    "            # For some reason, some kmers miss. Thus these sequences have to be removed\n",
    "            try:\n",
    "                sequence.append(kmer_to_id[kmer])\n",
    "            except:\n",
    "                remove_idxs.append(i)\n",
    "            \n",
    "        tokenized.append(sequence)\n",
    "            \n",
    "    df['tokenized_seqs'] = tokenized\n",
    "    \n",
    "    df.drop(remove_idxs, inplace=True)\n",
    "    \n",
    "    return df, vocab_sz, kmer_to_id, id_to_kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "b0Sufgyqkfpc"
   },
   "outputs": [],
   "source": [
    "# Function to show the accuracy of the model.\n",
    "def accuracy(model, data_loader):\n",
    "    r\"\"\"Calculate accuracy of the model.\n",
    "    \n",
    "    Calculates the accuracy of the trained `model`\n",
    "    for the `data_loader` in the input.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch Model\n",
    "        The trained model.\n",
    "    data_loader : torch Dataloader\n",
    "        Torch dataloader containing the samples you want to test the accuracy for.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Prints the accurucy of the `model` for the samples in the `data_loader`\n",
    "    \n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for ind, data in enumerate(data_loader, 0):\n",
    "            x, y = data\n",
    "            x = x.squeeze(0) # Squeeze x in the correct shape\n",
    "            y = y.squeeze(0) # Squeeze y in the correct shape\n",
    "            \n",
    "            output = model(x)\n",
    "            output = output.unsqueeze(0)\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "\n",
    "        accuracy = np.round((correct / total * 100), 2)\n",
    "        print(f'Accuracy of the network is {str(accuracy)}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcW0LHbzkfpe"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-40604388b89b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'content/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "drive.mount('content/', force_remount=True)\n",
    "base = Path('/content/content/My Drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google drive\n",
    "#file_paths = Path('/content/content/MyDrive/subcellular-location/v2/')\n",
    "\n",
    "# Linux path\n",
    "file_paths = Path('/home/mees/Desktop/Machine_Learning/subcellular_location/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "tbRFWsBukfpe",
    "outputId": "2290f72f-7005-49f3-f695-ea688643ffb6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasmic vesicle, sec...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Early endosome {ECO:0000...</td>\n",
       "      <td>Endosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton,...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Mitochondrion {ECO:00003...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "      <td>Cell membrane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence  \\\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...   \n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...   \n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...   \n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...   \n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...   \n",
       "\n",
       "                           Subcellular location [CC]       Location  \n",
       "0  SUBCELLULAR LOCATION: Cytoplasmic vesicle, sec...      Cytoplasm  \n",
       "1  SUBCELLULAR LOCATION: Early endosome {ECO:0000...       Endosome  \n",
       "2  SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton,...      Cytoplasm  \n",
       "3  SUBCELLULAR LOCATION: Mitochondrion {ECO:00003...  Mitochondrion  \n",
       "4  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  Cell membrane  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = Path('data/processed/protein_data_2021-04-04.csv')\n",
    "data_file = file_paths / data_file\n",
    "df = pd.read_csv(data_file, sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "sNGEB5i4kfpf",
    "outputId": "4a0f1948-d4c9-4891-df86-74d7100fa941"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Endosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence       Location\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...      Cytoplasm\n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...       Endosome\n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...      Cytoplasm\n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  Mitochondrion\n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  Cell membrane"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['Subcellular location [CC]'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNktYsKWkfpf"
   },
   "source": [
    "### Tokenize the Data\n",
    "\n",
    "The data has to be tokenized according to the Language Model trained before, therefore, we have to load in that dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vJwNg0L9kfpg"
   },
   "outputs": [],
   "source": [
    "# Set-up numpy generator for random numbers\n",
    "random_number_generator = np.random.default_rng(seed=42)\n",
    "KMER_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "j3QzdCOfkfpg"
   },
   "outputs": [],
   "source": [
    "# Load the vocabolary from the Language Model\n",
    "vocab_save_file = Path('data/interim/LM_vocab.pkl')\n",
    "vocab_save_file = file_paths / vocab_save_file\n",
    "vocab = pickle.load(open(vocab_save_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AtZA6bQ5kfpg"
   },
   "outputs": [],
   "source": [
    "# Tokenize the protein sequence\n",
    "df, vocab_sz, kmer_to_id, id_to_kmer = tokenize(df, 'Sequence', KMER_SIZE, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "v-GIN4-rkfph",
    "outputId": "30a21a4d-c729-406c-9eb4-feecbdf60810"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Location</th>\n",
       "      <th>tokenized_seqs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[3884, 8570, 3840, 6832, 2277, 2221, 1020, 904...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Endosome</td>\n",
       "      <td>[8772, 7207, 1857, 1688, 5461, 3901, 4899, 424...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[1565, 3797, 2513, 516, 1428, 6558, 6568, 7337...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "      <td>[8939, 2538, 9262, 4438, 2547, 302, 60, 3064, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>[8939, 6897, 6013, 1021, 3034, 2863, 8501, 697...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence       Location  \\\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...      Cytoplasm   \n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...       Endosome   \n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...      Cytoplasm   \n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  Mitochondrion   \n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  Cell membrane   \n",
       "\n",
       "                                      tokenized_seqs  \n",
       "0  [3884, 8570, 3840, 6832, 2277, 2221, 1020, 904...  \n",
       "1  [8772, 7207, 1857, 1688, 5461, 3901, 4899, 424...  \n",
       "2  [1565, 3797, 2513, 516, 1428, 6558, 6568, 7337...  \n",
       "3  [8939, 2538, 9262, 4438, 2547, 302, 60, 3064, ...  \n",
       "4  [8939, 6897, 6013, 1021, 3034, 2863, 8501, 697...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZGZZKjMkfph",
    "outputId": "c32523a2-881e-42f9-a52e-50f143d7c48f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16614"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_s83p4ekfpi"
   },
   "source": [
    "### Numericalize the label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6RnA7Ahwkfpi",
    "outputId": "21fbf819-505f-455f-dbd5-49004a725221"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16614"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some fields are NaN, remove these\n",
    "df.dropna(inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqwWz9y7kfpi"
   },
   "source": [
    "Create a dictionary to numericalize the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "rVTVFS09kfpj"
   },
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "\n",
    "for i, label in enumerate(df['Location'].unique(), 0):\n",
    "    label_dict[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "GSU8mia8kfpj"
   },
   "outputs": [],
   "source": [
    "def numericalizeClass(df, class_column, label_dict, label_column='Label'):\n",
    "    df[label_column] = df[class_column].map(label_dict)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "q-gQqK4Zkfpk",
    "outputId": "888c57aa-9f2f-4a3c-c3ea-65d897189647"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Location</th>\n",
       "      <th>tokenized_seqs</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[3884, 8570, 3840, 6832, 2277, 2221, 1020, 904...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Endosome</td>\n",
       "      <td>[8772, 7207, 1857, 1688, 5461, 3901, 4899, 424...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[1565, 3797, 2513, 516, 1428, 6558, 6568, 7337...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "      <td>[8939, 2538, 9262, 4438, 2547, 302, 60, 3064, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>[8939, 6897, 6013, 1021, 3034, 2863, 8501, 697...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence       Location  \\\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...      Cytoplasm   \n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...       Endosome   \n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...      Cytoplasm   \n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  Mitochondrion   \n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  Cell membrane   \n",
       "\n",
       "                                      tokenized_seqs  Label  \n",
       "0  [3884, 8570, 3840, 6832, 2277, 2221, 1020, 904...      0  \n",
       "1  [8772, 7207, 1857, 1688, 5461, 3901, 4899, 424...      1  \n",
       "2  [1565, 3797, 2513, 516, 1428, 6558, 6568, 7337...      0  \n",
       "3  [8939, 2538, 9262, 4438, 2547, 302, 60, 3064, ...      2  \n",
       "4  [8939, 6897, 6013, 1021, 3034, 2863, 8501, 697...      3  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = numericalizeClass(df, 'Location', label_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JGmFcAjJkfpk",
    "outputId": "c8a3c476-176a-44a9-edb3-40dbd8b5fecb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cytoplasm': 0,\n",
       " 'Endosome': 1,\n",
       " 'Mitochondrion': 2,\n",
       " 'Cell membrane': 3,\n",
       " 'Nucleus': 4,\n",
       " 'Endoplasmic reticulum': 5,\n",
       " 'Secreted': 6,\n",
       " 'Golgi apparatus': 7,\n",
       " 'Extracellular': 8,\n",
       " 'Peroxisome': 9,\n",
       " 'Lysosome/ Vacuole': 10}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "wlpBWvdWkfpl",
    "outputId": "30c5f808-392a-43fd-e799-742ca4ef9fa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3647.,   95.,  858., 3993., 5462.,  651., 1079.,  316.,  375.,\n",
       "         138.]),\n",
       " array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAnUfQNPkfpl"
   },
   "source": [
    "! It might help to rebalance the classes since classes 0, 3 and 4 are now over represented. Therefore, the model can just default to class 4. Since, if the model predicts class 4 for every sequence it will have an accuracy of around 34% already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jSGjtNVkfpm"
   },
   "source": [
    "An other point is that the sequence should have some information to work with, according to this paper:\n",
    "> https://www.nature.com/articles/s41598-019-38746-w\n",
    "\n",
    "A motif can be in between 3-20 amino acids, thus 1-6/7 kmers. I chose 5 kmers. However training time took way longer in that way. Therefore, I know choose another seqeunce length to reduce training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "hV_YQwZCkfpm"
   },
   "outputs": [],
   "source": [
    "df['Length'] = df['tokenized_seqs'].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "M1Uk4Cffkfpm",
    "outputId": "45aa26d8-3bba-4630-dafb-b4318e382baa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.,  21.,  15.,   5.,  18.,  31.,  57.,  93., 104., 163.]),\n",
       " array([  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90., 100.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['Length'], range=(0, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoMVJdmwkfpn"
   },
   "source": [
    "Based on this, and reduce training time. I now choose a sequence length of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2SGZXjgkfpn",
    "outputId": "2c59f273-d341-40e0-8242-a8f76a04d6ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16552"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['Length'] >= 50]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Se0r2uHnkfpo"
   },
   "source": [
    "This only removed about 60 entries.\n",
    "Now, to address class imbalance. Create a weighted sampler. However, the current weight labels are not correct. For each entry there should be a weight attached.\n",
    "\n",
    "Unfortunately, adding weight labeling to the cross entropy loss doesn't work. So, for each label we are selecteing 100 randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3628, 1: 95, 2: 854, 3: 3966, 4: 5459, 5: 649, 6: 1072, 7: 316, 8: 375, 9: 53, 10: 85}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.562293274531422,\n",
       " 174.23157894736843,\n",
       " 19.38173302107728,\n",
       " 4.173474533535048,\n",
       " 3.032057153324785,\n",
       " 25.503852080123266,\n",
       " 15.440298507462686,\n",
       " 52.379746835443036,\n",
       " 44.138666666666666,\n",
       " 312.3018867924528,\n",
       " 194.7294117647059]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_of_labels = dict(df['Label'].value_counts(sort=False))\n",
    "\n",
    "weight_labels = []\n",
    "\n",
    "print(total_of_labels)\n",
    "\n",
    "for total_of_label in total_of_labels.values():\n",
    "    weight = 1 / (total_of_label / len(df))\n",
    "    weight_labels.append(weight)\n",
    "    \n",
    "weight_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_total = len(df)\n",
    "\n",
    "threshold = 100\n",
    "\n",
    "for label, amount in total_of_labels.items():\n",
    "    if amount > threshold:\n",
    "        keep_fraction = (threshold / (amount / 100) / 100)\n",
    "        del_fraction = 1 - keep_fraction\n",
    "\n",
    "        label = str(label)\n",
    "\n",
    "        df = df.drop(df.query('Label == ' + label).sample(frac=del_fraction).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([100.,  95., 100., 100., 100., 100., 100., 100., 100., 138.]),\n",
       " array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aMErvL7kfpo"
   },
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ZdainIjGkfpp"
   },
   "outputs": [],
   "source": [
    "class AminoClassifierDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, num_classes, bs=1):\n",
    "        self.df = df\n",
    "        self.num_classes = num_classes \n",
    "        self.batch_sz = bs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        x = torch.LongTensor(self.df.iloc[idx]['tokenized_seqs'])\n",
    "        x = x.to(dev) \n",
    "        \n",
    "        y = torch.LongTensor([self.df.iloc[idx]['Label']])\n",
    "        y = y.to(dev)\n",
    "    \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTJPhWeQkfpp"
   },
   "source": [
    "## The Protein Classifier\n",
    "\n",
    "Creating the complete protein from its parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tS3F6Jmkkfpp"
   },
   "source": [
    "### AWD-LSTM\n",
    "Start with the AWD-LSTM, which encodes the protein sequence and is already trained.\n",
    "\n",
    "I can add more dropout and I should add including the last hidden layers (cell and hidden states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(torch.nn.Module):\n",
    "    \"Apply dropout to an Embedding with probability emp_p\"\n",
    "\n",
    "    def __init__(self, emb_p=0):\n",
    "        super(EmbeddingDropout, self).__init__()\n",
    "        \n",
    "        self.emb_p = emb_p\n",
    "\n",
    "    def forward(self, inp):\n",
    "       \n",
    "        drop = torch.nn.Dropout(self.emb_p)\n",
    "        placeholder = torch.ones((inp.size(0), 1)).to(dev)\n",
    "        mask = drop(placeholder)      \n",
    "        out = inp * mask\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "8-QAxw1zkfpp"
   },
   "outputs": [],
   "source": [
    "class WeightDropout(torch.nn.Module):\n",
    "    \"Apply dropout to LSTM's hidden-hidden weights\"\n",
    "\n",
    "    def __init__(self, module, weight_p):\n",
    "        super(WeightDropout, self).__init__()\n",
    "        self.module = module\n",
    "        self.weight_p = weight_p\n",
    "\n",
    "        # Save the name of the layer weights in a list\n",
    "        num_layers = module.num_layers\n",
    "        layer_base_name = 'weight_hh_l'      \n",
    "        self.layer_weights = [layer_base_name + str(i) for i in range(num_layers)]\n",
    "\n",
    "        # Make a copy of the weights in weightname_raw\n",
    "        for weight in self.layer_weights:\n",
    "\n",
    "            w = getattr(self.module, weight)\n",
    "            del module._parameters[weight]\n",
    "            self.module.register_parameter(f'{weight}_raw', torch.nn.Parameter(w))\n",
    "\n",
    "    def _setweights(self):\n",
    "        \"Apply dropout to the raw weights\"\n",
    "        for weight in self.layer_weights:\n",
    "            raw_w = getattr(self.module, f'{weight}_raw')\n",
    "            if self.training:\n",
    "                w = torch.nn.functional.dropout(raw_w, p=self.weight_p)\n",
    "            else:\n",
    "                w = raw_w.clone()\n",
    "            setattr(self.module, weight, w)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        return self.module(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "id": "l91JM4j3kfpq"
   },
   "outputs": [],
   "source": [
    "class AWD_LSTM(torch.nn.Module):\n",
    "    def __init__(self, num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz = 1):\n",
    "        super(AWD_LSTM, self).__init__()\n",
    "\n",
    "        # Embedding with droput\n",
    "        self.encoder = torch.nn.Embedding(vocab_sz, emb_dim)\n",
    "        self.emb_drop = EmbeddingDropout(emb_p=embed_p)\n",
    "\n",
    "\n",
    "        # Dropouts on the inputs and the hidden layers\n",
    "        self.input_dp = torch.nn.Dropout(p=input_p)\n",
    "        self.hid_dp = torch.nn.Dropout(p=hidden_p)\n",
    "\n",
    "        # Create a list of lstm layers with wieghtdropout\n",
    "        self.lstms = []\n",
    "        for i in range(num_layers):\n",
    "            self.lstms.append(\n",
    "                WeightDropout(nn.LSTM(input_size=emb_dim, hidden_size=hid_sz, num_layers=1), weight_p))\n",
    "        self.lstms = nn.ModuleList(self.lstms)\n",
    "\n",
    "        # Save all variables        \n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_sz = hid_sz\n",
    "        self.hidden_p = hidden_p\n",
    "        self.embed_p = embed_p\n",
    "        self.input_p = input_p\n",
    "        self.weight_p = weight_p\n",
    "        self.batch_sz = batch_sz\n",
    "\n",
    "        # Initialize hidden layers        \n",
    "        self.reset_hidden()\n",
    "        self.last_hiddens = (self.hidden_state, self.cell_state)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        \"\"\"Forward pass AWD-LSTM\"\"\" \n",
    "\n",
    "        bs, sl = xs.shape\n",
    "        \n",
    "        # Because sequences consisting of only padding are removed from the mini-batch, the mini-batch alters\n",
    "        # Therefore we have to adjust the hidden state for that\n",
    "        if bs != self.last_hiddens[0].shape[1]:\n",
    "            self._change_bs_hidden(bs)\n",
    "\n",
    "        ys = []\n",
    "\n",
    "        hiddens = self.last_hiddens\n",
    "\n",
    "        hidden_states = [hiddens]\n",
    "\n",
    "        for i, lstm in enumerate(self.lstms):\n",
    "\n",
    "            # Embed the input and add dropout to it  \n",
    "            x = xs[:, i]\n",
    "            \n",
    "            try:\n",
    "                embed = self.encoder(x)\n",
    "            except:\n",
    "                print('Embedding error:')\n",
    "                print(x)\n",
    "            embed_dp = self.emb_drop(embed)\n",
    "            \n",
    "            # Again add dropout, this feels like doing dropout on dropout, I dont know if it is worth\n",
    "          \n",
    "            input_dp = self.input_dp(embed_dp)\n",
    "\n",
    "            # Dropout on the hidden states\n",
    "            hiddens_dp = []\n",
    "\n",
    "            for hidden_state in hidden_states[i]:\n",
    "                hiddens_dp.append(self.hid_dp(hidden_state))\n",
    "\n",
    "            hiddens_dp = tuple(hiddens_dp)\n",
    "            \n",
    "            # Go trough one LSTM layer\n",
    "            output, hiddens = lstm(input_dp.view(1, bs, -1), hiddens_dp) \n",
    "\n",
    "            # Detach hidden states\n",
    "            det_hiddens = []\n",
    "\n",
    "            for hidden in hiddens:\n",
    "                det_hiddens.append(hidden.detach())\n",
    "\n",
    "            det_hiddens = tuple(det_hiddens)\n",
    "\n",
    "            hidden_states.append(det_hiddens)\n",
    "\n",
    "            y = output.view(bs, 1, -1)\n",
    "            \n",
    "            ys.append(y)\n",
    "\n",
    "        y = torch.stack(ys, dim=0)\n",
    "        \n",
    "        y = y.view(bs, sl, -1)\n",
    "\n",
    "        self.last_hiddens = hidden_states[-1]\n",
    "\n",
    "        return y, hidden_states\n",
    "\n",
    "    def reset_hidden(self):\n",
    "                   \n",
    "        self.hidden_state = torch.zeros((1, self.batch_sz, self.hid_sz)).to(dev)\n",
    "        self.cell_state = torch.zeros((1, self.batch_sz, self.hid_sz)).to(dev)\n",
    "        self.last_hiddens = (self.hidden_state, self.cell_state)\n",
    "        \n",
    "    def _change_bs_hidden(self, bs):\n",
    "        hidden_state = self.last_hiddens[0]\n",
    "        cell_state = self.last_hiddens[1]\n",
    "        \n",
    "        if bs > hidden_state.shape[1]:\n",
    "            self.reset_hidden()\n",
    "        else:\n",
    "        \n",
    "            corr_hidden_state = hidden_state[:,:bs,:]\n",
    "            corr_cell_state = cell_state[:,:bs,:]\n",
    "        \n",
    "            self.last_hiddens = (corr_hidden_state, corr_cell_state)\n",
    "                                  \n",
    "    def freeze_to(self , n):\n",
    "        \n",
    "        params_to_freeze = n * 4 + 1 # Since each LSTM layer has 4 parameters plus 1 to also freeze the encoder\n",
    "        \n",
    "        total_params = len(list(self.parameters()))\n",
    "        \n",
    "        for i, parameter in enumerate(self.parameters()):\n",
    "            parameter.requires_grad = True\n",
    "            \n",
    "            if i < params_to_freeze:\n",
    "                parameter.requires_grad = False\n",
    "            \n",
    "            \n",
    "        for name, parameter in self.named_parameters():\n",
    "            print(name)\n",
    "            print(parameter.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gr8AWMzkfpq"
   },
   "source": [
    "### SentenceEncoder\n",
    "\n",
    "This part encodes the whole sequences in seq_lenghts using the pretrained AWD-LSTM language model.\n",
    "\n",
    "We use the Identity class to replace the decoder in the original AWD-LSTM. \n",
    "\n",
    "Finally, the model should not be updated. Therefore, the forward pass is in torch.no_grad()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "id": "jlFVmQtqkfpr"
   },
   "outputs": [],
   "source": [
    "class Identity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "id": "cA2cTY_Ekfpr"
   },
   "outputs": [],
   "source": [
    "class SentenceEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, model, pad_token):\n",
    "        super(SentenceEncoder, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.model = model\n",
    "        self.pad_token = pad_token\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        with torch.no_grad():      \n",
    "            \n",
    "            # First element is batch size, second element is the sequence\n",
    "            inp_size = inp.shape[1]\n",
    "            bs = inp.shape[0]\n",
    "            \n",
    "            # Padded parts should not be taken into account and these padded sequences cannot go trough the embedding\n",
    "            padded = inp == self.pad_token\n",
    "            \n",
    "            # It is nicer to add padding\n",
    "            max_iterations = int(inp_size / self.seq_len)\n",
    "\n",
    "            hidden_state_outputs = []\n",
    "            cell_state_outputs = []\n",
    "            \n",
    "            # Save masked output for the pooling calculations\n",
    "            masked_outputs = []\n",
    "            corr_size = 0\n",
    "            \n",
    "            for i in range(0, self.seq_len * max_iterations, self.seq_len):\n",
    "\n",
    "                # Calculate the corrected batch size, meaning that a sequence only consisting of \n",
    "                # the padding_token will be not put in the LSTM\n",
    "                total_padded = padded[:,i : i + self.seq_len].sum(dim = 1) == self.seq_len\n",
    "                corr_bs = (total_padded == False).sum()\n",
    "                \n",
    "                _, hidden = self.model(inp[:corr_bs, i: i + self.seq_len])\n",
    "                \n",
    "                for states in hidden:\n",
    "                    hidden_state_output = states[0]\n",
    "                    cell_state_output = states[1]\n",
    "                    \n",
    "                    # In order to concate the hidden outputs we have to correct for the decrease in batch size\n",
    "                    if corr_bs != bs:\n",
    "                        corr_size = bs - corr_bs\n",
    "                        corr_hidden = torch.zeros(1, corr_size, hidden_state_output.shape[2])\n",
    "                        corr_mask = torch.zeros(1, corr_size)\n",
    "                        \n",
    "\n",
    "                        hidden_state_output = torch.cat([hidden_state_output, corr_hidden], dim = 1)\n",
    "                        cell_state_output = torch.cat([cell_state_output, corr_hidden], dim = 1) \n",
    "                    \n",
    "                    \n",
    "                    # If no corrections for sequence length is made the mask will be one (there is no mask)\n",
    "                    if corr_size == 0:\n",
    "                        mask = torch.ones(1, bs)\n",
    "                    # If there are corrections made, mask will be a combination of ones for the sequence in the batch\n",
    "                    # that is not corrected and zeros for the sequence that is corrected \n",
    "                    else:\n",
    "                        unmasked = bs - corr_size\n",
    "                        mask = torch.ones(1, unmasked)\n",
    "                        \n",
    "                        mask = torch.cat([mask, corr_mask], dim = 1)\n",
    "                        \n",
    "                        \n",
    "                    masked_outputs.append(mask)\n",
    "                    hidden_state_outputs.append(hidden_state_output)\n",
    "                    cell_state_outputs.append(cell_state_output)\n",
    "                \n",
    "                   \n",
    "            hidden_state_outputs = torch.cat(hidden_state_outputs, dim = 0)\n",
    "            cell_state_outputs = torch.cat(cell_state_outputs, dim = 0)\n",
    "            masked_outputs = torch.cat(masked_outputs, dim = 0)\n",
    "\n",
    "            return (hidden_state_outputs, cell_state_outputs), masked_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeO_z7m-kfpr"
   },
   "source": [
    "### PoolingLinearClassifier\n",
    "\n",
    "The encoded sequence is needed to be pooled, otherwise the model can not use the information for classification.\n",
    "\n",
    "Then, the data is normalized using batchnorm.\n",
    "Dropout is applied to prevent overfitting.\n",
    "And linear layers with a ReLU activiation are used to classify the pooled protein data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "id": "nCOiXkDUkfpr"
   },
   "outputs": [],
   "source": [
    "class PoolingLinearClassifier(torch.nn.Module):\n",
    "    r\"\"\"Pool the outputs from the encoder and classify it.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, batch_sz):\n",
    "        super(PoolingLinearClassifier, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.batch_sz = batch_sz\n",
    "        \n",
    "        if batch_sz > 1:\n",
    "            \n",
    "            self.layers = nn.Sequential(\n",
    "                nn.BatchNorm1d(1150 * 3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Dropout(p=0.2, inplace=False),\n",
    "                nn.Linear(in_features=1150 * 3, out_features=50, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Dropout(p=0.1, inplace=False),\n",
    "                nn.Linear(in_features=50, out_features=num_classes, bias=True)\n",
    "            )\n",
    "        else:\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Dropout(p=0.2, inplace=False),\n",
    "                nn.Linear(in_features=1150 * 3, out_features=50, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=0.1, inplace=False),\n",
    "                nn.Linear(in_features=50, out_features=num_classes, bias=True)\n",
    "            )\n",
    "            \n",
    "        \n",
    "    \n",
    "    def forward(self, inp):\n",
    "        output_encoder, padded = inp\n",
    "        pooled_output = pool_encoded_sequence(output_encoder, padded)\n",
    "        y = self.layers(pooled_output)\n",
    "        \n",
    "        return y        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aW7z7yI-kfpu",
    "outputId": "62562bb1-aa62-444f-e180-8d93ff377aaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(label_dict)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZmt9xEekfpr"
   },
   "source": [
    "### Combine everything in the protein classifier\n",
    "\n",
    "Combine every class and part in the protein classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "id": "KPF-qx-lkfpr"
   },
   "outputs": [],
   "source": [
    "class proteinClassifier(torch.nn.Module):\n",
    "    r\"\"\"The complete protein classifier\"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, seq_len, num_classes, batch_size, pretrained_file=False):\n",
    "        super(proteinClassifier, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_sz = hid_sz\n",
    "        self.hidden_p = hidden_p\n",
    "        self.embed_p = embed_p\n",
    "        self.input_p = input_p\n",
    "        self.seq_len = seq_len\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        pad_token = vocab_sz + 1\n",
    "        \n",
    "        language_model = AWD_LSTM(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, \n",
    "                                embed_p, input_p, weight_p, batch_sz=batch_size)\n",
    "        \n",
    "        if pretrained_file:\n",
    "            language_mode = torch.load(pretrained_file, map_location=torch.device(dev))\n",
    "        \n",
    "        encoder = SentenceEncoder(seq_len, language_model, pad_token)\n",
    "        \n",
    "        classifier = PoolingLinearClassifier(num_classes, self.batch_size)\n",
    "        \n",
    "        self.layers = nn.Sequential(encoder, classifier)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        y = self.layers(inp)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSUp6mZHkfps"
   },
   "source": [
    "## Model hyperparameters and train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "id": "Rf1MiKM_kfps"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "emb_dim = 400 # Embeddding dimension\n",
    "hid_sz = 1150 # Hidden size\n",
    "num_layers = 3 # Number of LSTM layers stacked together\n",
    "seq_len = 50 # Based on paper mentioned above\n",
    "batch_size = 2 \n",
    "\n",
    "# Dropout parameters\n",
    "\n",
    "embed_p = 0.1 # Dropout probability on the embedding\n",
    "hidden_p = 0.3 # Dropout probability on hidden-to-hidden weight matrices\n",
    "input_p = 0.3 # Dropout probablity on the LSTM input between LSTMS\n",
    "weight_p = 0.5 # Dropout probability on LSTM-to-LSTM weight matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMaUgLbpkfpu"
   },
   "source": [
    "### Load in the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "id": "-RhParSjkfpu"
   },
   "outputs": [],
   "source": [
    "pretrained_model = Path('models/AA_LM_v3_ph2.pt')\n",
    "pretrained_model = file_paths / pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJuNCOxskfpu",
    "outputId": "1a6f8ffc-4452-4d83-f3cc-d21bf831e1c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "proteinClassifier(\n",
       "  (layers): Sequential(\n",
       "    (0): SentenceEncoder(\n",
       "      (model): AWD_LSTM(\n",
       "        (encoder): Embedding(9317, 400)\n",
       "        (emb_drop): EmbeddingDropout()\n",
       "        (input_dp): Dropout(p=0.3, inplace=False)\n",
       "        (hid_dp): Dropout(p=0.3, inplace=False)\n",
       "        (lstms): ModuleList(\n",
       "          (0): WeightDropout(\n",
       "            (module): LSTM(400, 1150)\n",
       "          )\n",
       "          (1): WeightDropout(\n",
       "            (module): LSTM(400, 1150)\n",
       "          )\n",
       "          (2): WeightDropout(\n",
       "            (module): LSTM(400, 1150)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): PoolingLinearClassifier(\n",
       "      (layers): Sequential(\n",
       "        (0): BatchNorm1d(3450, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Dropout(p=0.2, inplace=False)\n",
       "        (2): Linear(in_features=3450, out_features=50, bias=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "        (6): Linear(in_features=50, out_features=11, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = proteinClassifier(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p,\n",
    "                         embed_p, input_p, weight_p, seq_len, num_classes, batch_size, pretrained_model)\n",
    "model.to(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYwstDEKkfpv"
   },
   "source": [
    "## Learning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "id": "FmSpwoR7kfpv"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "adam_betas = (0.7, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "id": "Ajfvm-krkfpv"
   },
   "outputs": [],
   "source": [
    "# Costfunction and optimize algorithm\n",
    "#criterion = torch.nn.CrossEntropyLoss(weight=torch.FloatTensor(weight_labels))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=adam_betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXkpsc7hkfpv"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data has different length, therefore I use the pack_sequence in the collate fn.\n",
    "https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deze functie voegt padding toe zodat ik een batch kan runnen want een batch moet gelijke grote hebben.\n",
    "# En sorteert ook op grote\n",
    "\n",
    "def collate_fn_padd(batch):\n",
    "    \n",
    "    padding_value = vocab_sz + 1\n",
    "\n",
    "    seqs = []\n",
    "    ys = []\n",
    "    \n",
    "    for seq in batch:\n",
    "        seqs.append(seq[0])\n",
    "        ys.append(seq[1])\n",
    "\n",
    "    # Add padding to the sequences\n",
    "    padded = pad_sequence(seqs, batch_first=True, padding_value=padding_value)\n",
    "    \n",
    "    ys = torch.stack(ys, dim=0)\n",
    "    \n",
    "    \n",
    "    # Calculate the real size so that the padded sequences can be sorted\n",
    "    masked = padded == padding_value    \n",
    "    real_seq_size = padded.size(1) - masked.sum(dim=1)\n",
    "    _, indices = torch.sort(real_seq_size, descending=True)\n",
    "    \n",
    "    sorted_masked = torch.zeros_like(masked)\n",
    "    sorted_padded_sequences = torch.zeros_like(padded)\n",
    "    \n",
    "    for i, ind in enumerate(indices, 0):\n",
    "        sorted_padded_sequences[i] = padded[ind]\n",
    "        sorted_masked[i] = masked[ind]\n",
    "        \n",
    "    # It is nicer to add padding\n",
    "    max_iterations = int(sorted_padded_sequences.size(1) / 50)\n",
    "    \n",
    "    \"\"\" \n",
    "    # Testen hoe ik uit kan vogelen hoe die batch sizes kunnen worden aangepast\n",
    "    for i in range(0, max_iterations * 50, 50):\n",
    "        print('Possible input')\n",
    "        inp = sorted_padded_sequences[:, i:i + 50]\n",
    "        total_mask = sorted_masked[:, i:i+50].sum(dim=1) == 50\n",
    "        true_bs = (total_mask == False).sum()\n",
    "        print(total_mask)\n",
    "        print(true_bs)\n",
    "    \"\"\" \n",
    "    \n",
    "    return sorted_padded_sequences, ys   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "id": "uDa2Eq40kfpv"
   },
   "outputs": [],
   "source": [
    "# Load the data in the DataSet\n",
    "AADataset = AminoClassifierDataset(df, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "id": "GHShLf7Lkfpw"
   },
   "outputs": [],
   "source": [
    "# Split the data in an 80/20% split for training and testing\n",
    "data_len = len(AADataset)\n",
    "train_part = int(0.8 * data_len)\n",
    "test_part = data_len - train_part\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(AADataset, [train_part, test_part])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbs-M47qkfpw"
   },
   "outputs": [],
   "source": [
    "# Load the data into data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_padd)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_padd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "id": "w28cdQdpkfpr"
   },
   "outputs": [],
   "source": [
    "def pool_encoded_sequence(output, masked):\n",
    "    r\"\"\"Pool the encoded AA sequence and \n",
    "    return one vector with the max_pool and avg_pool concatenated\"\"\"\n",
    "    \n",
    "    \n",
    "    #print('Input pooling:')\n",
    "    \n",
    "    #print(output)\n",
    "    \n",
    "    #global masked_input\n",
    "    #global masked_output\n",
    "    \n",
    "    #masked_input = output[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    hidden_states = output[0]\n",
    "    #cell_states = output[1]\n",
    "    \n",
    "    mask = masked == 0\n",
    "    \n",
    "    sl, bs = hidden_states.shape[:2]\n",
    "    \n",
    "    \n",
    "    lens = hidden_states.shape[0] - mask.long().sum(dim = 0)\n",
    "    last_lens = mask[-sl:,:].long().sum(dim = 0)\n",
    "\n",
    "    last_hidden_state = hidden_states[-last_lens, torch.arange(0, hidden_states.size(1))]\n",
    "    #last_cell_state = cell_states[-1, :, :]\n",
    "    \n",
    "    hidden_state_avg = hidden_states.masked_fill_(mask[:, :, None], 0).sum(dim = 0)\n",
    "    hidden_state_avg.div_(lens[:, None])\n",
    "    #cell_state_avg = cell_states.sum(dim=0) / sl\n",
    "    \n",
    "\n",
    "    hidden_state_max = hidden_states.masked_fill_(mask[:, :, None], -float('inf')).max(dim = 0)[0]\n",
    "    #cell_state_max = hidden_states.max(dim=0)[0]\n",
    "\n",
    "    #x = torch.cat([last_hidden_state, last_cell_state, hidden_state_avg, cell_state_avg, \\\n",
    "    #              hidden_state_max, cell_state_max], 0)\n",
    "\n",
    "    x = torch.cat([last_hidden_state, hidden_state_avg, hidden_state_max], 1)\n",
    "    \n",
    "    x = x.view(bs, -1)\n",
    "    \n",
    "    #print('Output pooling:')\n",
    "    #print(x)\n",
    "    #masked_output = x\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-396-c48f60de694b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#print(ys.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Flatten the label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-381-95167b182617>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-378-02397839009b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mcorr_bs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_padded\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcorr_bs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-376-9d3f10af1b99>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# Go trough one LSTM layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# Detach hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-6ae087298e53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setweights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-6ae087298e53>\u001b[0m in \u001b[0;36m_setweights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mraw_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{weight}_raw'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test for the real work\n",
    "for i, entry in enumerate(train_loader, 0):\n",
    "    xs, ys = entry[0], entry[1]\n",
    "    \n",
    "    #print(xs.shape)\n",
    "    #print(ys.shape)\n",
    "    \n",
    "    outputs = model(xs)\n",
    "    \n",
    "    # Flatten the label\n",
    "    ys = ys.view(-1)\n",
    "\n",
    "    #print(outputs.shape)\n",
    "    #print(outputs)\n",
    "   # print(ys.shape)\n",
    "    \n",
    "    #print(ys)\n",
    "    \n",
    "    loss = criterion(outputs, ys)\n",
    "    #print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor([[[0.7424, 0.1422, 0.2362, 0.2392, 0.8468],\n",
      "         [0.8157, 0.2329, 0.8180, 0.6716, 0.0414]],\n",
      "\n",
      "        [[0.4204, 0.8071, 0.5335, 0.5156, 0.1111],\n",
      "         [0.7769, 0.0149, 0.7695, 0.5507, 0.1134]]])\n",
      "tensor([[ True,  True],\n",
      "        [ True, False]])\n",
      "tensor([2, 1])\n",
      "torch.Size([2, 5])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7769, 0.0149, 0.7695, 0.5507, 0.1134]])\n",
      "tensor([0, 1])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7769, 0.0149, 0.7695, 0.5507, 0.1134]])\n",
      "tensor([[  -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.7769, 0.0149, 0.7695, 0.5507, 0.1134]])\n",
      "torch.Size([2, 5])\n",
      "torch.Size([2, 5])\n",
      "torch.Size([2, 15])\n"
     ]
    }
   ],
   "source": [
    "# Pooling testing (see fastai docs)\n",
    "test = torch.rand(2, 2, 5)\n",
    "full_length = torch.ones(2, 1)\n",
    "half_length = torch.ones(2, 1)\n",
    "half_length[1:,:] = 0\n",
    "print(full_length.sum())\n",
    "print(half_length.sum())\n",
    "\n",
    "\n",
    "print(test)\n",
    "\n",
    "mask = torch.cat([full_length, half_length], dim = 1) == 1\n",
    "\n",
    "print(mask)\n",
    "\n",
    "last_lens = mask[:, -5:].long().sum(dim = 0)\n",
    "\n",
    "print(last_lens)\n",
    "\n",
    "test_out = test[torch.arange(0, test.size(0)),-last_lens]\n",
    "print(test_out.shape)\n",
    "\n",
    "avg_pool = test.masked_fill(mask[:, :, None], 0).sum(dim = 0)\n",
    "print(avg_pool)\n",
    "\n",
    "lens = test.shape[1] - mask.long().sum(dim = 1)\n",
    "print(lens)\n",
    "\n",
    "avg_pool.div(avg_pool / lens[:, None])\n",
    "\n",
    "print(avg_pool)\n",
    "\n",
    "max_pool = test.masked_fill(mask[:, :, None], -float('inf')).max(dim = 0)[0]\n",
    "print(max_pool)\n",
    "\n",
    "print(avg_pool.shape)\n",
    "print(max_pool.shape)\n",
    "\n",
    "x = torch.cat([test_out, avg_pool, max_pool], 1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKsYU03Nkfpw",
    "outputId": "a6006639-7874-4394-ccc1-b58cd88af89d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 2, 1150), got [1, 1, 1150]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-10682932e135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-660aa33b8331>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epochs, model, train_loader, test_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;31m# Flatten the label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-95167b182617>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-d312a746c02c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mcorr_bs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_padded\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcorr_bs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-438163ead6bd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# Go trough one LSTM layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Detach hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-6ae087298e53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setweights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[0m\u001b[1;32m    607\u001b[0m                                'Expected hidden[0] size {}, got {}')\n\u001b[1;32m    608\u001b[0m         self.check_hidden_size(hidden[1], self.get_expected_cell_size(input, batch_sizes),\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    221\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 2, 1150), got [1, 1, 1150]"
     ]
    }
   ],
   "source": [
    "trained_model, loss_history, val_loss_history = train_model(epochs, model, train_loader, test_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "keinNjOmkfpx",
    "outputId": "76b14142-0506-4857-80dc-1227d8c617fd"
   },
   "outputs": [],
   "source": [
    "show_losses(loss_history, val_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99DY1ETOkfpx"
   },
   "outputs": [],
   "source": [
    "save_model_file = '/home/mees/Desktop/Machine_Learning/subcellular_location/models/trained_protein_classifier_part2.pt'\n",
    "torch.save(model.state_dict, save_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ik ga de outputs controleren van de verschillende layers of die hetzelfde zijn als Fastai outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(test_loader, 0):\n",
    "    # get the inputs; data is a one input (batch size), and y\n",
    "\n",
    "    x, y = data\n",
    "    x = x.squeeze(0) # Squeeze x in the correct shape\n",
    "    y = y.squeeze(0) # Squeeze y in the correct shape\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    output = model(x)   \n",
    "    output = output.unsqueeze(0) # For the correct shape\n",
    "    loss = criterion(output, y)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLlLv3BVkfpx",
    "outputId": "a9d34cdd-4d40-4753-afb2-3b8f7c60df15"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-59b6bd33a9dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Show the accuracy on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-128-0dd93e7cab89>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Squeeze y in the correct shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-8b020a65e3ff>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-eedab9e09650>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-a3ac69f1cf6a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mhiddens_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiddens_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-6ae087298e53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setweights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-6ae087298e53>\u001b[0m in \u001b[0;36m_setweights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mraw_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{weight}_raw'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    981\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[1;32m    982\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Show the accuracy on test data\n",
    "accuracy(trained_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THbPwNEKkfpy",
    "outputId": "6929400f-45b9-4ada-f680-e9f7ceb5a985"
   },
   "outputs": [],
   "source": [
    "# Show the accuracy on train data\n",
    "accuracy(trained_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FbCIF6Vekfpy",
    "outputId": "7771899f-5e6e-460b-bd3a-8c14098aae56"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    all_predicted = []\n",
    "    ys = []\n",
    "    \n",
    "    for ind, data in enumerate(test_loader, 0):\n",
    "        x, y = data\n",
    "        x = x.squeeze(0) # Squeeze x in the correct shape\n",
    "        y = y.squeeze(0) # Squeeze y in the correct shape\n",
    "\n",
    "        output = model(x)\n",
    "        output = output.unsqueeze(0)\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        all_predicted.append(predicted.item())\n",
    "        ys.append(y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dyS-kzfdkfpy",
    "outputId": "b3a6ad8d-b851-43d8-ea25-d4e925779f5f"
   },
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(ys, all_predicted)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix, display_labels=list(label_dict.values()))\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "disp.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from this confusion matrix everything is mapped to class 4, this class is over represented ~5000 out of 16000 examples. So the cost is lowest to predict that one. Therefore, we have to use a weighted random sampler. And probably also batches to train faster. So first, implement training in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssrnJCMGkfpz",
    "outputId": "da80c4cc-f824-47f4-da19-7394882254d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Membrane': 0, 'Endosome': 1, 'Cytoplasm': 2, 'Mitochondrion': 3, 'pass membrane protein': 4, 'Single': 5, 'Nucleus': 6, 'Cell membrane': 7, 'Secreted': 8, 'Golgi apparatus': 9, 'Cell junction': 10, 'Peroxisome': 11, 'Cell projection': 12, 'Peripheral membrane protein': 13, 'Cytoplasmic vesicle': 14, 'Lysosome': 15, 'Chromosome': 16, 'anchor': 17, 'Endoplasmic reticulum membrane': 18, 'Cytoplasmic granule': 19, 'Cell surface': 20, 'Endoplasmic reticulum': 21, 'Endoplasmic reticulum lumen': 22, 'Melanosome': 23, 'Virion': 24, 'Vesicle': 25, 'Isoform 1': 26, 'Peroxisome matrix': 27, 'Sarcoplasmic reticulum': 28, 'Endomembrane system': 29, 'Membrane raft': 30, 'Midbody': 31, 'Lipid droplet': 32}\n"
     ]
    }
   ],
   "source": [
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testen met Fastai als referentie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.0000, -0.0000,  0.2283, -0.0674,  0.0000,  0.0000],\n",
       "        [-0.8458,  6.0831,  0.8827, -1.7225,  1.2066,  1.5383, -0.6647],\n",
       "        [-0.4909, -1.9670,  2.3101, -0.0000, -2.5325,  4.1651, -0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.2306, -0.0000,  1.5061,  0.0000,  0.0000,  0.0360, -0.0000],\n",
       "        [-0.0000,  0.0000,  0.0902,  1.2305,  1.1262,  0.0000, -0.0000],\n",
       "        [-0.0000, -2.6635,  1.5061,  1.3771,  0.0000,  0.0000, -0.0000],\n",
       "        [-0.0000, -0.0000,  2.3669, -0.7374,  1.4977, -1.2494,  0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding dropout\n",
    "encoder = torch.nn.Embedding(10, 7, padding_idx=1)\n",
    "emb_drop = torch.nn.Dropout(p=0.5)\n",
    "tst_inp = torch.randint(0,10,(8,))\n",
    "tst_out = emb_drop(encoder(tst_inp))\n",
    "tst_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(torch.nn.Module):\n",
    "    \"Apply dropout to an Embedding with probability emp_p\"\n",
    "\n",
    "    def __init__(self, emb_p=0):\n",
    "        super(EmbeddingDropout, self).__init__()\n",
    "        \n",
    "        self.emb_p = emb_p\n",
    "\n",
    "    def forward(self, inp):\n",
    "       \n",
    "        drop = torch.nn.Dropout(self.emb_p)\n",
    "        placeholder = torch.ones((inp.size(0), 1))\n",
    "        mask = drop(placeholder)      \n",
    "        out = inp * mask\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "tensor([5])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 7])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d454925e4c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtst_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtst_inp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "tst_inp = torch.randint(0,10,(1,))\n",
    "print(tst_inp.shape)\n",
    "print(tst_inp)\n",
    "encoder = torch.nn.Embedding(10, 7, padding_idx=1)\n",
    "encoded = encoder(tst_inp)\n",
    "print(encoded.shape)\n",
    "emb_drop = EmbeddingDropout(emb_p=0.5)\n",
    "tst_out = emb_drop(encoded)\n",
    "print(tst_out.shape)\n",
    "for i in range(8):\n",
    "    assert (tst_out[i]==0).all() or torch.allclose(tst_out[i], 2*encoder.weight[tst_inp[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8539,  2.2006, -0.0222,  0.4224,  1.2578, -0.5769, -0.7109],\n",
      "        [ 1.2339,  0.9903, -1.4282,  0.4318, -1.2289,  0.8093, -0.5293],\n",
      "        [ 2.0111, -0.3286,  0.6406,  1.2247,  0.5189,  0.4388, -0.2033],\n",
      "        [-0.7689, -1.3048,  0.5878,  0.1067,  0.1493, -0.9369,  0.1112],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-1.0289,  0.4262, -1.3303, -0.7120,  1.0064,  0.7244, -0.7196],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "torch.Size([8, 7])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.]])\n",
      "torch.Size([8, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
       "        [ 4.0222, -0.6571,  1.2812,  2.4494,  1.0377,  0.8776, -0.4066],\n",
       "        [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-2.0579,  0.8524, -2.6606, -1.4240,  2.0127,  1.4488, -1.4393],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_inp = torch.randint(0,10,(8,))\n",
    "drop = torch.nn.Dropout(0.5)\n",
    "encoder = torch.nn.Embedding(10, 7, padding_idx=1)\n",
    "encoded = encoder(tst_inp)\n",
    "print(encoded)\n",
    "print(encoded.shape)\n",
    "\n",
    "placeholder = torch.ones((tst_inp.size(0), 1))\n",
    "mask = drop(placeholder)\n",
    "print(mask)\n",
    "print(mask.shape)\n",
    "\n",
    "encoded * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 1, 6, 9, 3, 7, 1])\n",
      "torch.Size([8])\n",
      "tensor([[-0.9948, -1.3262, -1.4499,  0.1425,  0.3206, -0.5852,  0.4061],\n",
      "        [-1.8594,  0.1413,  1.6815,  0.9142, -0.5407, -0.8299,  0.1996],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.5156,  0.3203,  1.5523,  1.3523,  1.1633, -0.5602, -0.1511],\n",
      "        [ 0.9568, -0.6867,  1.0243,  1.7308, -0.3247,  0.3144, -0.0669],\n",
      "        [ 1.0880, -1.1082,  2.1468, -0.7218,  1.9135, -0.3273,  0.4820],\n",
      "        [-0.5337,  0.5620, -2.7735,  1.1568, -0.2488,  0.5844, -2.2863],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9948, -1.3262, -1.4499,  0.1425,  0.3206, -0.5852,  0.4061],\n",
       "        [-1.8594,  0.1413,  1.6815,  0.9142, -0.5407, -0.8299,  0.1996],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.5156,  0.3203,  1.5523,  1.3523,  1.1633, -0.5602, -0.1511],\n",
       "        [ 0.9568, -0.6867,  1.0243,  1.7308, -0.3247,  0.3144, -0.0669],\n",
       "        [ 1.0880, -1.1082,  2.1468, -0.7218,  1.9135, -0.3273,  0.4820],\n",
       "        [-0.5337,  0.5620, -2.7735,  1.1568, -0.2488,  0.5844, -2.2863],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding dropout\n",
    "encoder = torch.nn.Embedding(10, 7, padding_idx=1)\n",
    "emb_drop = EmbeddingDropout(emb_p=0.5)\n",
    "tst_inp = torch.randint(0,10,(8,))\n",
    "print(tst_inp)\n",
    "print(tst_inp.shape)\n",
    "encoded = encoder(tst_inp)\n",
    "print(encoded)\n",
    "tst_out = emb_drop(encoded)\n",
    "tst_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-197-d05f4a6a9746>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-197-d05f4a6a9746>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    (tst_out==).sum()\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "(tst_out==).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tst_out==1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 20, got 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-9c2712273055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-d575e1e6e46e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# Go trough one LSTM layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# Detach hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-6ae087298e53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setweights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    178\u001b[0m             raise RuntimeError(\n\u001b[1;32m    179\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m--> 180\u001b[0;31m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 20, got 100"
     ]
    }
   ],
   "source": [
    "tst = AWD_LSTM(2, 100, 20, 10, 0.2, 0.02, 0.1, 0.2)\n",
    "x = torch.randint(0, 100, (1,5))\n",
    "bs,sl = x.shape[:2]\n",
    "print(bs)\n",
    "print(sl)\n",
    "r = tst(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1154])\n",
      "torch.Size([8, 1])\n",
      "Input pooling:\n",
      "(tensor([[[ 0.0109, -0.0156, -0.1034,  ..., -0.3028, -0.0630,  0.0039],\n",
      "         [-0.0007,  0.0800, -0.0562,  ..., -0.0127,  0.0086, -0.1029],\n",
      "         [-0.1946,  0.1257,  0.0612,  ..., -0.3574, -0.1059,  0.1546],\n",
      "         ...,\n",
      "         [-0.1482, -0.0831, -0.0399,  ..., -0.2573, -0.2074,  0.0869],\n",
      "         [-0.0902, -0.2804, -0.1619,  ..., -0.1385, -0.0978,  0.2239],\n",
      "         [-0.0192,  0.1015,  0.0245,  ..., -0.1114, -0.0303,  0.1074]],\n",
      "\n",
      "        [[-0.1561, -0.1073, -0.0386,  ..., -0.1562,  0.0135,  0.0362],\n",
      "         [ 0.0459, -0.0251,  0.0359,  ..., -0.1482,  0.0461, -0.2484],\n",
      "         [-0.2142, -0.0040,  0.0209,  ..., -0.3017, -0.1606, -0.0058],\n",
      "         ...,\n",
      "         [ 0.0006, -0.1642,  0.0095,  ..., -0.0509, -0.0118, -0.1041],\n",
      "         [-0.0852, -0.1714, -0.0269,  ...,  0.0247,  0.1178,  0.2502],\n",
      "         [-0.0392, -0.0319, -0.0186,  ..., -0.0938,  0.0887,  0.0617]],\n",
      "\n",
      "        [[ 0.0406, -0.0638, -0.0874,  ...,  0.0450, -0.0027,  0.1466],\n",
      "         [-0.1039,  0.0448, -0.0068,  ..., -0.2545, -0.0166, -0.1091],\n",
      "         [-0.2801, -0.1042, -0.1414,  ...,  0.0537,  0.0650, -0.1037],\n",
      "         ...,\n",
      "         [ 0.0685, -0.2285,  0.1106,  ..., -0.0449, -0.0288, -0.0616],\n",
      "         [ 0.1203, -0.1806, -0.1277,  ...,  0.0269,  0.1136,  0.4545],\n",
      "         [ 0.0310,  0.0244, -0.1569,  ..., -0.0765,  0.1006,  0.0302]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1273,  0.0506, -0.2706,  ..., -0.1768, -0.1032,  0.0333],\n",
      "         [ 0.1609,  0.0380,  0.0376,  ..., -0.0572, -0.1551,  0.0825],\n",
      "         [-0.0850, -0.0971, -0.0036,  ..., -0.2602, -0.0784, -0.0361],\n",
      "         ...,\n",
      "         [-0.1194, -0.0319, -0.1161,  ..., -0.0584, -0.1569, -0.1197],\n",
      "         [ 0.0341, -0.0936,  0.0262,  ..., -0.0818, -0.0788,  0.0075],\n",
      "         [-0.1028, -0.1219, -0.0809,  ...,  0.0920, -0.1039, -0.0929]],\n",
      "\n",
      "        [[-0.0334, -0.0512, -0.1560,  ..., -0.1722, -0.0782,  0.0474],\n",
      "         [-0.0912, -0.0577, -0.0456,  ..., -0.1417, -0.0207,  0.2727],\n",
      "         [-0.0560, -0.0092, -0.0052,  ..., -0.1805, -0.0944, -0.0511],\n",
      "         ...,\n",
      "         [ 0.0562, -0.0577, -0.1898,  ..., -0.1436, -0.0340, -0.0247],\n",
      "         [-0.0906, -0.1809, -0.0641,  ..., -0.0798,  0.0125,  0.0079],\n",
      "         [-0.1601, -0.0915, -0.1354,  ..., -0.0427, -0.0591,  0.0820]],\n",
      "\n",
      "        [[ 0.0214,  0.0066,  0.1569,  ..., -0.2040, -0.2041,  0.0528],\n",
      "         [-0.0474, -0.0526, -0.0711,  ..., -0.0933,  0.0597, -0.1431],\n",
      "         [-0.0862,  0.1314,  0.0855,  ..., -0.1590, -0.0077,  0.0642],\n",
      "         ...,\n",
      "         [-0.0108,  0.1254, -0.0168,  ..., -0.1295, -0.0276,  0.1142],\n",
      "         [-0.0305, -0.1708, -0.0032,  ..., -0.2405,  0.0805,  0.0738],\n",
      "         [-0.0726, -0.0443,  0.0742,  ..., -0.0966, -0.0728,  0.0321]]]), tensor([[[ 0.0254, -0.0421, -0.2282,  ..., -0.5728, -0.1008,  0.0084],\n",
      "         [-0.0015,  0.1308, -0.1533,  ..., -0.0254,  0.0144, -0.3956],\n",
      "         [-0.5689,  0.2758,  0.1426,  ..., -0.6084, -0.1664,  0.3571],\n",
      "         ...,\n",
      "         [-0.4274, -0.2119, -0.0837,  ..., -0.4268, -0.3909,  0.1791],\n",
      "         [-0.2105, -0.6578, -0.3054,  ..., -0.2410, -0.1572,  0.6257],\n",
      "         [-0.0488,  0.2576,  0.0412,  ..., -0.1834, -0.0561,  0.2423]],\n",
      "\n",
      "        [[-0.2113, -0.1818, -0.0836,  ..., -0.3555,  0.0297,  0.1372],\n",
      "         [ 0.0997, -0.0587,  0.0672,  ..., -0.2593,  0.0995, -0.5323],\n",
      "         [-0.4919, -0.0080,  0.0394,  ..., -0.6491, -0.3116, -0.0121],\n",
      "         ...,\n",
      "         [ 0.0011, -0.3356,  0.0212,  ..., -0.0774, -0.0183, -0.2073],\n",
      "         [-0.1657, -0.4687, -0.0549,  ...,  0.0421,  0.2187,  0.6818],\n",
      "         [-0.0597, -0.0562, -0.0327,  ..., -0.2247,  0.1343,  0.1592]],\n",
      "\n",
      "        [[ 0.1102, -0.1162, -0.1277,  ...,  0.0724, -0.0070,  0.3272],\n",
      "         [-0.2428,  0.0901, -0.0117,  ..., -0.5924, -0.0404, -0.2332],\n",
      "         [-0.6868, -0.2808, -0.2331,  ...,  0.0868,  0.1269, -0.2107],\n",
      "         ...,\n",
      "         [ 0.1166, -0.3584,  0.2094,  ..., -0.1172, -0.0675, -0.1525],\n",
      "         [ 0.2956, -0.3419, -0.2469,  ...,  0.0574,  0.1893,  0.9094],\n",
      "         [ 0.0946,  0.0503, -0.3826,  ..., -0.1462,  0.5325,  0.0480]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2889,  0.1074, -0.6135,  ..., -0.5274, -0.1794,  0.0934],\n",
      "         [ 0.3020,  0.0755,  0.0715,  ..., -0.0984, -0.2258,  0.1196],\n",
      "         [-0.2412, -0.1967, -0.0072,  ..., -0.5045, -0.1810, -0.0829],\n",
      "         ...,\n",
      "         [-0.2640, -0.0678, -0.3160,  ..., -0.1112, -0.2861, -0.2669],\n",
      "         [ 0.0682, -0.1935,  0.0500,  ..., -0.1550, -0.1527,  0.0159],\n",
      "         [-0.1953, -0.2416, -0.2365,  ...,  0.1908, -0.2034, -0.2223]],\n",
      "\n",
      "        [[-0.0674, -0.0938, -0.3517,  ..., -0.2864, -0.2956,  0.1107],\n",
      "         [-0.2026, -0.1054, -0.0680,  ..., -0.2502, -0.0359,  0.3834],\n",
      "         [-0.1013, -0.0181, -0.0099,  ..., -0.4118, -0.1855, -0.0967],\n",
      "         ...,\n",
      "         [ 0.0883, -0.0927, -0.4636,  ..., -0.2519, -0.1387, -0.0720],\n",
      "         [-0.1840, -0.2957, -0.1439,  ..., -0.1436,  0.0492,  0.0158],\n",
      "         [-0.3267, -0.1416, -0.2357,  ..., -0.0635, -0.2229,  0.2392]],\n",
      "\n",
      "        [[ 0.0533,  0.0166,  0.3109,  ..., -0.4199, -0.4568,  0.1006],\n",
      "         [-0.0901, -0.0751, -0.1424,  ..., -0.1950,  0.1220, -0.3977],\n",
      "         [-0.2520,  0.2658,  0.1843,  ..., -0.2798, -0.0127,  0.1220],\n",
      "         ...,\n",
      "         [-0.0397,  0.2612, -0.0336,  ..., -0.1936, -0.0472,  0.2971],\n",
      "         [-0.0666, -0.3978, -0.0085,  ..., -0.3602,  0.1611,  0.1434],\n",
      "         [-0.2523, -0.1042,  0.1601,  ..., -0.1535, -0.1574,  0.0784]]]))\n",
      "Output pooling:\n",
      "tensor([[ 0.0214,  0.0066,  0.1569,  ..., -0.1590, -0.0077,  0.0642],\n",
      "        [-0.1815, -0.0634, -0.1178,  ..., -0.1295, -0.0276,  0.1142],\n",
      "        [-0.0305, -0.1708, -0.0032,  ..., -0.1198, -0.0491,  0.0511],\n",
      "        ...,\n",
      "        [-0.0940, -0.0654, -0.0361,  ...,  0.3639,  0.3294,  0.2727],\n",
      "        [ 0.3768,  0.2668,  0.2348,  ...,  0.1992,  0.1603,  0.3107],\n",
      "        [ 0.1818,  0.1484,  0.2544,  ...,  0.3616,  0.2598,  0.2571]])\n",
      "torch.Size([8, 11])\n",
      "torch.Size([8])\n",
      "tensor([0, 8, 1, 2, 7, 1, 5, 8])\n",
      "tensor(2.5700, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Test for the real work\n",
    "for i, entry in enumerate(train_loader, 0):\n",
    "    xs, ys = entry[0], entry[1]\n",
    "\n",
    "    \n",
    "    outputs = model(xs)\n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_concat_pool(output, mask, bptt):\n",
    "    \"Pool `MultiBatchEncoder` outputs into one vector [last_hidden, max_pool, avg_pool]\"\n",
    "    last_hidden_state = output[:8, :, :]\n",
    "    print(last_hidden_state.shape)\n",
    "    lens = output.shape[1] - mask.long().sum(dim=1)\n",
    "    last_lens = mask[:,-bptt:].long().sum(dim=1)\n",
    "    avg_pool = output.masked_fill(mask[:, :, None], 0).sum(dim=1)\n",
    "    avg_pool.div_(lens.type(avg_pool.dtype)[:,None])\n",
    "    print(avg_pool.shape)\n",
    "    max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "    print(max_pool.shape)\n",
    "    x = torch.cat([last_hidden_state, max_pool, avg_pool], 1) #Concat pooling.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = masked_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = inp == 1\n",
    "mask = mask.view(8, 60, 1150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 8, 1150])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 60, 1150])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8, 1150])\n",
      "torch.Size([8, 8, 1150])\n",
      "torch.Size([8, 8, 1150])\n"
     ]
    }
   ],
   "source": [
    "fastai_mask = masked_concat_pool(masked_input, mask, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3450])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 24, 1150])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastai_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingLinearClassifier(torch.nn.Module):\n",
    "    r\"\"\"Pool the outputs from the encoder and classify it.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, batch_sz):\n",
    "        super(PoolingLinearClassifier, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.batch_sz = batch_sz\n",
    "        \n",
    "        if batch_sz > 1:\n",
    "            \n",
    "            self.layers = nn.Sequential(\n",
    "                nn.BatchNorm1d(1150 * 3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Dropout(p=0.2, inplace=False),\n",
    "                nn.Linear(in_features=1150 * 3, out_features=50, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Dropout(p=0.1, inplace=False),\n",
    "                nn.Linear(in_features=50, out_features=num_classes, bias=True)\n",
    "            )\n",
    "        else:\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Dropout(p=0.2, inplace=False),\n",
    "                nn.Linear(in_features=1150 * 3, out_features=50, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=0.1, inplace=False),\n",
    "                nn.Linear(in_features=50, out_features=num_classes, bias=True)\n",
    "            )\n",
    "            \n",
    "        \n",
    "    \n",
    "    def forward(self, inp):\n",
    "        y = self.layers(inp)\n",
    "        \n",
    "        return y        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = PoolingLinearClassifier(11, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0982, -0.4168, -0.8017,  0.1025, -0.3626, -0.3975,  0.0582, -0.2017,\n",
       "         -0.9875, -0.8402,  0.2354],\n",
       "        [-0.3887,  0.4075, -0.3249, -0.0906, -0.6913,  0.3584, -0.9185, -0.3244,\n",
       "         -1.0759,  0.0927, -0.7938],\n",
       "        [-0.1573,  0.4150, -0.3590,  0.2015, -0.7950, -0.2468, -0.3407,  0.6312,\n",
       "         -0.6507,  0.4991, -0.0674],\n",
       "        [-0.3795, -0.0206,  0.3589, -0.2763, -0.3663,  0.3285, -1.4653,  0.7496,\n",
       "          0.4183,  0.0714, -0.8963],\n",
       "        [-0.2388,  0.3797,  0.2177,  0.2608, -0.4071,  0.6272, -0.4977,  0.4818,\n",
       "          0.5715, -0.3453,  0.1413],\n",
       "        [ 0.2414, -0.3238,  0.0107,  0.5495,  1.0083, -0.9280,  0.8907,  0.4594,\n",
       "         -0.4298,  0.1371,  0.5316],\n",
       "        [ 0.4511, -0.6765,  0.8761,  0.2007,  0.8004,  0.4020,  0.9858, -0.7390,\n",
       "          0.6992,  0.5957, -0.5275],\n",
       "        [-0.1340, -0.5162, -0.2578, -0.3707,  1.2367, -0.3099,  0.2414, -0.2288,\n",
       "          1.1941, -0.3654,  0.1177]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(masked_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "proteinClassifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
