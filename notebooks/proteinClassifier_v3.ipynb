{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVoWWP2TkfpH"
   },
   "source": [
    "# Protein Classifier using AWD-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "05m_tDaZkfpQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import sklearn.metrics as metrics\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mp69KYdNkfpS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mees/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "56HVWDq5kfpU",
    "outputId": "034e4862-f298-4b3f-e3be-b653ffaac8e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0frf8WW_kfpW",
    "outputId": "e8496ecc-dc07-43da-a355-0b2fd1a85b0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7feaa548b198>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Q-6XVNYTkfpX",
    "outputId": "d567d9d9-1fb1-4dd3-8565-feeb354188de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "table, th, td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<style>\n",
    "table, th, td {\n",
    "  border: 1px solid black;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qq9C8202kfpZ"
   },
   "source": [
    "## Usefull Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "V7-BmN0zkfpb"
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation loss using matplotlib\n",
    "def show_losses(training_loss, validation_loss):\n",
    "    r\"\"\"Plot graphs with on x-axis epochs and y-axis the loss.\n",
    "\n",
    "        This graph contains both the `training_loss` and `validation_loss`\n",
    "        for each epoch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        training_loss : array\n",
    "            Containing the training loss for each epoch.\n",
    "        validation_loss : array\n",
    "            Containing the validation loss for each epoch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Plot of the two graphs.\n",
    "\n",
    "    \"\"\"\n",
    "    np_loss = np.asarray(training_loss)\n",
    "    np_val_loss = np.asarray(validation_loss)\n",
    "    plt.plot(np_loss, label='Train loss')\n",
    "    plt.plot(np_val_loss, label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Cross Entropy Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dX9RNmgTkfpZ"
   },
   "outputs": [],
   "source": [
    "def train_model(epochs, model, train_loader, test_loader, optimizer, criterion):\n",
    "    r\"\"\"Train the resnet18 model.\n",
    "    \n",
    "    Train the resnet18 model for `epochs`. Using the\n",
    "    `optimizer` and the loss function in `criterion`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : int\n",
    "        Number of epochs to train the model.\n",
    "    model : torch Model\n",
    "        Model that is being trained.\n",
    "    train_loader : torch Dataloader\n",
    "        Dataloader containing the train data to train the model.\n",
    "    test_loader : torch Dataloader\n",
    "        Dataloader containing the test data to validate the model.\n",
    "    optimizer : torch Optimizer\n",
    "        Optimizer used to optimize the model.\n",
    "    criterion : torch.nn Lossfunction\n",
    "        Loss function used. This loss function has to be minimilized by the model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch Model\n",
    "        The trained model.\n",
    "    loss_history : array\n",
    "        Training loss for each epoch.\n",
    "    val_loss_history : array\n",
    "        Validation loss for each epoch.\n",
    "    \n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Inititiate loss variables\n",
    "        epoch_loss = 0.0\n",
    "        epoch_val_loss = 0.0\n",
    "\n",
    "        print(f'Epoch: {str(epoch + 1)}')\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Alterate between train and validaton phase\n",
    "        for phase in ['train', 'val']:\n",
    "\n",
    "          if phase == 'train':\n",
    "            model.train(True) # Set model to training mode\n",
    "            data_loader = train_loader\n",
    "          else:\n",
    "            model.train(False) # Set model to evaluate mode\n",
    "            data_loader = test_loader\n",
    "\n",
    "\n",
    "          # Loop over the data in batch sizes.\n",
    "          for i, data in enumerate(data_loader, 0):\n",
    "            # get the inputs; data is a one input (batch size), and y\n",
    "\n",
    "            x, y = data\n",
    "            x = x.squeeze(0) # Squeeze x in the correct shape\n",
    "            y = y.squeeze(0) # Squeeze y in the correct shape\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            output = model(x)   \n",
    "            output = output.unsqueeze(0) # For the correct shape\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            # Add loss to each epoch\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss = output.shape[0] * loss.item()\n",
    "            else:\n",
    "                epoch_val_loss += output.shape[0] * loss.item()\n",
    "\n",
    "        epoch_loss /= len(train_set)\n",
    "        epoch_val_loss /= len(test_set)\n",
    "\n",
    "        loss_history.append(epoch_loss)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "\n",
    "        print(f'Epoch {str(epoch)} MSE Train Loss: {str(epoch_loss)} ; Validation Loss: {str(epoch_val_loss)}.')\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time_minutes = (end_time - start_time) / 60\n",
    "        \n",
    "        print(f'Epoch duration {str(epoch_time_minutes)} minutes.')\n",
    "        \n",
    "    print('Finished Training')\n",
    "    return model, loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dObmbgbvkfpc"
   },
   "outputs": [],
   "source": [
    "# Tokenize the protein sequence (or any sequence) in kmers.\n",
    "def tokenize(df, protein_seqs_column, kmer_sz, premade_vocab=False):\n",
    "    \n",
    "    if not premade_vocab:\n",
    "        kmers = set()\n",
    "        # Loop over protein sequences\n",
    "        for protein_seq in df[protein_seqs_column]:\n",
    "            # Loop over the whole sequence\n",
    "            for i in range(len(protein_seq) - (kmer_sz - 1)):\n",
    "                # Add kmers to the set, thus only unique kmers will remain\n",
    "                kmers.add(protein_seq[i: i + kmer_sz])\n",
    "\n",
    "        # Map kmers for one hot-encoding\n",
    "        kmer_to_id = dict()\n",
    "        id_to_kmer = dict()\n",
    "\n",
    "        for ind, kmer in enumerate(kmers):\n",
    "            kmer_to_id[kmer] = ind\n",
    "            id_to_kmer[ind] = kmer\n",
    "\n",
    "        vocab_sz = len(kmers)\n",
    "\n",
    "        assert vocab_sz == len(kmer_to_id.keys())\n",
    "    \n",
    "    else:\n",
    "        kmer_to_id, id_to_kmer = premade_vocab\n",
    "        vocab_sz = len(kmer_to_id)\n",
    "    \n",
    "    # Tokenize the protein sequence to integers\n",
    "    tokenized = []\n",
    "    for i, protein_seq in enumerate(df[protein_seqs_column], 0):\n",
    "        sequence = []\n",
    "        \n",
    "        # If the kmer can't be found these indexes should be deleted\n",
    "        remove_idxs = []\n",
    "        \n",
    "        for i in  range(len(protein_seq) - (kmer_sz -1)):\n",
    "            # Convert kmer to integer\n",
    "            kmer = protein_seq[i: i + kmer_sz]\n",
    "            \n",
    "            # For some reason, some kmers miss. Thus these sequences have to be removed\n",
    "            try:\n",
    "                sequence.append(kmer_to_id[kmer])\n",
    "            except:\n",
    "                remove_idxs.append(i)\n",
    "            \n",
    "        tokenized.append(sequence)\n",
    "            \n",
    "    df['tokenized_seqs'] = tokenized\n",
    "    \n",
    "    df.drop(remove_idxs, inplace=True)\n",
    "    \n",
    "    return df, vocab_sz, kmer_to_id, id_to_kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "b0Sufgyqkfpc"
   },
   "outputs": [],
   "source": [
    "# Function to show the accuracy of the model.\n",
    "def accuracy(model, data_loader):\n",
    "    r\"\"\"Calculate accuracy of the model.\n",
    "    \n",
    "    Calculates the accuracy of the trained `model`\n",
    "    for the `data_loader` in the input.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch Model\n",
    "        The trained model.\n",
    "    data_loader : torch Dataloader\n",
    "        Torch dataloader containing the samples you want to test the accuracy for.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Prints the accurucy of the `model` for the samples in the `data_loader`\n",
    "    \n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for ind, data in enumerate(data_loader, 0):\n",
    "            x, y = data\n",
    "            x = x.squeeze(0) # Squeeze x in the correct shape\n",
    "            y = y.squeeze(0) # Squeeze y in the correct shape\n",
    "            \n",
    "            output = model(x)\n",
    "            output = output.unsqueeze(0)\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "\n",
    "        accuracy = np.round((correct / total * 100), 2)\n",
    "        print(f'Accuracy of the network is {str(accuracy)}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcW0LHbzkfpe"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "tbRFWsBukfpe",
    "outputId": "2290f72f-7005-49f3-f695-ea688643ffb6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasmic vesicle, sec...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Early endosome {ECO:0000...</td>\n",
       "      <td>Endosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton,...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Mitochondrion {ECO:00003...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "      <td>Cell membrane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence  \\\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...   \n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...   \n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...   \n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...   \n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...   \n",
       "\n",
       "                           Subcellular location [CC]       Location  \n",
       "0  SUBCELLULAR LOCATION: Cytoplasmic vesicle, sec...      Cytoplasm  \n",
       "1  SUBCELLULAR LOCATION: Early endosome {ECO:0000...       Endosome  \n",
       "2  SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton,...      Cytoplasm  \n",
       "3  SUBCELLULAR LOCATION: Mitochondrion {ECO:00003...  Mitochondrion  \n",
       "4  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  Cell membrane  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = Path('/home/mees/Desktop/Machine_Learning/subcellular_location/data/processed/protein_data_2021-04-04.csv')\n",
    "df = pd.read_csv(data_file, sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "sNGEB5i4kfpf",
    "outputId": "4a0f1948-d4c9-4891-df86-74d7100fa941"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Endosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence       Location\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...      Cytoplasm\n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...       Endosome\n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...      Cytoplasm\n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  Mitochondrion\n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  Cell membrane"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['Subcellular location [CC]'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNktYsKWkfpf"
   },
   "source": [
    "### Tokenize the Data\n",
    "\n",
    "The data has to be tokenized according to the Language Model trained before, therefore, we have to load in that dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vJwNg0L9kfpg"
   },
   "outputs": [],
   "source": [
    "# Set-up numpy generator for random numbers\n",
    "random_number_generator = np.random.default_rng(seed=42)\n",
    "KMER_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "j3QzdCOfkfpg"
   },
   "outputs": [],
   "source": [
    "# Load the vocabolary from the Language Model\n",
    "vocab_save_file = '/home/mees/Desktop/Machine_Learning/subcellular_location/data/interim/LM_vocab.pkl'\n",
    "vocab = pickle.load(open(vocab_save_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AtZA6bQ5kfpg"
   },
   "outputs": [],
   "source": [
    "# Tokenize the protein sequence\n",
    "df, vocab_sz, kmer_to_id, id_to_kmer = tokenize(df, 'Sequence', KMER_SIZE, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "v-GIN4-rkfph",
    "outputId": "30a21a4d-c729-406c-9eb4-feecbdf60810"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Location</th>\n",
       "      <th>tokenized_seqs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[3884, 8570, 3840, 6832, 2277, 2221, 1020, 904...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Endosome</td>\n",
       "      <td>[8772, 7207, 1857, 1688, 5461, 3901, 4899, 424...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[1565, 3797, 2513, 516, 1428, 6558, 6568, 7337...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "      <td>[8939, 2538, 9262, 4438, 2547, 302, 60, 3064, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>[8939, 6897, 6013, 1021, 3034, 2863, 8501, 697...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence       Location  \\\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...      Cytoplasm   \n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...       Endosome   \n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...      Cytoplasm   \n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  Mitochondrion   \n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  Cell membrane   \n",
       "\n",
       "                                      tokenized_seqs  \n",
       "0  [3884, 8570, 3840, 6832, 2277, 2221, 1020, 904...  \n",
       "1  [8772, 7207, 1857, 1688, 5461, 3901, 4899, 424...  \n",
       "2  [1565, 3797, 2513, 516, 1428, 6558, 6568, 7337...  \n",
       "3  [8939, 2538, 9262, 4438, 2547, 302, 60, 3064, ...  \n",
       "4  [8939, 6897, 6013, 1021, 3034, 2863, 8501, 697...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZGZZKjMkfph",
    "outputId": "c32523a2-881e-42f9-a52e-50f143d7c48f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16614"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_s83p4ekfpi"
   },
   "source": [
    "### Numericalize the label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6RnA7Ahwkfpi",
    "outputId": "21fbf819-505f-455f-dbd5-49004a725221"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16614"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some fields are NaN, remove these\n",
    "df.dropna(inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqwWz9y7kfpi"
   },
   "source": [
    "Create a dictionary to numericalize the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "rVTVFS09kfpj"
   },
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "\n",
    "for i, label in enumerate(df['Location'].unique(), 0):\n",
    "    label_dict[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "GSU8mia8kfpj"
   },
   "outputs": [],
   "source": [
    "def numericalizeClass(df, class_column, label_dict, label_column='Label'):\n",
    "    df[label_column] = df[class_column].map(label_dict)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "q-gQqK4Zkfpk",
    "outputId": "888c57aa-9f2f-4a3c-c3ea-65d897189647"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Location</th>\n",
       "      <th>tokenized_seqs</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[3884, 8570, 3840, 6832, 2277, 2221, 1020, 904...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...</td>\n",
       "      <td>Endosome</td>\n",
       "      <td>[8772, 7207, 1857, 1688, 5461, 3901, 4899, 424...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>[1565, 3797, 2513, 516, 1428, 6558, 6568, 7337...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...</td>\n",
       "      <td>Mitochondrion</td>\n",
       "      <td>[8939, 2538, 9262, 4438, 2547, 302, 60, 3064, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>[8939, 6897, 6013, 1021, 3034, 2863, 8501, 697...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence       Location  \\\n",
       "0  MTDTVFSNSSNRWMYPSDRPLQSNDKEQLQAGWSVHPGGQPDRQRK...      Cytoplasm   \n",
       "1  MDTDSQRSHLSSFTMKLMDKFHSPKIKRTPSKKGKPAEVSVKIPEK...       Endosome   \n",
       "2  MEDSTSPKQEKENQEELGETRRPWEGKTAASPQYSEPESSEPLEAK...      Cytoplasm   \n",
       "3  MALPGARARGWAAAARAAQRRRRVENAGGSPSPEPAGRRAALYVHW...  Mitochondrion   \n",
       "4  MALLVDRVRGHWRIAAGLLFNLLVSICIVFLNKWIYVYHGFPNMSL...  Cell membrane   \n",
       "\n",
       "                                      tokenized_seqs  Label  \n",
       "0  [3884, 8570, 3840, 6832, 2277, 2221, 1020, 904...      0  \n",
       "1  [8772, 7207, 1857, 1688, 5461, 3901, 4899, 424...      1  \n",
       "2  [1565, 3797, 2513, 516, 1428, 6558, 6568, 7337...      0  \n",
       "3  [8939, 2538, 9262, 4438, 2547, 302, 60, 3064, ...      2  \n",
       "4  [8939, 6897, 6013, 1021, 3034, 2863, 8501, 697...      3  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = numericalizeClass(df, 'Location', label_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JGmFcAjJkfpk",
    "outputId": "c8a3c476-176a-44a9-edb3-40dbd8b5fecb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cytoplasm': 0,\n",
       " 'Endosome': 1,\n",
       " 'Mitochondrion': 2,\n",
       " 'Cell membrane': 3,\n",
       " 'Nucleus': 4,\n",
       " 'Endoplasmic reticulum': 5,\n",
       " 'Secreted': 6,\n",
       " 'Golgi apparatus': 7,\n",
       " 'Extracellular': 8,\n",
       " 'Peroxisome': 9,\n",
       " 'Lysosome/ Vacuole': 10}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "wlpBWvdWkfpl",
    "outputId": "30c5f808-392a-43fd-e799-742ca4ef9fa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3647.,   95.,  858., 3993., 5462.,  651., 1079.,  316.,  375.,\n",
       "         138.]),\n",
       " array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAnUfQNPkfpl"
   },
   "source": [
    "! It might help to rebalance the classes since classes 0, 3 and 4 are now over represented. Therefore, the model can just default to class 4. Since, if the model predicts class 4 for every sequence it will have an accuracy of around 34% already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jSGjtNVkfpm"
   },
   "source": [
    "An other point is that the sequence should have some information to work with, according to this paper:\n",
    "> https://www.nature.com/articles/s41598-019-38746-w\n",
    "\n",
    "A motif can be in between 3-20 amino acids, thus 1-6/7 kmers. I chose 5 kmers. However training time took way longer in that way. Therefore, I know choose another seqeunce length to reduce training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "hV_YQwZCkfpm"
   },
   "outputs": [],
   "source": [
    "df['Length'] = df['tokenized_seqs'].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "M1Uk4Cffkfpm",
    "outputId": "45aa26d8-3bba-4630-dafb-b4318e382baa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.,  21.,  15.,   5.,  18.,  31.,  57.,  93., 104., 163.]),\n",
       " array([  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90., 100.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['Length'], range=(0, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoMVJdmwkfpn"
   },
   "source": [
    "Based on this, and reduce training time. I now choose a sequence length of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2SGZXjgkfpn",
    "outputId": "2c59f273-d341-40e0-8242-a8f76a04d6ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16552"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['Length'] >= 50]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Se0r2uHnkfpo"
   },
   "source": [
    "This only removed about 60 entries.\n",
    "Now, to address class imbalance. Create a weighted sampler. However, the current weight labels are not correct. For each entry there should be a weight attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3628, 1: 95, 2: 854, 3: 3966, 4: 5459, 5: 649, 6: 1072, 7: 316, 8: 375, 9: 53, 10: 85}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2191880135331078,\n",
       " 0.005739487675205413,\n",
       " 0.05159497341710972,\n",
       " 0.23960850652489124,\n",
       " 0.3298090865152247,\n",
       " 0.03920976317061382,\n",
       " 0.06476558724021267,\n",
       " 0.019091348477525374,\n",
       " 0.02265587240212663,\n",
       " 0.0032020299661672308,\n",
       " 0.0051353310778153695]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_of_labels = dict(df['Label'].value_counts(sort=False))\n",
    "\n",
    "weight_labels = []\n",
    "\n",
    "print(total_of_labels)\n",
    "\n",
    "for total_of_label in total_of_labels.values():\n",
    "    weight = total_of_label / len(df)\n",
    "    weight_labels.append(weight)\n",
    "    \n",
    "weight_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aMErvL7kfpo"
   },
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ZdainIjGkfpp"
   },
   "outputs": [],
   "source": [
    "class AminoClassifierDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, num_classes):\n",
    "        self.df = df\n",
    "        self.num_classes = num_classes \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        x = torch.LongTensor(self.df.iloc[idx]['tokenized_seqs'])\n",
    "        x = x.to(dev) \n",
    "        \n",
    "        y = torch.LongTensor([self.df.iloc[idx]['Label']])\n",
    "        y = y.to(dev)\n",
    "    \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTJPhWeQkfpp"
   },
   "source": [
    "## The Protein Classifier\n",
    "\n",
    "Creating the complete protein from its parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tS3F6Jmkkfpp"
   },
   "source": [
    "### AWD-LSTM\n",
    "Start with the AWD-LSTM, which encodes the protein sequence and is already trained.\n",
    "\n",
    "I can add more dropout and I should add including the last hidden layers (cell and hidden states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "8-QAxw1zkfpp"
   },
   "outputs": [],
   "source": [
    "class WeightDropout(torch.nn.Module):\n",
    "    \"Apply dropout to LSTM's hidden-hidden weights\"\n",
    "\n",
    "    def __init__(self, module, weight_p):\n",
    "        super(WeightDropout, self).__init__()\n",
    "        self.module = module\n",
    "        self.weight_p = weight_p\n",
    "\n",
    "        # Save the name of the layer weights in a list\n",
    "        num_layers = module.num_layers\n",
    "        layer_base_name = 'weight_hh_l'      \n",
    "        self.layer_weights = [layer_base_name + str(i) for i in range(num_layers)]\n",
    "\n",
    "        # Make a copy of the weights in weightname_raw\n",
    "        for weight in self.layer_weights:\n",
    "\n",
    "            w = getattr(self.module, weight)\n",
    "            del module._parameters[weight]\n",
    "            self.module.register_parameter(f'{weight}_raw', torch.nn.Parameter(w))\n",
    "\n",
    "    def _setweights(self):\n",
    "        \"Apply dropout to the raw weights\"\n",
    "        for weight in self.layer_weights:\n",
    "            raw_w = getattr(self.module, f'{weight}_raw')\n",
    "            if self.training:\n",
    "                w = torch.nn.functional.dropout(raw_w, p=self.weight_p)\n",
    "            else:\n",
    "                w = raw_w.clone()\n",
    "            setattr(self.module, weight, w)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        return self.module(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(torch.nn.Module):\n",
    "    \"Apply dropout to an Embedding with probability emp_p\"\n",
    "\n",
    "    def __init__(self, emb_p=0):\n",
    "        super(EmbeddingDropout, self).__init__()\n",
    "        \n",
    "        self.emb_p = emb_p\n",
    "\n",
    "    def forward(self, inp):\n",
    "       \n",
    "        drop = torch.nn.Dropout(self.emb_p)\n",
    "        placeholder = torch.ones((inp.size(0), 1))\n",
    "        mask = drop(placeholder)      \n",
    "        out = inp * mask\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "l91JM4j3kfpq"
   },
   "outputs": [],
   "source": [
    "class AWD_LSTM(torch.nn.Module):\n",
    "    def __init__(self, num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz = 1):\n",
    "        super(AWD_LSTM, self).__init__()\n",
    "\n",
    "        # Embedding with droput\n",
    "        self.encoder = torch.nn.Embedding(vocab_sz, emb_dim)\n",
    "        self.emb_drop = EmbeddingDropout(emb_p=embed_p)\n",
    "\n",
    "\n",
    "        # Dropouts on the inputs and the hidden layers\n",
    "        self.input_dp = torch.nn.Dropout(p=input_p)\n",
    "        self.hid_dp = torch.nn.Dropout(p=hidden_p)\n",
    "\n",
    "        # Create a list of lstm layers with wieghtdropout\n",
    "        self.lstms = []\n",
    "        for i in range(num_layers):\n",
    "            self.lstms.append(\n",
    "                WeightDropout(nn.LSTM(input_size=emb_dim, hidden_size=hid_sz, num_layers=1), weight_p))\n",
    "        self.lstms = nn.ModuleList(self.lstms)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = torch.nn.Linear(hid_sz, vocab_sz)\n",
    "\n",
    "        # Save all variables        \n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_sz = hid_sz\n",
    "        self.hidden_p = hidden_p\n",
    "        self.embed_p = embed_p\n",
    "        self.input_p = input_p\n",
    "        self.weight_p = weight_p\n",
    "        self.batch_sz = batch_sz\n",
    "\n",
    "        # Initialize hidden layers        \n",
    "        self.reset_hidden()\n",
    "        self.last_hiddens = (self.hidden_state, self.cell_state)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        \"\"\"Forward pass AWD-LSTM\"\"\" \n",
    "\n",
    "        ys = []\n",
    "\n",
    "        hiddens = self.last_hiddens\n",
    "\n",
    "        hidden_states = [hiddens]\n",
    "\n",
    "        for i, lstm in enumerate(self.lstms):\n",
    "\n",
    "            # Embed the input and add dropout\n",
    "            embed = self.encoder(xs[i])\n",
    "            embed_dp = self.emb_drop(embed)\n",
    "            input_dp = self.input_dp(embed_dp)\n",
    "\n",
    "            # Dropout on the hidden states\n",
    "            hiddens_dp = []\n",
    "\n",
    "            for hidden_state in hidden_states[i]:\n",
    "                hiddens_dp.append(self.hid_dp(hidden_state))\n",
    "\n",
    "            hiddens_dp = tuple(hiddens_dp)\n",
    "            \n",
    "            # Go trough one LSTM layer\n",
    "            output, hiddens = lstm(input_dp.view(1, 1, -1), hiddens_dp) \n",
    "\n",
    "            # Detach hidden states\n",
    "            det_hiddens = []\n",
    "\n",
    "            for hidden in hiddens:\n",
    "                det_hiddens.append(hidden.detach())\n",
    "\n",
    "            det_hiddens = tuple(det_hiddens)\n",
    "\n",
    "            hidden_states.append(det_hiddens)\n",
    "\n",
    "            ys.append(output.squeeze(0))\n",
    "\n",
    "        y = torch.stack(ys, dim=0)\n",
    "\n",
    "        self.last_hiddens = hidden_states[-1]\n",
    "\n",
    "        return y, hidden_states\n",
    "\n",
    "    def reset_hidden(self):\n",
    "        self.hidden_state = torch.zeros((1, self.batch_sz, self.hid_sz)).to(dev)\n",
    "        self.cell_state = torch.zeros((1, self.batch_sz, self.hid_sz)).to(dev)\n",
    "        self.last_hiddens = (self.hidden_state, self.cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gr8AWMzkfpq"
   },
   "source": [
    "### SentenceEncoder\n",
    "\n",
    "This part encodes the whole sequences in seq_lenghts using the pretrained AWD-LSTM language model.\n",
    "\n",
    "We use the Identity class to replace the decoder in the original AWD-LSTM. \n",
    "\n",
    "Finally, the model should not be updated. Therefore, the forward pass is in torch.no_grad()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jlFVmQtqkfpr"
   },
   "outputs": [],
   "source": [
    "class Identity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "cA2cTY_Ekfpr"
   },
   "outputs": [],
   "source": [
    "class SentenceEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, model):\n",
    "        super(SentenceEncoder, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            inp_size = inp.shape[0]\n",
    "\n",
    "            # It is nicer to add padding\n",
    "            max_iterations = int(inp_size / self.seq_len)\n",
    "\n",
    "            hidden_state_outputs = []\n",
    "            cell_state_outputs = []\n",
    "            \n",
    "            for i in range(0, self.seq_len * max_iterations, self.seq_len):\n",
    "                _, hidden = self.model(inp[i: i + self.seq_len + 1])\n",
    "                \n",
    "                for states in hidden:\n",
    "                    hidden_state_outputs.append(states[0])\n",
    "                    cell_state_outputs.append(states[1])\n",
    "                        \n",
    "            hidden_state_outputs = torch.cat(hidden_state_outputs, dim = 1)\n",
    "            cell_state_outputs = torch.cat(cell_state_outputs, dim = 1)\n",
    "\n",
    "            return (hidden_state_outputs, cell_state_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeO_z7m-kfpr"
   },
   "source": [
    "### PoolingLinearClassifier\n",
    "\n",
    "The encoded sequence is needed to be pooled, otherwise the model can not use the information for classification.\n",
    "\n",
    "Then, the data is normalized using batchnorm.\n",
    "Dropout is applied to prevent overfitting.\n",
    "And linear layers with a ReLU activiation are used to classify the pooled protein data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "w28cdQdpkfpr"
   },
   "outputs": [],
   "source": [
    "def pool_encoded_sequence(output):\n",
    "    r\"\"\"Pool the encoded AA sequence and \n",
    "    return one vector with the max_pool and avg_pool concatenated\"\"\"\n",
    "    \n",
    "    hidden_states = output[0].squeeze(0)\n",
    "    cell_states = output[1].squeeze(0)\n",
    "\n",
    "    last_hidden_state = hidden_states[-1, :]\n",
    "    last_cell_state = cell_states[-1, :]\n",
    "\n",
    "    hidden_state_avg = hidden_states.sum(dim=0) / hidden_states.shape[0]\n",
    "    cell_state_avg = cell_states.sum(dim=0) / cell_states.shape[0]\n",
    "\n",
    "    hidden_state_max = hidden_states.max(dim=0)[0]\n",
    "    cell_state_max = hidden_states.max(dim=0)[0]\n",
    "\n",
    "    x = torch.cat([last_hidden_state, last_cell_state, hidden_state_avg, cell_state_avg, \\\n",
    "                  hidden_state_max, cell_state_max], 0).unsqueeze(0)  \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "nCOiXkDUkfpr"
   },
   "outputs": [],
   "source": [
    "class PoolingLinearClassifier(torch.nn.Module):\n",
    "    r\"\"\"Pool the outputs from the encoder and classify it.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, batch_sz):\n",
    "        super(PoolingLinearClassifier, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.batch_sz = batch_sz\n",
    "        \n",
    "        if batch_sz > 1:\n",
    "            \n",
    "            self.layers = nn.Sequential(\n",
    "                nn.BatchNorm1d(1150 * 6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Dropout(p=0.2, inplace=False),\n",
    "                nn.Linear(in_features=1150 * 6, out_features=1150, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm1d(1150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Dropout(p=0.1, inplace=False),\n",
    "                nn.Linear(in_features=1150, out_features=50, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Dropout(p=0.1, inplace=False),\n",
    "                nn.Linear(in_features=50, out_features=num_classes, bias=True)\n",
    "            )\n",
    "        else:\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Dropout(p=0.2, inplace=False),\n",
    "                nn.Linear(in_features=1150 * 6, out_features=1150, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=0.1, inplace=False),\n",
    "                nn.Linear(in_features=1150, out_features=50, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=0.1, inplace=False),\n",
    "                nn.Linear(in_features=50, out_features=num_classes, bias=True)\n",
    "            )\n",
    "            \n",
    "        \n",
    "    \n",
    "    def forward(self, inp):\n",
    "        output_encoder = inp\n",
    "        pooled_output = pool_encoded_sequence(output_encoder)\n",
    "        y = self.layers(pooled_output)\n",
    "        y = y.sum(dim=0)\n",
    "        \n",
    "        return y        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZmt9xEekfpr"
   },
   "source": [
    "### Combine everything in the protein classifier\n",
    "\n",
    "Combine every class and part in the protein classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "KPF-qx-lkfpr"
   },
   "outputs": [],
   "source": [
    "class proteinClassifier(torch.nn.Module):\n",
    "    r\"\"\"The complete protein classifier\"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, seq_len, num_classes, batch_size, pretrained_file=False):\n",
    "        super(proteinClassifier, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_sz = hid_sz\n",
    "        self.hidden_p = hidden_p\n",
    "        self.embed_p = embed_p\n",
    "        self.input_p = input_p\n",
    "        self.seq_len = seq_len\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        language_model = AWD_LSTM(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p)\n",
    "        \n",
    "        if pretrained_file:\n",
    "            language_mode = torch.load(pretrained_file, map_location=torch.device(dev))\n",
    "        \n",
    "        language_model.decoder = Identity()\n",
    "        \n",
    "        encoder = SentenceEncoder(seq_len, language_model)\n",
    "        \n",
    "        classifier = PoolingLinearClassifier(num_classes, self.batch_size)\n",
    "        \n",
    "        self.layers = nn.Sequential(encoder, classifier)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        y = self.layers(inp)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSUp6mZHkfps"
   },
   "source": [
    "## Model hyperparameters and train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "Rf1MiKM_kfps"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "emb_dim = 400 # Embeddding dimension\n",
    "hid_sz = 1150 # Hidden size\n",
    "num_layers = 3 # Number of LSTM layers stacked together\n",
    "seq_len = 50 # Based on paper mentioned above\n",
    "batch_size = 1 # Cannot use other batch size since inputs are not the same length \n",
    "\n",
    "# Dropout parameters\n",
    "\n",
    "embed_p = 0.1 # Dropout probability on the embedding\n",
    "hidden_p = 0.3 # Dropout probability on hidden-to-hidden weight matrices\n",
    "input_p = 0.3 # Dropout probablity on the LSTM input between LSTMS\n",
    "weight_p = 0.5 # Dropout probability on LSTM-to-LSTM weight matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMaUgLbpkfpu"
   },
   "source": [
    "### Load in the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "-RhParSjkfpu"
   },
   "outputs": [],
   "source": [
    "pretrained_model = '/home/mees/Desktop/Machine_Learning/subcellular_location/models/AA_LM_v2.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aW7z7yI-kfpu",
    "outputId": "62562bb1-aa62-444f-e180-8d93ff377aaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(label_dict)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJuNCOxskfpu",
    "outputId": "1a6f8ffc-4452-4d83-f3cc-d21bf831e1c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "proteinClassifier(\n",
       "  (layers): Sequential(\n",
       "    (0): SentenceEncoder(\n",
       "      (model): AWD_LSTM(\n",
       "        (encoder): Embedding(9317, 400)\n",
       "        (emb_drop): Dropout(p=0.1, inplace=False)\n",
       "        (input_dp): Dropout(p=0.3, inplace=False)\n",
       "        (hid_dp): Dropout(p=0.3, inplace=False)\n",
       "        (lstms): ModuleList(\n",
       "          (0): WeightDropout(\n",
       "            (module): LSTM(400, 1150)\n",
       "          )\n",
       "          (1): WeightDropout(\n",
       "            (module): LSTM(400, 1150)\n",
       "          )\n",
       "          (2): WeightDropout(\n",
       "            (module): LSTM(400, 1150)\n",
       "          )\n",
       "        )\n",
       "        (decoder): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): PoolingLinearClassifier(\n",
       "      (layers): Sequential(\n",
       "        (0): Dropout(p=0.2, inplace=False)\n",
       "        (1): Linear(in_features=6900, out_features=1150, bias=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "        (4): Linear(in_features=1150, out_features=50, bias=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Dropout(p=0.1, inplace=False)\n",
       "        (7): Linear(in_features=50, out_features=11, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = proteinClassifier(num_layers, vocab_sz, emb_dim, hid_sz, hidden_p,\n",
    "                         embed_p, input_p, weight_p, seq_len, num_classes, batch_size, pretrained_model)\n",
    "model.to(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYwstDEKkfpv"
   },
   "source": [
    "## Learning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "FmSpwoR7kfpv"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "adam_betas = (0.7, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "Ajfvm-krkfpv"
   },
   "outputs": [],
   "source": [
    "# Costfunction and optimize algorithm\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=adam_betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXkpsc7hkfpv"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "uDa2Eq40kfpv"
   },
   "outputs": [],
   "source": [
    "# Load the data in the DataSet\n",
    "AADataset = AminoClassifierDataset(df, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "GHShLf7Lkfpw"
   },
   "outputs": [],
   "source": [
    "# Split the data in an 80/20% split for training and testing\n",
    "data_len = len(AADataset)\n",
    "train_part = int(0.8 * data_len)\n",
    "test_part = data_len - train_part\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(AADataset, [train_part, test_part])\n",
    "\n",
    "train_weight_sampler = torch.utils.data.WeightedRandomSampler(weight_labels, train_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "fbs-M47qkfpw"
   },
   "outputs": [],
   "source": [
    "# Load the data into data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, sampler=train_weight_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKsYU03Nkfpw",
    "outputId": "a6006639-7874-4394-ccc1-b58cd88af89d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 0 MSE Train Loss: 0.0 ; Validation Loss: 11.35682161984423.\n",
      "Epoch duration 437.68601691325506 minutes.\n",
      "Epoch: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-10682932e135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-126-bf0909efd8b0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epochs, model, train_loader, test_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# For the correct shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-8b020a65e3ff>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-eedab9e09650>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-a3ac69f1cf6a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mhiddens_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiddens_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-6ae087298e53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setweights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-6ae087298e53>\u001b[0m in \u001b[0;36m_setweights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mraw_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{weight}_raw'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    981\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[1;32m    982\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model, loss_history, val_loss_history = train_model(epochs, model, train_loader, test_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "keinNjOmkfpx",
    "outputId": "76b14142-0506-4857-80dc-1227d8c617fd"
   },
   "outputs": [],
   "source": [
    "show_losses(loss_history, val_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99DY1ETOkfpx"
   },
   "outputs": [],
   "source": [
    "save_model_file = '/home/mees/Desktop/Machine_Learning/subcellular_location/models/trained_protein_classifier_part2.pt'\n",
    "torch.save(model.state_dict, save_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ik ga de outputs controleren van de verschillende layers of die hetzelfde zijn als Fastai outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(test_loader, 0):\n",
    "    # get the inputs; data is a one input (batch size), and y\n",
    "\n",
    "    x, y = data\n",
    "    x = x.squeeze(0) # Squeeze x in the correct shape\n",
    "    y = y.squeeze(0) # Squeeze y in the correct shape\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    output = model(x)   \n",
    "    output = output.unsqueeze(0) # For the correct shape\n",
    "    loss = criterion(output, y)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLlLv3BVkfpx",
    "outputId": "a9d34cdd-4d40-4753-afb2-3b8f7c60df15"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-59b6bd33a9dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Show the accuracy on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-128-0dd93e7cab89>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Squeeze y in the correct shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-8b020a65e3ff>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-eedab9e09650>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-a3ac69f1cf6a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mhiddens_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiddens_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-6ae087298e53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setweights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-6ae087298e53>\u001b[0m in \u001b[0;36m_setweights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mraw_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{weight}_raw'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    981\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[1;32m    982\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Show the accuracy on test data\n",
    "accuracy(trained_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THbPwNEKkfpy",
    "outputId": "6929400f-45b9-4ada-f680-e9f7ceb5a985"
   },
   "outputs": [],
   "source": [
    "# Show the accuracy on train data\n",
    "accuracy(trained_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FbCIF6Vekfpy",
    "outputId": "7771899f-5e6e-460b-bd3a-8c14098aae56"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    all_predicted = []\n",
    "    ys = []\n",
    "    \n",
    "    for ind, data in enumerate(test_loader, 0):\n",
    "        x, y = data\n",
    "        x = x.squeeze(0) # Squeeze x in the correct shape\n",
    "        y = y.squeeze(0) # Squeeze y in the correct shape\n",
    "\n",
    "        output = model(x)\n",
    "        output = output.unsqueeze(0)\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        all_predicted.append(predicted.item())\n",
    "        ys.append(y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dyS-kzfdkfpy",
    "outputId": "b3a6ad8d-b851-43d8-ea25-d4e925779f5f"
   },
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(ys, all_predicted)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix, display_labels=list(label_dict.values()))\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "disp.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from this confusion matrix everything is mapped to class 4, this class is over represented ~5000 out of 16000 examples. So the cost is lowest to predict that one. Therefore, we have to use a weighted random sampler. And probably also batches to train faster. So first, implement training in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssrnJCMGkfpz",
    "outputId": "da80c4cc-f824-47f4-da19-7394882254d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Membrane': 0, 'Endosome': 1, 'Cytoplasm': 2, 'Mitochondrion': 3, 'pass membrane protein': 4, 'Single': 5, 'Nucleus': 6, 'Cell membrane': 7, 'Secreted': 8, 'Golgi apparatus': 9, 'Cell junction': 10, 'Peroxisome': 11, 'Cell projection': 12, 'Peripheral membrane protein': 13, 'Cytoplasmic vesicle': 14, 'Lysosome': 15, 'Chromosome': 16, 'anchor': 17, 'Endoplasmic reticulum membrane': 18, 'Cytoplasmic granule': 19, 'Cell surface': 20, 'Endoplasmic reticulum': 21, 'Endoplasmic reticulum lumen': 22, 'Melanosome': 23, 'Virion': 24, 'Vesicle': 25, 'Isoform 1': 26, 'Peroxisome matrix': 27, 'Sarcoplasmic reticulum': 28, 'Endomembrane system': 29, 'Membrane raft': 30, 'Midbody': 31, 'Lipid droplet': 32}\n"
     ]
    }
   ],
   "source": [
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testen met Fastai als referentie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.0000, -0.0000,  0.2283, -0.0674,  0.0000,  0.0000],\n",
       "        [-0.8458,  6.0831,  0.8827, -1.7225,  1.2066,  1.5383, -0.6647],\n",
       "        [-0.4909, -1.9670,  2.3101, -0.0000, -2.5325,  4.1651, -0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.2306, -0.0000,  1.5061,  0.0000,  0.0000,  0.0360, -0.0000],\n",
       "        [-0.0000,  0.0000,  0.0902,  1.2305,  1.1262,  0.0000, -0.0000],\n",
       "        [-0.0000, -2.6635,  1.5061,  1.3771,  0.0000,  0.0000, -0.0000],\n",
       "        [-0.0000, -0.0000,  2.3669, -0.7374,  1.4977, -1.2494,  0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding dropout\n",
    "encoder = torch.nn.Embedding(10, 7, padding_idx=1)\n",
    "emb_drop = torch.nn.Dropout(p=0.5)\n",
    "tst_inp = torch.randint(0,10,(8,))\n",
    "tst_out = emb_drop(encoder(tst_inp))\n",
    "tst_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(torch.nn.Module):\n",
    "    \"Apply dropout to an Embedding with probability emp_p\"\n",
    "\n",
    "    def __init__(self, emb_p=0):\n",
    "        super(EmbeddingDropout, self).__init__()\n",
    "        \n",
    "        self.emb_p = emb_p\n",
    "\n",
    "    def forward(self, inp):\n",
    "       \n",
    "        drop = torch.nn.Dropout(self.emb_p)\n",
    "        placeholder = torch.ones((inp.size(0), 1))\n",
    "        mask = drop(placeholder)      \n",
    "        out = inp * mask\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "tensor([5])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 7])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d454925e4c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtst_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtst_inp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "tst_inp = torch.randint(0,10,(1,))\n",
    "print(tst_inp.shape)\n",
    "print(tst_inp)\n",
    "encoder = torch.nn.Embedding(10, 7, padding_idx=1)\n",
    "encoded = encoder(tst_inp)\n",
    "print(encoded.shape)\n",
    "emb_drop = EmbeddingDropout(emb_p=0.5)\n",
    "tst_out = emb_drop(encoded)\n",
    "print(tst_out.shape)\n",
    "for i in range(8):\n",
    "    assert (tst_out[i]==0).all() or torch.allclose(tst_out[i], 2*encoder.weight[tst_inp[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8539,  2.2006, -0.0222,  0.4224,  1.2578, -0.5769, -0.7109],\n",
      "        [ 1.2339,  0.9903, -1.4282,  0.4318, -1.2289,  0.8093, -0.5293],\n",
      "        [ 2.0111, -0.3286,  0.6406,  1.2247,  0.5189,  0.4388, -0.2033],\n",
      "        [-0.7689, -1.3048,  0.5878,  0.1067,  0.1493, -0.9369,  0.1112],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-1.0289,  0.4262, -1.3303, -0.7120,  1.0064,  0.7244, -0.7196],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "torch.Size([8, 7])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.]])\n",
      "torch.Size([8, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
       "        [ 4.0222, -0.6571,  1.2812,  2.4494,  1.0377,  0.8776, -0.4066],\n",
       "        [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-2.0579,  0.8524, -2.6606, -1.4240,  2.0127,  1.4488, -1.4393],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_inp = torch.randint(0,10,(8,))\n",
    "drop = torch.nn.Dropout(0.5)\n",
    "encoder = torch.nn.Embedding(10, 7, padding_idx=1)\n",
    "encoded = encoder(tst_inp)\n",
    "print(encoded)\n",
    "print(encoded.shape)\n",
    "\n",
    "placeholder = torch.ones((tst_inp.size(0), 1))\n",
    "mask = drop(placeholder)\n",
    "print(mask)\n",
    "print(mask.shape)\n",
    "\n",
    "encoded * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 1, 6, 9, 3, 7, 1])\n",
      "torch.Size([8])\n",
      "tensor([[-0.9948, -1.3262, -1.4499,  0.1425,  0.3206, -0.5852,  0.4061],\n",
      "        [-1.8594,  0.1413,  1.6815,  0.9142, -0.5407, -0.8299,  0.1996],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.5156,  0.3203,  1.5523,  1.3523,  1.1633, -0.5602, -0.1511],\n",
      "        [ 0.9568, -0.6867,  1.0243,  1.7308, -0.3247,  0.3144, -0.0669],\n",
      "        [ 1.0880, -1.1082,  2.1468, -0.7218,  1.9135, -0.3273,  0.4820],\n",
      "        [-0.5337,  0.5620, -2.7735,  1.1568, -0.2488,  0.5844, -2.2863],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9948, -1.3262, -1.4499,  0.1425,  0.3206, -0.5852,  0.4061],\n",
       "        [-1.8594,  0.1413,  1.6815,  0.9142, -0.5407, -0.8299,  0.1996],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.5156,  0.3203,  1.5523,  1.3523,  1.1633, -0.5602, -0.1511],\n",
       "        [ 0.9568, -0.6867,  1.0243,  1.7308, -0.3247,  0.3144, -0.0669],\n",
       "        [ 1.0880, -1.1082,  2.1468, -0.7218,  1.9135, -0.3273,  0.4820],\n",
       "        [-0.5337,  0.5620, -2.7735,  1.1568, -0.2488,  0.5844, -2.2863],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding dropout\n",
    "encoder = torch.nn.Embedding(10, 7, padding_idx=1)\n",
    "emb_drop = EmbeddingDropout(emb_p=0.5)\n",
    "tst_inp = torch.randint(0,10,(8,))\n",
    "print(tst_inp)\n",
    "print(tst_inp.shape)\n",
    "encoded = encoder(tst_inp)\n",
    "print(encoded)\n",
    "tst_out = emb_drop(encoded)\n",
    "tst_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-197-d05f4a6a9746>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-197-d05f4a6a9746>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    (tst_out==).sum()\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "(tst_out==).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tst_out==1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_layers, vocab_sz, emb_dim, hid_sz, hidden_p, embed_p, input_p, weight_p, batch_sz = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 20, got 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-9c2712273055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-d575e1e6e46e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# Go trough one LSTM layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# Detach hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-6ae087298e53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setweights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    178\u001b[0m             raise RuntimeError(\n\u001b[1;32m    179\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m--> 180\u001b[0;31m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 20, got 100"
     ]
    }
   ],
   "source": [
    "tst = AWD_LSTM(2, 100, 20, 10, 0.2, 0.02, 0.1, 0.2)\n",
    "x = torch.randint(0, 100, (1,5))\n",
    "bs,sl = x.shape[:2]\n",
    "print(bs)\n",
    "print(sl)\n",
    "r = tst(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "proteinClassifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
