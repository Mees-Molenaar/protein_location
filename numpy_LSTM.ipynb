{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = open('shakespeare.txt', 'r').read()\n",
    "data = open('nescio.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size = len(data)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set is length 202367\n",
      "Vocab set is length 89\n"
     ]
    }
   ],
   "source": [
    "print(f'Data set is length {data_size}')\n",
    "print(f'Vocab set is length {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple character embedding\n",
    "char_to_idx = {char:i for i, char in enumerate(chars)}\n",
    "idx_to_char = {i:char for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 # I think its times 0.01 to avoid exploding gradients\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "\n",
    "# bias\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should happen:\n",
    "\n",
    "x = data[0]\n",
    "y = data[1]\n",
    "\n",
    "h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h))\n",
    "\n",
    "y_pred = np.dot(Why, h)\n",
    "\n",
    "loss = y - y_pred (Simplified, we will use Cross-entropy loss for it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(xs, targets, hidden, Wxh, Whh, Why, bh, by):\n",
    "    \"\"\"Calculate the forward pass\n",
    "    Calculate the cross-entropy loss, \n",
    "    which is based on the softmax functon and the negative log likelyhood.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_preds = {}\n",
    "    hs = {}\n",
    "    softmax_probs = {}\n",
    "        \n",
    "    hs[-1] = np.copy(hidden)\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(xs)):\n",
    "        x = xs[i]\n",
    "        x_vec = np.zeros((vocab_size, 1)) # vectorize the input\n",
    "        x_vec[x] = 1\n",
    "\n",
    "        # Calculate the new hidden, which is based on the input and the previous hidden layer\n",
    "        hs[i] = np.tanh(np.dot(Wxh, x_vec) + np.dot(Whh, hs[i - 1]) + bh)\n",
    "        # Predict y\n",
    "        y_preds[i] = np.dot(Why, hs[i]) + by\n",
    "        \n",
    "        softmax_probs[i] = np.exp(y_preds[i]) / np.sum(np.exp(y_preds[i])) # Softmax probabilty\n",
    "        loss += -np.log(softmax_probs[i][targets[i], 0]) #Negative loss likelyhood\n",
    "    \n",
    "    prev_hidden = hs[len(xs) - 1]\n",
    "\n",
    "    return hs, softmax_probs, loss, prev_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 5 required positional arguments: 'Wxh', 'Whh', 'Why', 'bh', and 'by'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-41d5146eacd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m126\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Karpathy code to test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 5 required positional arguments: 'Wxh', 'Whh', 'Why', 'bh', and 'by'"
     ]
    }
   ],
   "source": [
    "# Test for the forward pass\n",
    "xs = [char_to_idx[ch] for ch in data[100:125]]\n",
    "prev_hidden = np.zeros((hidden_size, 1))\n",
    "targets = [char_to_idx[ch] for ch in data[101:126]]\n",
    "\n",
    "hs, softmax_probs, loss, prev_hidden = forward(xs, targets, prev_hidden)\n",
    "\n",
    "# Karpathy code to test\n",
    "test_hs = {}\n",
    "test_ys = {}\n",
    "test_xs = {}\n",
    "test_loss = 0\n",
    "test_ps = {}\n",
    "test_hs[-1] = np.copy(prev_hidden)\n",
    "\n",
    "for t in range(len(xs)):\n",
    "    test_xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    test_xs[t][xs[t]] = 1\n",
    "    test_hs[t] = np.tanh(np.dot(Wxh, test_xs[t]) + np.dot(Whh, test_hs[t-1]) + bh) # hidden state\n",
    "    test_ys[t] = np.dot(Why, test_hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    test_ps[t] = np.exp(test_ys[t]) / np.sum(np.exp(test_ys[t])) # probabilities for next chars\n",
    "    test_loss += -np.log(test_ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "assert loss - test_loss < 0.01 or test_loss - loss > 0.01 #Klein verschil in loss kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_preds, target): \n",
    "    \"\"\"Calculate the cross-entropy loss, \n",
    "    which is based on the softmax functon and the negative log likelyhood.\"\"\"\n",
    "    \n",
    "    softmax_probs = {}\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(y_preds)):\n",
    "        softmax_probs[i] = np.exp(y_preds[i]) / np.sum(np.exp(y_preds[i])) # Softmax probabilty\n",
    "\n",
    "        loss += -np.log(softmax_probs[i][target[i], 0]) #Negative loss likelyhood\n",
    "    \n",
    "    return softmax_probs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-8c33e1059bf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Test for loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msoftmax_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Karpathy code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_preds' is not defined"
     ]
    }
   ],
   "source": [
    "#Test for loss function\n",
    "target = [char_to_idx[ch] for ch in data[101]]\n",
    "softmax_probs, loss = loss_function(y_preds, target)\n",
    "\n",
    "# Karpathy code\n",
    "test_loss = 0\n",
    "test_ps = {}\n",
    "test_ps[0] = np.exp(test_ys[0]) / np.sum(np.exp(test_ys[0])) # probabilities for next chars\n",
    "test_loss += -np.log(test_ps[0][target[0],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "assert softmax_probs[0][0] == test_ps[0][0]\n",
    "assert loss == test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_gradients():\n",
    "    \"\"\"Initialize the gradients to 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dby, dbh = np.zeros_like(by), np.zeros_like(bh)\n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dby, dbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(softmax_probs, hs, xs, targets):\n",
    "    \"\"\"Perform the backward pass\"\"\"\n",
    "    \n",
    "    dWxh, dWhh, dWhy, dby, dbh = initialize_gradients()\n",
    "    \n",
    "    # Initialize empty next hidden layer for the first backprop\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    \n",
    "    for i in reversed(range(len(xs))):\n",
    "        # X to vector\n",
    "        x = xs[i]    \n",
    "        x_vec = np.zeros((vocab_size, 1))\n",
    "        x_vec[x] = 1\n",
    "\n",
    "        dy = np.copy(softmax_probs[i])\n",
    "        dy[targets[i]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "\n",
    "        dby += dy   \n",
    "        dWhy += np.dot(dy, hs[i].T)\n",
    "        dh = np.dot(Why.T, dy) + dhnext\n",
    "        dhraw = (1 - hs[i] * hs[i]) * dh  \n",
    "        dWxh += np.dot(dhraw, x_vec.T)\n",
    "        dWhh += np.dot(dhraw, hs[i-1].T)\n",
    "        dbh += dhraw\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "    # Clip to prevent exploding gradients\n",
    "    for dparam in [dWhy, dWxh, dWhh, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dby, dbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for gradient initialization\n",
    "dWxh, dWhh, dWhy, dby, dbh = initialize_gradients()\n",
    "\n",
    "\n",
    "# Karpathys code\n",
    "test_dWxh, test_dWhh, test_dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "test_dbh, test_dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "\n",
    "assert dWhy.shape == test_dWhy.shape\n",
    "assert dbh.shape == test_dbh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'softmax_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-f45e03b34414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Test backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Karpathy code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'softmax_probs' is not defined"
     ]
    }
   ],
   "source": [
    "#Test backward\n",
    "x = [char_to_idx[ch] for ch in data[100]]\n",
    "dWxh, dWhh, dWhy, dby, dbh = backward(softmax_probs, hs, x, target)\n",
    "\n",
    "#Karpathy code\n",
    "test_dhnext = np.zeros_like(hs[0])\n",
    "test_dy = np.copy(test_ps[0])\n",
    "test_dy[target[0]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "test_dWhy += np.dot(test_dy, hs[0].T)\n",
    "test_dby += test_dy\n",
    "test_dh = np.dot(Why.T, test_dy) + test_dhnext # backprop into h\n",
    "test_dhraw = (1 - hs[0] * hs[0]) * test_dh # backprop through tanh nonlinearity\n",
    "test_dbh += test_dhraw\n",
    "\n",
    "x_vec = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "x_vec[x] = 1\n",
    "\n",
    "test_dWxh += np.dot(test_dhraw, x_vec.T)\n",
    "test_dWhh += np.dot(test_dhraw, hs[-1].T)\n",
    "test_dhnext = np.dot(Whh.T, test_dhraw)\n",
    "\n",
    "for dparam in [test_dWxh, test_dWhh, test_dWhy, test_dbh, test_dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    \n",
    "print(test_dWhy[0][0])\n",
    "print(dWhy[0][0])\n",
    "\n",
    "assert test_dWhy[0][0] == dWhy[0][0]\n",
    "assert test_dby[0][0] == dby[0][0]\n",
    "assert test_dWxh[0][0] == dWxh[0][0]\n",
    "assert test_dWhh[0][0] == dWhh[0][0]\n",
    "assert test_dhnext[0][0] == dhnext[0][0]\n",
    "assert test_dbh[0][0] == dbh[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_gradients(Wxh, Whh, Why, by, bh, dWxh, dWhh, dWhy, dby, dbh, learning_rate):\n",
    "    \"\"\"Update the gradients using stochastic gradient descent.\"\"\"\n",
    "    Wxh -= learning_rate * dWxh\n",
    "    Whh -= learning_rate * dWhh\n",
    "    Why -= learning_rate * dWhy\n",
    "    bh -= learning_rate * dbh\n",
    "    by -= learning_rate * dby\n",
    "    \n",
    "    return Wxh, Whh, Why, bh, by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad(Wxh, Whh, Why, by, bh, dWxh, dWhh, dWhy, dby, dbh, mWxh, mWhh, mWhy, mbh, mby, learning_rate):\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update\n",
    "        \n",
    "    return Wxh, Whh, Why, by, bh, mWxh, mWhh, mWhy, mbh, mby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n, Wxh, Whh, Why, bh, by):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters from the model\n",
    "    h is the memory state, seed_ix is seed letter for the first time step\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(seed_ix, int):\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[seed_ix] = 1\n",
    "        ixes = []\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "            y = np.dot(Why, h) + by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "            x = np.zeros((vocab_size, 1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "    else:\n",
    "        print(seed_ix)\n",
    "        xs = [char_to_idx[ch] for ch in seed_ix]\n",
    "        ixes = list(xs[:-1])\n",
    "        \n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[xs[-1]] = 1\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "            y = np.dot(Why, h) + by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "            x = np.zeros((vocab_size, 1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        \n",
    "        \n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(data, seq_length, epochs):\n",
    "    \"\"\"Perform RNN over the data\"\"\"\n",
    "    data_len = len(data)\n",
    "    \n",
    "    # Initialize weights\n",
    "    Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "    Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "    Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "    \n",
    "    # bias\n",
    "    bh = np.zeros((hidden_size, 1))\n",
    "    by = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    weights = [Wxh, Whh, Why, bh, by]\n",
    "    \n",
    "    # Store losses\n",
    "    losses = []\n",
    "    smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "    \n",
    "    # Memory voor Adagrad\n",
    "    mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    mbh, mby  = np.zeros_like(bh), np.zeros_like(by)\n",
    "    \n",
    "    # Loop over the epochs\n",
    "    for i in range(epochs):\n",
    "        n = 0\n",
    "        # Loop over the amount of sequences\n",
    "        sequences_amount = int(data_len // seq_length)\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        for j in range(sequences_amount):\n",
    "            \n",
    "            start_pos = seq_length * j\n",
    "            # Reset and go from the start of data\n",
    "            if n == 0 or start_pos + seq_length + 1 >= data_len:\n",
    "                prev_hidden = np.zeros((hidden_size, 1))\n",
    "             \n",
    "            # Embed the inputs and targets\n",
    "            xs = [char_to_idx[ch] for ch in data[start_pos:start_pos+seq_length]]\n",
    "            targets = [char_to_idx[ch] for ch in data[start_pos+1:start_pos+seq_length+1]]\n",
    "\n",
    "            # Forward pass\n",
    "            hs, softmax_probs, loss, prev_hidden = forward(xs, targets, prev_hidden, *weights)\n",
    "    \n",
    "            #Backward\n",
    "            dWxh, dWhh, dWhy, dby, dbh = backward(softmax_probs, hs, xs, targets)\n",
    "\n",
    "            # Update gradients with adagrad\n",
    "            Wxh, Whh, Why, by, bh, mWxh, mWhh, mWhy, mbh, mby = adagrad(Wxh, Whh, Why, by, bh, dWxh, dWhh, dWhy, dby, dbh, mWxh, mWhh, mWhy, mbh, mby, learning_rate)\n",
    "\n",
    "            # Update gradients with gradient descent\n",
    "            #Wxh, Whh, Why, bh, by = update_gradients(Wxh, Whh, Why, by, bh, dWxh, dWhh, dWhy, dby, dbh, learning_rate)\n",
    "            \n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "            \n",
    "            if n % 100 == 0:\n",
    "                losses.append(smooth_loss)\n",
    "                print(f'{i + 1}: {n} / {sequences_amount}: {smooth_loss}')\n",
    "                \n",
    "                \n",
    "                # Print a sample\n",
    "                sample_ix = sample(prev_hidden, xs[0], 200, *weights)\n",
    "                txt = ''.join(idx_to_char[ix] for ix in sample_ix)\n",
    "                print('----- \\n' + txt + '  \\n------')\n",
    "\n",
    "            n += 1\n",
    "            \n",
    "        print(f'Finished epoch {i + 1}.')\n",
    "        \n",
    "        \n",
    "    # Print gefeliciteerd bericht\n",
    "    gefeliciteerd = sample(prev_hidden, 'Gefeliciteerd', 50, *weights)\n",
    "    txt = ''.join(idx_to_char[ix] for ix in gefeliciteerd)\n",
    "    print('----- \\n' + txt + '  \\n------')\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0 / 8094: 112.21591544290557\n",
      "----- \n",
      "wDL8ü3rkFsxZFòSgf mSG;:é!qs-écM3xe;T8tlMkDbeZP(Bâôm( B2â-7y\n",
      "züFKâròê0lKTE/ë.Sa5j(3û8ó1eG  1oReK;4yoi\n",
      "û(UJIHI?cGDM1h6(Sxi át.7Dc?ïV,ZWç(i3W4ál'./jLe)y9tKk,wIçBVwMRVgPfOC?âçxáMD3I!(N6\"mO SuèIfdNRK;ô;D'6  \n",
      "------\n",
      "1: 100 / 8094: 111.72232367033438\n",
      "----- \n",
      ",etiignisjic ceoicitHhhre \n",
      "irhhh,whcHdratnpiditihtojcnasiptrwiiti jjeircccjc\n",
      "tcepeeke itchdcccn ejipte.rtesita geietd whetdijirntge\n",
      "ditcDhtsneeeevnahai,cecie,estiE ngtec\n",
      "cDhhteedscetnecd\n",
      "u near d cnec  \n",
      "------\n",
      "1: 200 / 8094: 109.00269987444308\n",
      "----- \n",
      "u do draidaaa\n",
      "   ez tt dguD agunieork deelet etlvvucboulgieninh hd  Dz  ugmnhgnar \n",
      "tz  uud innreaggddgsd\n",
      "  hnodgau tvndgn umnd \"eiorg nkl  z gl\n",
      "aDguGng,kizg.lg ai\n",
      " rh\"uwaveuacb Duijgasggsgjtek guagDtg  \n",
      "------\n",
      "1: 300 / 8094: 106.24318227319259\n",
      "----- \n",
      "dsv' 'dwenudjskisdlenmkzopeecirlal ed'giatenlt\n",
      "athee soj. ''eeszisne i.tg me we'e ezjieszote.  op lj  tononinl''z'esrer.umj'nae  n wl  'ezeenhn .zi p' egvn den  lninsek'e eess sUl ls oe t' d an esronn  \n",
      "------\n",
      "1: 400 / 8094: 103.670286577302\n",
      "----- \n",
      "celetjcman f  inlea, aino,mnded nehl hre me r\n",
      "ef Hs.nwesdaié zsc e\n",
      "Hn hcmadec. n\n",
      "s ,i\n",
      "ahnc.\"ur eodndediaar,ni é\n",
      "m\n",
      "ilngeci. n\n",
      "dnlr anuagnahaHnc,ima t\n",
      "ei hetmoggladégmirebutien nirgaDin \n",
      "éi\"r hDsae hih\n",
      "  \n",
      "------\n",
      "1: 500 / 8094: 100.95461585072616\n",
      "----- \n",
      "ae.eetreeevtn,s eeatjutirezler e gahreen etjihZtn \n",
      "eitsnretlnrtaree drne utntajttf taatatk   ewneneejeteab wag eaersaeu rwtt \n",
      "eet eerntieu jt urrEaedj nen leaDaee zb k\n",
      "egtozreeei ne  hmeetrtaze n aant  \n",
      "------\n",
      "1: 600 / 8094: 98.21430821723891\n",
      "----- \n",
      "ad he mosf vo'iaanZvavaeadas tan vn doeavshEan zet aoeda'iseot .ataoea \n",
      " aoeladloasi\n",
      "d sener, vthin aoon dm e oadthn oneedzaeahd eeahoon diee ek  petdotnron lDen oatisen aad adeazd, alnhlel awn tn das  \n",
      "------\n",
      "1: 700 / 8094: 95.39186873778748\n",
      "----- \n",
      "wae \n",
      "l , den zen en ei hen \n",
      "u en orate En keivaavenren vtro' hroin vien zidre en eaeeeje se aakn rrs be he habn reehaaan ben nawekin ot sn ernEkipkee en kawet kozke dmvven keelaotn b,nt Wr tenven ze r  \n",
      "------\n",
      "1: 800 / 8094: 92.58568329384202\n",
      "----- \n",
      "hitozan olan w' recher efuEd oern haausalde ies geee, has Eaadal waalaovije teogen zrorelwen ee deen Itel Aeegen gegfelen ab gigen hijn habaweeouan zo\n",
      "we jlars hée aacet Aoanet E je,orwars Ael zen maa  \n",
      "------\n",
      "1: 900 / 8094: 90.14515677382443\n",
      "----- \n",
      "n hd nt tensr e daer zon hen okt Maar Ji datt lrgnb ket ke d ovot ns ger k ten bre haaaven harg,\n",
      "stkr 'at\n",
      "st  an vaars dt e .at. mp buaoe torwen ht dij tintoocNagn hazamelss\n",
      "\n",
      "jt rt getj kijst zin zaan  \n",
      "------\n",
      "1: 1000 / 8094: 87.6049848792407\n",
      "----- \n",
      " gtimwj\n",
      " z'aaa, Bet ban tart iet or crk Be dtiee di rBi\n",
      "r, men winde \"fjtenditn. zoee dJat me, toerd t bveu 't hfk ark\n",
      "n\n",
      "de ttnitjdt \"Emt mt hiraten, g, 'tou he deekt woor Aam bitt,. ?att o?t ze ht \n",
      "w  \n",
      "------\n",
      "1: 1100 / 8094: 85.45487924466468\n",
      "----- \n",
      "en jejerget ee sFen eooden voa er den e aru hatjen ae dag,. e den denvogdenss hren ne cer kinr vaare haar Mn 'a keaaoije\n",
      "dmier laaar ie iite ie g' geneen mebt bgerZ ivge woeneo s, cote etgek,  vaor zs  \n",
      "------\n",
      "1: 1200 / 8094: 83.28531113393225\n",
      "----- \n",
      "en\n",
      "vens 't hutelte doen hero\" oen\n",
      "hdenen lIjen deon ea mem d ts zevden en ddkegde 't en daat aav dis'nn jel dinge, Hn gehs  plen ve 'p n uasn an. piesmendjt en Zaaal'chag ed del lett iend le pen dBerm  \n",
      "------\n",
      "1: 1300 / 8094: 81.46250269124832\n",
      "----- \n",
      "s zeln bendep maaden zoeb vete deooven v'nmeleer mooter mleet wik van\n",
      "he peur gel even bevet,\n",
      "mouben en lteniju\n",
      "mpen moane\n",
      "maen zin zsg, en bn ven ouaget en jet zen gen 'ibet en.zrigomisk gpen  ichene  \n",
      "------\n",
      "1: 1400 / 8094: 79.48410793864288\n",
      "----- \n",
      "venden vense op gekRnentee en tesfen begee ze zeteen, bse\n",
      "boovor , dijelhenDei-telt venst\n",
      ". uHt g r.. WieMben dourkzenge kehden deichaneeter eie dijomoosste sndeuvvten. en wan z, \"fjeoeleuwerwelres ge  \n",
      "------\n",
      "1: 1500 / 8094: 77.66870400463479\n",
      "----- \n",
      "men in delwoen em wars duche mart.\n",
      "rteoo-e me claarchet sn da't luk et inige diuf oom Ee denrde e zer\n",
      "daar kanr let it tTops waarn wel wan.lgen. vin handent wi rt \n",
      "t int wend vef wier wamd Kaage wuien  \n",
      "------\n",
      "1: 1600 / 8094: 76.13161957681189\n",
      "----- \n",
      "gel.\n",
      "en vag 't ril op hleg ke op henddak hte\n",
      "aane Elers hij. Din lp hot, guHmoomsep.\n",
      "s. z.rfsf dierEwe 't n iefDeneeroiber \"kooitn\n",
      "s  as poil opimn\n",
      "dles heereekoen oe dan hr HCag boihben vent mp ven o  \n",
      "------\n",
      "1: 1700 / 8094: 74.89109786358705\n",
      "----- \n",
      "nt wiguén dinzan. oede\n",
      "ge ertenkeen op. \" ruqcuien\" maando'k oezop ge Ze non ele,, Douloooi hatn ae en \"de ecs n \" get., kpel, grger ookHen\n",
      "\n",
      "nicgsn.\n",
      "Doo\"\n",
      "\"kontus na daar dtanuinde roe? ge me tichasten  \n",
      "------\n",
      "1: 1800 / 8094: 73.33346041593477\n",
      "----- \n",
      "hen ar en oon Hier dat vais laer woaan vatpteref rl ne g bachien aa ven ger opjelcegeraarondan ilin 't mienddichuae ss.gen ven 'chelier ofs algegeet wien d amn zen boeenate zen hrbmnoihijn schazen die  \n",
      "------\n",
      "1: 1900 / 8094: 72.04460652749907\n",
      "----- \n",
      "ondgeget g wi :Ierovas ki din hidher jnijdijdinte kakl nalk\n",
      "diep gake di, van hevie kek \"iniktezvoatwjebgnt\n",
      "zoop zvevein oo zwadivak ve Haaavin, a. haar ovtt okt\n",
      "redaraan vat ken athis ves\n",
      "doecht er i  \n",
      "------\n",
      "1: 2000 / 8094: 71.1261694854986\n",
      "----- \n",
      "n ip\n",
      "er ze dar.dol\n",
      ", mocht. zonden zo azeihrichnee waskilwaeHgeg klerint een zovest zen hp  wieusckaenrdet Hwess wle nie detemen\n",
      "en hordop\n",
      "de.gmije oet jog enling kwschn ist dijfdareen eèu kis ilfsskc  \n",
      "------\n",
      "1: 2100 / 8094: 70.17818978012511\n",
      "----- \n",
      "n el\"s ovar zat hoi gael zot mast.\n",
      "rls Iuich oo zoouks Er ben jn zaaim re zelwonijd st dee lan po zan ars,\n",
      "Ass Iest art vwartee den Doge  sten ar di en.\n",
      "st je daee  ad , zilej zoad  \n",
      "ar n, e! jls iHet  \n",
      "------\n",
      "1: 2200 / 8094: 68.9642547161403\n",
      "----- \n",
      "tadevdens in zaabrien orin limrotijj. erhst.ten, wijllen hoes zaeten. Zood\n",
      "\"Ee st\n",
      "szier\"\n",
      "\n",
      "ij oaoas ge di nginfd.e aat n wa ooe dijlijrjo\" minfp\n",
      "t.t\n",
      "nlerfs \"t blilbelaan er\n",
      ", haar\n",
      "gegetet ak biwdeef wa  \n",
      "------\n",
      "1: 2300 / 8094: 67.8940282158684\n",
      "----- \n",
      "n sen oon lien s erE blsn dijk \n",
      "itetet endeift werwerier he rokt haar d. h't veldemen cn zvegs wag o, koek vaoroom et het el zoluin een. ict wen datevunder: dve htg vog nweeg waen uichte moosIn mon ei  \n",
      "------\n",
      "1: 2400 / 8094: 67.10712443683319\n",
      "----- \n",
      "renden haar boovwobst\n",
      "'t, hfscht mn, zoj win\n",
      "eg, buge rresenfenkiN aan movtezelst\n",
      "wie bpjrange val vetuZlin voof toe delel veer,\n",
      "r darplee, ble \n",
      "Vtmt en aan hèed Zeeplet\"blaan\n",
      "ove lee ten aan jen han   \n",
      "------\n",
      "1: 2500 / 8094: 66.58154621643507\n",
      "----- \n",
      "wVr bi zos daarger wavzrawren eizeig\n",
      "haar\n",
      "elawanger lijgelals vaalte jeet aales, zot dattipege hlzr dats vaijj koe  eridf ren daargens wan panlaanozaalrin dool\n",
      "\n",
      "ren heelt. zie naor wert ni rsben, st s  \n",
      "------\n",
      "1: 2600 / 8094: 65.70817815226843\n",
      "----- \n",
      "heit gan eelapt daar tatelat, Higliero'tx lleroraar stet dat 't llarpsg zen tek vess vatahlolelooClaal\n",
      "\n",
      "lkOd lOtera Das  eer helst voerhelkjef, z't en ocht li ar date op degdòen hachlawas kesH gen 't,  \n",
      "------\n",
      "1: 2700 / 8094: 64.8529679056292\n",
      "----- \n",
      "aut ze w't bord wlold te1wveren gaht an geng zoanden\n",
      " En dagen aar zind Moor ood El gijt, woerdegowtentententautetoganekogen vog voo, wid ndef oogen En doena he wane Hal zootwa\"erki,then\n",
      "wengen dien a  \n",
      "------\n",
      "1: 2800 / 8094: 64.34045069018877\n",
      "----- \n",
      "r lendetj rp keertef\n",
      "ge 't ket eVen, maalet eng iooetemoek hootet iMtearr, schek, we wacgeite wij gen deks gerenl\n",
      "Hikgeme \n",
      "de vovers men lijg totookt niben jetebvake wt arengereetemire uogje wietmoek   \n",
      "------\n",
      "1: 2900 / 8094: 63.88873254525951\n",
      "----- \n",
      "rteng moi Zijd hag  oo di muitd uezs ien\n",
      " waad nEhr hoondij daamek lielen hom, iad hagild uinklieé in en dordjen\".jer bdirloble ne din. zoeens vegelold dilf aad.\n",
      "\n",
      "nepp\n",
      "li f he h, ze wlandehos hh, waur  \n",
      "------\n",
      "1: 3000 / 8094: 63.599574714241896\n",
      "----- \n",
      "hijen vitos\" vitd 't, leers zien motzij lij ge Wa p, \"echder eren. daar Ge ste \"rva\" kerolicht. wat \n",
      "at s \"l: dicht zood.zHr ze dan?e schn waldj De zelwaulite gelget Mat dih dechichscltje he en vcn, E  \n",
      "------\n",
      "1: 3100 / 8094: 63.01426387010913\n",
      "----- \n",
      " zer, nijg wald nt 't vonke pcht  Ier, ilon zoud z'n niopjen ven door elkradi pend uoo vooin dinn Derijn in de looruloordevonde daviktezdozij \"e lindeldoor zm keden\" niel Nton Hi olken, W'd da en daw   \n",
      "------\n",
      "1: 3200 / 8094: 63.10382177551808\n",
      "----- \n",
      "t zr oerain er\n",
      "er een om. en\n",
      "ichete t z'n de, an\n",
      "om ens icht n toee t, Dendrof erken mog ien ue bieker Gt ben schek de ver mZint die ar bdasleeer aileu  Dhar\n",
      "st zn 't lati, Be ben, sfgen Hijnide\n",
      "Zoo w  \n",
      "------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 3300 / 8094: 63.35675686040822\n",
      "----- \n",
      " Zatvok. , a8rt.\n",
      "\n",
      "a8 Fr wantemooppthruchtjend zwu ds dalenddett zakmer. Zar deekladori \"Zat .sV\n",
      "vanlie\n",
      "toBwaen ee  e daEn ketem \"uikemagen sten kt haan denr Lan laaarBa pig sp haateu. Darteg.\n",
      "Dan na t  \n",
      "------\n",
      "1: 3400 / 8094: 63.23306161231633\n",
      "----- \n",
      "hik even zer pe mend een ?ader waoet, Iaan eid endt dubist ze van ilBo k een nienken\n",
      "ke.\"\n",
      "\"?ig wer Jien, 't sskipt, Endael deesslet, watdindt 't taat rteenuat s Ak dazopaak tfe \n",
      "mn\n",
      "\n",
      "k bier watiamint t  \n",
      "------\n",
      "1: 3500 / 8094: 62.98942644689497\n",
      "----- \n",
      "ov ninie z'ers derj.\n",
      "der\n",
      "nadejte, es vee ent mf tt dad een nind\n",
      "\n",
      "kap. we smget neen woed z'ns en movlen 't zot oop gen ooet hmokjveeZd\n",
      "s kers amroBnerooherstelen aedeem\n",
      "ge BwaezeuHschors doorie deei    \n",
      "------\n",
      "1: 3600 / 8094: 62.66048765436689\n",
      "----- \n",
      "n ildet Jaan de rins weeind de st eek beerdaars hicht duen ap ienscelorassscheri. dadiem ele marlanoomaareufleldugen en nezsttjelils ood vatk ij, En an daalaleo\n",
      "dlife:\n",
      "\"ozilgen. Els on weelun zitdegen  \n",
      "------\n",
      "1: 3700 / 8094: 62.07468646628963\n",
      "----- \n",
      "telin nijkmerderdelboen Oiekterkit han. tetekrdoed detem einr 't nleel dij. kje. naarchachorere vit. it ben\n",
      "'t\n",
      "ke mor eren Bmen. AJan en, oon dorgen vek  e ve hatte\n",
      ";loier aorest en. mee st. dieit vor  \n",
      "------\n",
      "1: 3800 / 8094: 61.75460957010357\n",
      "----- \n",
      "t den tendente\" j de schecht er, \"me deraps domdij mij\" ak den Ja zenGhijkk. hijrgkestaar alsei ken Vak en 't kn iem ij en i\n",
      "Aanden z't mooittumen Ilakdajschijs roer lesk.geriet biin at bie,\n",
      "De keof.   \n",
      "------\n",
      "1: 3900 / 8094: 61.377711398040205\n",
      "----- \n",
      ".,\n",
      "\".\n",
      "bar ad en an ap aatijstwe\n",
      "dfiaptoudet;e 't aJe hanit\n",
      " Dan je, Bacham e vamat zig ze\n",
      "wark Joem hen au, Ban k werd ele uwag bantwag luwami beeenwaave\n",
      "\n",
      "Jani. En 'n Jos past kitden. Den ze bond teee  \n",
      "------\n",
      "1: 4000 / 8094: 61.154858410822904\n",
      "----- \n",
      " en\n",
      ". Oeimin at er Fien ad kaoi da lum en 't zijbs nJan catd itdei zeen aanen zeven\n",
      "d aravu oond aan vout. de sin uijderden er der \"a ijgamod liemme, er?enczoeg ein liorij tind,\n",
      "ereen wabgen zaiden ba  \n",
      "------\n",
      "1: 4100 / 8094: 61.097827323863385\n",
      "----- \n",
      "uwer beermen. wold. \"amen zid mi schahtegen leigeen tjek Uast wacht hij, kzon Jai \"alichasg.len wari derizenkKescher tiet den Bon dan \"kieig\"J hiegtt\n",
      "\"s vag enagemt hu\" mhger. bui ten, \"e. ZoAgt,.zih   \n",
      "------\n",
      "1: 4200 / 8094: 61.03284623467644\n",
      "----- \n",
      "dender. wan dee\"gen toal\n",
      " 'k di naveng 't zig hen ten 'twnterert man n. Ne val el keelke. vel litve ktadenk:\n",
      "den Gakit om evgatten wit heg moderjwardaZe de. hen danIben kandze gengen ieed kgen En wele  \n",
      "------\n",
      "1: 4300 / 8094: 60.76670051898164\n",
      "----- \n",
      "id,\n",
      "ijnbe vai\n",
      "erin zoetigen keid map 't vacht mim det ee don, Dasnde,\n",
      "Zrdamre dijsdeeerdkoom eint, tendaapeteet lat gm enmerebeerden\n",
      "hat Longen, arijr,\n",
      "baad heejchte Danenden, Akteret b't gen bat das   \n",
      "------\n",
      "1: 4400 / 8094: 60.55788684009401\n",
      "----- \n",
      "kikteupeeenkrag,, \"en has Lier gk le bpeen dier \"ievrmiki, kaItbg zievkook, oen eeralnoou\"jegeen zoug ek zer?kbl zvel Zaal \"op e\" iehak wet \"\"tezpikit,\n",
      "\n",
      "AAat ge ten,\"\n",
      "er st n.\"\"\" 't zets.\n",
      "\"owi. En. Je  \n",
      "------\n",
      "1: 4500 / 8094: 60.767772036334904\n",
      "----- \n",
      "icn au dan en deé rap Izoer vék IW en z't veen\n",
      "Vopdeg Bepr schte ten.\n",
      "\n",
      "Daer hat laopkevtt zuomlas?\n",
      "Istee. dep Jaaft 'che teens sh) tepin jlem zoofgapen faapkij bijnn je. oomdatt he ba 'a ziek\n",
      "kp hen k  \n",
      "------\n",
      "1: 4600 / 8094: 60.58820624978248\n",
      "----- \n",
      "emer al Hievik ie er7e\" tes Jatjet tk\n",
      "min kijert mirndeerd aan de kvek veten we aloi, Mien ge tloe nIom. Zij tVtken\n",
      "de jini hiepl mimaop\" is Iiut...\n",
      "\n",
      "e, n he\n",
      "rsteg nijn venrijer, aldtenst toli. hmen g  \n",
      "------\n",
      "1: 4700 / 8094: 60.26125669246506\n",
      "----- \n",
      "ge mag uit brie averrjeeren\" be gentacn ek voi on \"ad oi z'\n",
      "\n",
      "heer herstad eereid zoi.n aag teen ke woonn wareer gegen, zos pare diemren ilnaet grse rijss z'n scewer zry gan ie nlit\"\n",
      "Hat nted den Aan w  \n",
      "------\n",
      "1: 4800 / 8094: 60.02543749272775\n",
      "----- \n",
      "zoydae panwet wan ik melerikonk hail Zij r ik,.\n",
      "\"\"\" w't n, Iongenpreen\n",
      "\"\n",
      "ork uichaat Bolieen auwer zoes teben. Ii waro-an\n",
      "aapHik\n",
      "\n",
      "cnn. ar ig iktem der.\"\"dapapg rao naar zooed ik houwate den \", dajee.\n",
      "  \n",
      "------\n",
      "1: 4900 / 8094: 59.79827360263774\n",
      "----- \n",
      "cniet vojn\n",
      "\n",
      "Dag bi Jvornapanije i)ver. toel 'e vat dijten.\"\" ip ek dij er ek van dij jege. \". zil\n",
      "\"en, \"Nes Japig óenn\n",
      "\"Zonizaan\"ui kigitank en kas\n",
      "ik.\"\" een zijg tooss wad rielt\" z't dees ik ruiklhoe  \n",
      "------\n",
      "1: 5000 / 8094: 59.64244954021745\n",
      "----- \n",
      " von\n",
      "wat erkeloogd mien im Aen jen\n",
      "he\n",
      "\n",
      "an ste ik oen isleel mat vam haite er seg, hoek heer miet ken ken. Ip waan gen ijçben mozievwul4en, doekege zen gevegen el de die open had ndogik ik dlom mook no  \n",
      "------\n",
      "1: 5100 / 8094: 59.32638568732165\n",
      "----- \n",
      "n ten lok kangen ge den, zwerten brkerans oont aigrlougelnencKrbisbenad ordanofjeig ien ren. Dak toven n gelelmen belgen,\n",
      "\n",
      "en lingjnt cvoet hijn.\" moon boren.\n",
      "er benier. noed en borgel bandden nabfen   \n",
      "------\n",
      "1: 5200 / 8094: 59.281948419955654\n",
      "----- \n",
      "r chter dep zoodstes. Ee hooke wer schoeneerte schtje\n",
      " dijteitê. haar onk Gpit Tij\n",
      "Bes geporer 's ter vat haar\n",
      "een dad bebehler  li boordten sp hi cl Tijers om n. en kedershten en\n",
      "en wiarlazett dijvou  \n",
      "------\n",
      "1: 5300 / 8094: 59.09176695429167\n",
      "----- \n",
      "eken kmen des ers mat h)eeros denit ilvetsterachnenpermerdendet\n",
      "'t nade meederven nitter\n",
      "hachtedeen darken omuichor toor. wachten wazas hanroopdocherden  af helen 't Jemen erhst. ste s, em ar en i daa  \n",
      "------\n",
      "1: 5400 / 8094: 58.691781067335086\n",
      "----- \n",
      "oul wess en zebromg loen iezin, Bier\n",
      "aln\n",
      "in?l. Oleldasten woodgens\n",
      "Jes\n",
      ", Dij honwershee voe  ond van doet, wan bo hiebren hier? out waviten zom zachtsoeel, Jan stuemard\n",
      "lachte dut ganchieT warez, roch  \n",
      "------\n",
      "1: 5500 / 8094: 58.62255793053247\n",
      "----- \n",
      "oépen daar zoed laar kleek han daar rookken om tee\n",
      ", zmonke Bog gerachten herord,\n",
      "\"Uens be en een, ze ketjarend zi lijgeiten klal sadaet de tNuitmijk hoen\n",
      "\n",
      "Ben haninNen iefgep: noger laar had de woomd  \n",
      "------\n",
      "1: 5600 / 8094: 58.51534087965483\n",
      "----- \n",
      "alom\n",
      "roof.\n",
      "\n",
      "Mooopwdieve ptetoorndei\n",
      "mede vaader zeldechon keenêwe gen ste mos scht hefd gaar kevler\n",
      "zijen wacheg gans\n",
      "'t verte aNersten den gen\n",
      "en. \n",
      "Hén eem veelt ween komel.\n",
      "Hij zoofn een n. en staad  \n",
      "------\n",
      "1: 5700 / 8094: 58.31635311098011\n",
      "----- \n",
      " vanlinis g uin en ze bijn een\n",
      " We keen riearn niet aat gen gen ern wad wat\n",
      "\n",
      "lij deroom brat wei\n",
      "ijn biet en nieten ziagges aantwalde zot was hat. naeken n, zij wantilebin jeftt\n",
      "wijn \n",
      "egs ar ir oorden  \n",
      "------\n",
      "1: 5800 / 8094: 58.09769674327739\n",
      "----- \n",
      " dee lijemels pan ellit en preheen meeellen opderalrnwe scher en erdoopter bloode Dat  ooptoonde mijj trach. oogaart zeitiGme\n",
      "\n",
      "me dait iend is Alde zijl\n",
      "zop engerk maalten he istr ooss wamen. Dai zeen  \n",
      "------\n",
      "1: 5900 / 8094: 58.003323875513274\n",
      "----- \n",
      " gomeroboomdeten stden?js\n",
      "ondes\n",
      "\n",
      "zoon\n",
      "en hoon opke stenen beelu\n",
      "\n",
      "Dan naam. Watmel 't miet 'n deen. 't zijn Hijnden en \"Iet aiprorde met nakskS en to moest waspakeen en kestemsden kijf hij nven marluni  \n",
      "------\n",
      "1: 6000 / 8094: 57.59626102596127\n",
      "----- \n",
      "n dezen en oNe ren wet mSvereg. zur zoon nat.u\". s deden darief de den zeet dagen haar hook ae e venkagIjet oik\n",
      "t den ean vourui zij de af we zeeerergen lategen, zost keg sk de ken.\n",
      "\n",
      "de er plooen, kom  \n",
      "------\n",
      "1: 6100 / 8094: 57.511488118473146\n",
      "----- \n",
      "zog\n",
      "\n",
      "\"andeikden erl-éi. htafd meele de sterd uit Brhaken ee schinderdad\n",
      "ur bvar latf een det zaar 't ten aut,t\n",
      "getp-en ood chaap j't ie Buaalber. de koorte ap rouschtar ek en makeelanieter\"\n",
      "éneete Zou  \n",
      "------\n",
      "1: 6200 / 8094: 57.41112482749194\n",
      "----- \n",
      "ls, des ge kr de det gen han jin moon moud. En deet. nuaug nee zoeken wien, er wink niet\n",
      "rij jed geer be zog zet hagveen deest, zief zittel. En naavechoor én naur, Seren war emen. Nier han\n",
      "doorte lwe   \n",
      "------\n",
      "1: 6300 / 8094: 56.974932987834784\n",
      "----- \n",
      " deik dazrlijescht. zwijfi. er wlonzij gedopbendeprm vicd de eik zijl schie kld bonden sond \"her me wet dood Jit en moot on 't te den zijn\n",
      "ze det he hadd oovatden. En aastood\n",
      "lacheen waar de hopd dan   \n",
      "------\n",
      "1: 6400 / 8094: 56.815830307887865\n",
      "----- \n",
      "zer maar geer hejopee nuann.\n",
      "\n",
      "Eò veevon en, Ouder, Ilaerzeil londt ze deest en\n",
      "alat. haadrwazie zit nie pe kon en eeten we z'n Pris dok ien Zezeelaan\n",
      "bluschij de zachudghietes,\"\"avei, aHven stee genk   \n",
      "------\n",
      "1: 6500 / 8094: 56.64225904626444\n",
      "----- \n",
      "ss\n",
      "dak jeng te ster de gen zes aaann.\n",
      "En dand. Ze en el en aan een tik las tagen, Aks, 'n on eoren\n",
      "vood, schied\n",
      "waad grwet 'tst deelt,\n",
      "steng of nom ,d di zeke gek Balieven\n",
      "\n",
      "é' heen onstam\n",
      "\n",
      "den wasten,  \n",
      "------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 6600 / 8094: 56.33679174816946\n",
      "----- \n",
      " nachtenk lijeevaviet, gaar kpen mundet?\n",
      "en maafelken, swoekóordeder en el gezij 't waad móek Bieft en en lede getankens vit en Nen er opd kafsstan deiker.\n",
      "\n",
      "Eg datreen vaat Zoos on verschachupiste mee  \n",
      "------\n",
      "1: 6700 / 8094: 56.23461047228062\n",
      "----- \n",
      "ri guori inzieri wadrer een teidalr gege deblerd gem asts eler muit me moen die, Maai waar he ret here haad oPden hond, dombrooken zs deezet-inbel-t. daartoedschan-iteden opdereuroegagegen Epkrsdeczis  \n",
      "------\n",
      "1: 6800 / 8094: 55.878325775986994\n",
      "----- \n",
      "nhharkens bilkounag zan ken\n",
      "Dooooan, zij hacht denk. donden en ul eerbschenen zij wachn nod or zon kovzeenten. oven denin schtene\n",
      " rom min pi tooe zoofblim en Hoyzegjnn, acnisten\n",
      "Bin hoo st aalloi kul  \n",
      "------\n",
      "1: 6900 / 8094: 55.921335822347004\n",
      "----- \n",
      "en ig 't 'n Hoyent.\"Gvat el hoen Bavit 'tehins van veebis bek, Doornat. Dat oijts st\n",
      "er paar ie cn aad naprtichachter de 's Gop, ee moeken wan hond davan dellerren. ek al aalroulin im dek zos deen zej  \n",
      "------\n",
      "1: 7000 / 8094: 55.78902785980037\n",
      "----- \n",
      "oaalaihton waa\n",
      "\n",
      "\"Weenikieb de deven zan jeden vaar dankte te alms lijvereraan tein lielakla'n zaar wad er lij; in Nervabpankale.k gees wan dareikuapoes schidre schamrkwipelotijlijenbip Hinn in\n",
      "zaluud   \n",
      "------\n",
      "1: 7100 / 8094: 55.4143756409851\n",
      "----- \n",
      "clan on zon al zeigeen v'n opn 'x tan vade\n",
      "zerden, einlen\n",
      "opcherder, In deremr,\n",
      "raatelIendistis en ijdeien ger jet wazeng degen zotdelwieder mijfdejengen, i-dert en de dichhuilooplelinrerargender: in\n",
      "  \n",
      "------\n",
      "1: 7200 / 8094: 55.092224121358974\n",
      "----- \n",
      "nd ichar'e ard vat wal-istindelast de zije. Eanken even\n",
      "en dazin\n",
      "chandeuasteesten mat daapmereen deeen elast geen er st en uijfe Jle 't Ioof, Eldemen, Woe Ren, 't we hes\n",
      "den ootraan den, En er nudelde  \n",
      "------\n",
      "1: 7300 / 8094: 54.815008257278684\n",
      "----- \n",
      "n en nigerun en aan. Ep hien zichschlaak deeten mad\n",
      "uin\n",
      "ender, zoug notttijn get wie votdel gen tjjderd, ovenstin\n",
      "mikt deer,\n",
      "\n",
      "geploonteri Bit opstaklig zijn nogden inger nad k(ge\n",
      "wat wispen zoeren in   \n",
      "------\n",
      "1: 7400 / 8094: 54.75990003745323\n",
      "----- \n",
      "Nui breen mas . mofd vichten daar helorgenn, Vonifunrgek w't. En dehtewen ie er\n",
      "ladondituwint. En gink. En poor mijn\n",
      "e zooen\n",
      "laar te wad sstelbelerorbijgt maarnizivig ief man de dee en hourd maar Hijg  \n",
      "------\n",
      "1: 7500 / 8094: 54.8518762088385\n",
      "----- \n",
      "dn aanteros himhlad. \"taain en be st dens erscjen. \"\n",
      "\n",
      "am\n",
      "we geenen. ok was heen. waar ne vat eravond nie aal ekt ov zeg lad noch htjkeisden jfder nogrige mokbjet aan has en dan.gjem\n",
      "vad weervelen, in   \n",
      "------\n",
      "1: 7600 / 8094: 54.81634757413661\n",
      "----- \n",
      "pingen bonglen!deren Besldengen In?aths var stontend rier nie en en omjpag,beeded watter \", ringet om zooidrieze fvek naten kx. 'd tie, ninkijenscger ged ikt 'n erscop voobn enget ti Bal dat ik foen n  \n",
      "------\n",
      "1: 7700 / 8094: 55.01446846025626\n",
      "----- \n",
      "j. \"Wat in zier.\n",
      "It?er mezC omst\n",
      "onde st moure haare oij ik it wa4erstar. Ten daguichti de de s. muge bomergrd ik ik ras\n",
      "duit1 ik suar opkloemf diet det ij kle kk te st\n",
      "we daf nare geek de tat me badi  \n",
      "------\n",
      "1: 7800 / 8094: 55.1045747042038\n",
      "----- \n",
      "iets hij de lloendenaaplst Dpag laad dief en Pandare dan in mirvonlien ergt neesje. an hiet naar stian\n",
      "en er op ' zenden zit dezren. Eens zetn vagen van voon te\n",
      " an ient.\n",
      "T0hres aam 't aanken\"\n",
      "\n",
      "Hoy3er  \n",
      "------\n",
      "1: 7900 / 8094: 55.16712790556147\n",
      "----- \n",
      "terzood, den ink veb vond vofd an leg,\n",
      "rats\n",
      "de k.\n",
      "Hijfzichen van heeteid meeut, in deer heeren en kens. dekijkenshetraam velwerrin Gonde kulen 't desten.\n",
      "Dondenen 't in dret\n",
      "waal 't denit hagenkgekdad  \n",
      "------\n",
      "1: 8000 / 8094: 55.19184490769793\n",
      "----- \n",
      "aar den eenond\n",
      "en lwei zre\n",
      "stosd zenkeen opered. en gekochandange en. Hen berkevendeginderdongen. itdeer voenverkein gre mens.\"\"delerlen nies man jinand, ret oongek indeen.  oogerrevens.\"dwellijhmes,   \n",
      "------\n",
      "Finished epoch 1.\n",
      "Gefeliciteerd\n",
      "----- \n",
      "Gefeliciteer\n",
      "A s  st t l    .\" wi   o wi ill   k . mie ut  e v  \n",
      "------\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "losses = RNN(data, seq_length, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1f71c63ba2d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(xs, targets, hidden):\n",
    "    y_preds = {}\n",
    "    hs = {}\n",
    "    softmax_probs = {}\n",
    "        \n",
    "    hs[-1] = np.copy(hidden)\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(xs)):\n",
    "        x = xs[i]\n",
    "        x_vec = np.zeros((vocab_size, 1)) # vectorize the input\n",
    "        x_vec[x] = 1\n",
    "\n",
    "        # Calculate the new hidden, which is based on the input and the previous hidden layer\n",
    "        hs[i] = np.tanh(np.dot(Wxh, x_vec) + np.dot(Whh, hs[i - 1]) + bh)\n",
    "        # Predict y\n",
    "        y_preds[i] = np.dot(Why, hs[i]) + by\n",
    "        \n",
    "        softmax_probs[i] = np.exp(y_preds[i]) / np.sum(np.exp(y_preds[i])) # Softmax probabilty\n",
    "        loss += -np.log(softmax_probs[i][targets[i], 0]) #Negative loss likelyhood\n",
    "    \n",
    "    prev_hidden = hs[len(xs) - 1]\n",
    "    \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dby, dbh = np.zeros_like(by), np.zeros_like(bh)\n",
    "    \n",
    "    # Initialize empty next hidden layer for the first backprop\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    \n",
    "    for i in reversed(range(len(xs))):\n",
    "        # X to vector\n",
    "        x = xs[i]    \n",
    "        x_vec = np.zeros((vocab_size, 1))\n",
    "        x_vec[x] = 1\n",
    "\n",
    "        dy = np.copy(softmax_probs[i])\n",
    "        dy[targets[i]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "\n",
    "        dby += dy   \n",
    "        dWhy += np.dot(dy, hs[i].T)\n",
    "        dh = np.dot(Why.T, dy) + dhnext\n",
    "        dhraw = (1 - hs[i] * hs[i]) * dh  \n",
    "        dWxh += np.dot(dhraw, x_vec.T)\n",
    "        dWhh += np.dot(dhraw, hs[i-1].T)\n",
    "        dbh += dhraw\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "    # Clip to prevent exploding gradients\n",
    "    for dparam in [dWhy, dWxh, dWhh, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "        \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, prev_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun_v2(xs, targets, hidden):\n",
    "    # Forward pass\n",
    "    hs, softmax_probs, loss, prev_hidden = forward(xs, targets, hidden)\n",
    "    \n",
    "    #Backward\n",
    "    dWxh, dWhh, dWhy, dby, dbh = backward(softmax_probs, hs, xs, targets)\n",
    "\n",
    "    # Update gradients with adagrad\n",
    "    uWxh, uWhh, uWhy, uby, ubh, umWxh, umWhh, umWhy, umbh, umby = adagrad(Wxh, Whh, Why, by, bh, dWxh, dWhh, dWhy, dby, dbh, mWxh, mWhh, mWhy, mbh, mby, learning_rate)\n",
    "    \n",
    "    return loss, uWxh, uWhh, uWhy, uby, ubh, umWxh, umWhh, umWhy, umbh, umby, prev_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " PrttsstoootGooooooss'sDsoootDot ooooooosstuosssboob GoosssotoooooDooosDootoooooooooooootososooGoooooosogstooGgoooosostoooostotobssgs'osotoostoossssbsGootoootssGoooss'touoooooooooGotosoooooG'tsssGstoss \n",
      "----\n",
      "iter 0, loss: 114.925547\n",
      "----\n",
      "  jareen zot, zlonk ot zrzlendzozbnt ztaat dtdoar zaan zap zmt kk ze kl tonek nzzd zt z'dHaande dhak zunt zin zok an elll z tun voedzzer 'm di kt zor zf hatTk zk 'nt zrat zaa  mogzkEk zal, zt zut zzbn  \n",
      "----\n",
      "iter 100, loss: 115.111534\n",
      "----\n",
      " te nekechtijrendst 'manden wak deeeron denhaeltek\n",
      "\"viddet dr lsTPoddid reskht\n",
      "tkbiesginen Ar.s dd, 'e dachdesvd.ens\n",
      "Zdchovssgd wen taar dn pechralkecd sd\n",
      "sitdendens.\n",
      "tok\n",
      "'ird. eng geind. den dlsst or  \n",
      "----\n",
      "iter 200, loss: 110.904239\n",
      "----\n",
      " jez\n",
      "matt de illen bie torin dirt  pt do ded sensorort e e hegeg venog teen niden der dat.in han me td dlg dord lidt g.\n",
      "da dn 'auen de domiwel. breer det Nooe oelter wenwoodden dee, irigtg. hijd de ber \n",
      "----\n",
      "iter 300, loss: 106.716997\n",
      "----\n",
      "  wen De dinf. zod,\n",
      "te'lli\n",
      "jaaen watdezen te be\"sett in koouteloen, tinief Aves 'lfer ge lrge eOdk ijdeel Dn lechtg.iddel dis éi tid ws  hajt Eeer huer\n",
      "\n",
      " dien je me ze tolhs wegen heer zokde det loin b \n",
      "----\n",
      "iter 400, loss: 102.746470\n",
      "----\n",
      "  hagijé wattd\n",
      "Egri.matsti, dij daar dij, Min zoerder sw\" drij aasmenderdaptfen wn.\n",
      "gen derdt, zesdmerden lim irap vu ede epMos kaardelerijuvis wam den boien et dap paar Aaar\n",
      "Maar\n",
      "Zig jm ke rat. Moees. \n",
      "----\n",
      "iter 500, loss: 99.132793\n",
      "----\n",
      "  etsCtii tacijaar nk e, tu lat\n",
      "jn in dood a s 'nuv ein ooen s\n",
      "\n",
      " is den haoe densss \n",
      " En oen ar,\n",
      "zoen oen ooen Lleofs vu bn\n",
      "en veen dn var ke daer en dor iaar man\n",
      "en orof vog scak kod gi i,\n",
      "denten ooen \n",
      "----\n",
      "iter 600, loss: 95.660731\n",
      "----\n",
      " domdg doarhaaf laed vik 't t. wezichdediez'linkek oourijd man zed dijet.fat hag tov zoion dz'mdie, lin mander. Eelee dim mie moe ien vae haeive demgeg zoreneenop. dui, zorenigmee den\n",
      "\"inbelôar1rderdrd \n",
      "----\n",
      "iter 700, loss: 92.257455\n",
      "----\n",
      " n han soened gaar en, Elan. Eoastee elaar g va en ouhe hos zugfee zee ei orej zelen . En. Erien. En ma. haand lrlen de halg. ze mareeel aenwas zou, ran zaaogde nar, haar gis eet zas magk wan her\n",
      "kezen \n",
      "----\n",
      "iter 800, loss: 89.067516\n",
      "----\n",
      " voirimin bor man gen vaar\n",
      "en en en.ejn zekebeen der nopme ben eems ze bg ee oool. in iar benden. canrder.bhaar ok daar Hog denukeelvaar Latdtdet ve 't\n",
      "bork en \n",
      "mdenin bomensfkde net en zde vor kenten  \n",
      "----\n",
      "iter 900, loss: 86.398585\n",
      "----\n",
      "  han biaan moes\n",
      "isken\n",
      "havaaenen detensjen haar inteluslaal dtaal mijet kijon k't dent\n",
      "jins Jan tende 't vauszheei ie hamd zout hee z'ziet. \" in buenbit dgaar haarjent hatdijrendbet'en 'n vaarden baar  \n",
      "----\n",
      "iter 1000, loss: 83.672857\n",
      "----\n",
      "  'td,\n",
      "dicht, elddenden en be d, oeld, dichte heg jiet ken\n",
      "dich eek denmecht det ten, 't dogreracherer ood voen igt doerdeld. dee 'lreroir, bun, we hupte, laarerdrgen zeé lte raar et ligdd ectdd et ttj \n",
      "----\n",
      "iter 1100, loss: 81.411818\n",
      "----\n",
      " rderwi nan ze zogTup s  'n mene zechd zasjenkem daor. Hicatjatgesg,\n",
      "oinjar momeichg vs zitdjjinooichaarden\n",
      "aanmu\n",
      "hijjeen zie zonig haugep van woooogue scogbejeeeniias, veand en , lachtijekar\n",
      "hijdeevin \n",
      "----\n",
      "iter 1200, loss: 79.228945\n",
      "----\n",
      "  var mate,\n",
      "doddomge, wattder mat \"Ian zakt oam hortkeelwe kortan.-od, rar dijre,\n",
      "ooe git zaomeltk rogets wot wad be kas rachtk\n",
      "che fod. \"ten kerten gdecht,\n",
      "wacee zaadmoar fer guit pal zat keed wal ies \n",
      "----\n",
      "iter 1300, loss: 77.306053\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-a13f00129be1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-15b8a78165fb>\u001b[0m in \u001b[0;36mlossFun_v2\u001b[0;34m(xs, targets, hidden)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Update gradients with adagrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-f7760ffcf2d0>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(softmax_probs, hs, xs, targets)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdby\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdWhy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdhnext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mdhraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mdWxh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_idx[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_idx[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(idx_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, Wxh, Whh, Why, by, bh, mWxh, mWhh, mWhy, mbh, mby, prev_hidden = lossFun_v2(inputs, targets, hprev)\n",
    "\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
