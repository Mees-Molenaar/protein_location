{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 222444 characters, 99 unique.\n",
      "----\n",
      " áL9PKMNWBvPû9è3Z1ML,ï@nPkRP3jy﻿'MhjO]s7gUbwE9Z#0FcqxMáXAvSsxDQGg?âçRH-6@ikè*Oá/]é(ós9!EBLE-\n",
      "\n",
      "ôT@[;36.\n",
      "j;Tw]RóL/s4AzZOèz4E#éóMáVOFièfc\"m JxE*?IP XJ; d]FïHç1N(N$8R﻿Cï,pw﻿cXVn8ê'os]]zygX6b:G'#T!c á/ëGcfX \n",
      "----\n",
      "iter 0, loss: 114.877998\n",
      "----\n",
      " fooo  gbeuais Ee.at otae.vnejken'vtes -  aesrneon n'ea\n",
      " ei ofei  atdalerleat hesi  avekl,weg t oK ledsrn ldrments e rl nt PT  dsa tolsa   vdoe a9 ee t nal atio 1ae ao aeoce kedressN aun t io iP.o   dg \n",
      "----\n",
      "iter 100, loss: 113.737387\n",
      "----\n",
      " eDoaOdo \n",
      "iidwglin1ad rgdl eomitli  d eoeh'lkttehe  tnj\n",
      " d ntkede  nhh, droneroondiMia hn vondindaednnde eaceii tiét.tandaddi nskk oo eHPnhaedagdog   lacz  teco ehanilawtndna'aenapeogel. i aoetogdadAan \n",
      "----\n",
      "iter 200, loss: 110.774213\n",
      "----\n",
      "   ,, am gcvsedl denonn,evaz g o eaihe irKi vnta nrnetemvteDes ,.izerfn al,ndlhs rlkaiaia\"Od \n",
      "n,Gnt . na dtow ,ejneneeavrepe,asMiveeti gegln b  hdiremDeece za\n",
      "ns 'i alt  ddd \n",
      "wvwhohn gM eeogs ki upGhlw \n",
      "----\n",
      "iter 300, loss: 107.857164\n",
      "----\n",
      "  nl,sj  diHr'ur'n s .térbl hip . de nieerd 9dgekzetoi ll  ja:f i wlstblkoflawlDugn gbezden tdbo gke retedk\n",
      "releDegie\n",
      "n\n",
      " ee n npf e' eet n saktie\n",
      "looé joete ifjter l bad g dooe wbswba \n",
      "eein dteg\n",
      "moo ni \n",
      "----\n",
      "iter 400, loss: 105.002724\n",
      "----\n",
      " en nn me rod vzeanevn eeridio-ni coe\n",
      "din ntwges b,haaN.ton nn kii 'e\n",
      " aae da.teeknes., 'aTzaiovjTodn rgelâ dmien ozjdu Mu lkn nteaaitedtiee ae, Ee ss Dawgi rmdj\n",
      "lteaen jt.jinavrs r?s kesttr tlaerktir\n",
      " \n",
      "----\n",
      "iter 500, loss: 102.151259\n",
      "----\n",
      " e hooag  hemerk dejiljemaba'ivhep ern sarege wen.,sent\n",
      "as dstouev gedoen ,oork ekret hsaonvemooereen brei ea vcoupes jil boo gookeennlde. gaaga'n nseh eleebean en tn,epgaanez as es  ae néis enaaowe. e \n",
      "----\n",
      "iter 600, loss: 99.219298\n",
      "----\n",
      " ch n irht.\n",
      "de\n",
      "koan peQn nlghovudizn zacevf kaar\n",
      "o'ien ,aien heg doamgn za \n",
      " kn ditn Da ,\"i\n",
      "zuehererin boen dfeivcn d boe ,in dojfen en 't\n",
      "zad logenoduh hoheK\n",
      "nhor ean.Enzb, zing\n",
      "zim aaonn eevi wsn er  \n",
      "----\n",
      "iter 700, loss: 96.216295\n",
      "----\n",
      " n vnjtae. Ede hijs ir graue goit. pegevobeme, wlene Eon diee le an Eiredan bahojn winvien\n",
      "doen\n",
      "\n",
      "aalvein zt aar vijg soVzieag veeteden valen hihwe hoewah mi cen wfeleeeloen\n",
      "jiegeden gegerluen nit en bo \n",
      "----\n",
      "iter 800, loss: 93.229385\n",
      "----\n",
      " hji ten jéden voet daan\n",
      "tee eden wij lionjg er enj ahonb Yenwctn van, men denlande ban zin nten vzn de nie ven den \n",
      "erbeltte kcheenüante 't me lean oetdicz, doe\n",
      "gen zak g\n",
      "wan kir sun hat k, dan dcfskd \n",
      "----\n",
      "iter 900, loss: 90.643707\n",
      "----\n",
      "  maaandmendijde\n",
      "zoaan noit\n",
      "ein heegt vreri gone dak boaast kart in Zaag it ligk taettdjjenvefd ?'j deevedgenn hboor zeekr., maal'in aod  din\n",
      "aarin nie s ben 'n  GMak meres maaene hesdetbaendetenide kt \n",
      "----\n",
      "iter 1000, loss: 87.964440\n",
      "----\n",
      " we \n",
      "zelvr5e beltv ols vert  mon.e tel,.et de ren aer\n",
      "\n",
      "lien, er 'l jf ded tte.gef\n",
      "'n mijs koteen 'et s dledt\n",
      "stnijedjdrekie dl\n",
      " oofr 'n l,chk baed hrijH maat m\n",
      "deren nwoon  er mrde goer denevacttenve\n",
      "' \n",
      "----\n",
      "iter 1100, loss: 85.665949\n",
      "----\n",
      " \n",
      "Hit' naa- wan,eten def jorjeust.s zovan en inel win en tenhjjelven Wejn it.gt. Is\n",
      "b?enzir var bavrag des\n",
      "ebne man een d fje\n",
      "sd nNoog Maabze z Hoalde zard zijt gon kad.tto, waalag dis Er.i zie kendecc \n",
      "----\n",
      "iter 1200, loss: 83.369388\n",
      "----\n",
      " mdio h\" Mij.\n",
      "d'j kad. jen Doi tjen vijbe htofg oster et gaormtat vier \"vaoltdit dualij hoervanbe, vaarw daErpije p, dijwchijh werdichj, d'de 'f ' zi haar j lwau zlin de, varen ze tad k zan di wo5toach \n",
      "----\n",
      "iter 1300, loss: 81.351233\n",
      "----\n",
      " iuojiesr v'n ik val k,\n",
      "mau zaag hha wen oogrenu Amant an wal ien Deechj dar een\n",
      "p.\n",
      "dene er fijctit waad zpieg srieg heraai be 'n hit doer on in en vee zertar ad lhteul er. en hen  derielperge vin ze u \n",
      "----\n",
      "iter 1400, loss: 79.463188\n",
      "----\n",
      " te,rwet de n uis oom konted aar kijjeren oalt wan. geen v hijs\"en, hieen dat rerle\n",
      "hoet en han muchle, namr\n",
      "er, Indjj sch7 eude\n",
      "den zaien zai. or dabn vaart zonen nsms kuihtek \n",
      "yaat kroet,dew ziktak z \n",
      "----\n",
      "iter 1500, loss: 77.457084\n",
      "----\n",
      " iten a]digerdeig haaver verdet DaCldjahel zag\n",
      "kn\n",
      "rijt vapren \n",
      "'nk zien\n",
      "\"uidden\n",
      "diud in Elde valkde vint\n",
      "z'n mes 'pkin terwamf pen \n",
      "En ven Cit\n",
      "ien, vp\n",
      "hoo haare 'l\n",
      "s.e dar aarr, vaaf wan Z'M \n",
      "watk oorg \n",
      "----\n",
      "iter 1600, loss: 75.672978\n",
      "----\n",
      " n de tan heen 'te heelmor koerin gilt \n",
      "onlaar wdieven hoor sarte zoar zen ven me wen en, dan oerDeafdogtenks m'mien ij kaen de. En zaik meow\n",
      "teenisgeriegchivanried zamen varlinctchp dak\n",
      "iemoente\n",
      "wobse \n",
      "----\n",
      "iter 1700, loss: 74.261266\n",
      "----\n",
      " ien ze le. doun nier er 1in wpet moon zoel.en lapvitltjter tecn.\n",
      "n, bin\n",
      "er sgtae ktsjer\n",
      "on 't\n",
      "'r en dical oldtiek vergen har eaD velst oolden al da, kaarn bldje brerle em ie\n",
      "gs, vook gie rier zvnar ad \n",
      "----\n",
      "iter 1800, loss: 73.007932\n",
      "----\n",
      " lamren daar\n",
      ": vichfop eanboloAk,\n",
      "doezeen, eree'tgeedkan tven \n",
      "zamied woelitgdai hof\n",
      "Zene zer ar oe s Ne er aar\n",
      "lroat wijoe rafg er wan, sen el jog\n",
      "oeben in \"'bn aachenten en waarkomtamte. henteefie ke \n",
      "----\n",
      "iter 1900, loss: 71.508644\n",
      "----\n",
      " be ongan de zicht ze dewapen keuzoez berchomende\n",
      "Ze s ne en oketsen uigen kulorit er zejs 'vk Cjon naat haHven en bat, wat oan\n",
      "Nem mee, enen en vaor\n",
      "opt, op gen kof\n",
      "zaer\" woere ban, vognorwen wat st2f \n",
      "----\n",
      "iter 2000, loss: 70.544268\n",
      "----\n",
      " aj womoEen\n",
      "me En d#e erd en wasscZtet or endgedeesenegseen op\n",
      " voitd, de s.eni zen \"kechtep...taat in de d ie en eveuf vat ij ont nZs has\n",
      "\":en ges ondurOn\n",
      "\n",
      "zeichhecht eu Mij en éom doe asthat eende ve \n",
      "----\n",
      "iter 2100, loss: 69.728592\n",
      "----\n",
      " a,. hwofden erde. ldeg noon\n",
      "dil-endet Padge moet teederhae\n",
      "kraar.\n",
      ", zolin hhjat hionzijé monYijt,ig,\n",
      "\n",
      "duilerde las zmee Hen kve, nawrarijl Vm eeelde dank hachtelda koochche zoon ding cht,  haaIhaaen n \n",
      "----\n",
      "iter 2200, loss: 68.502275\n",
      "----\n",
      " om. \n",
      "kri ken enge nf kat oprals uar ne\n",
      "vein c.T\n",
      "zaket\n",
      "lovie treei de Enlmiken venterken hant\n",
      "etder dien endun  zes hergerr en zijrschamen diet zar naf Z'v, ziend wat ben in ond in d hoolig lne oge raa \n",
      "----\n",
      "iter 2300, loss: 67.567247\n",
      "----\n",
      " en gen\n",
      "\n",
      "erEr kidtet egje amef, wet ef di\n",
      "fren. haat h dar pas zoin.\n",
      "voNseren aa isl elre \"en daat de 't den\n",
      "winst oork onwuvelen pekunauDre dlden hsane hij loendi lan zins\n",
      "zes rliel gute daj Zoordi ge \n",
      "----\n",
      "iter 2400, loss: 66.644362\n",
      "----\n",
      " Man vaardinchan wijlaare Hlse\n",
      "ichate he waar eremet enten algge\n",
      "di, piester en hat Gorderdtaignit nam en en\n",
      "'tielich monen in ak, vickt an\n",
      "En woossten wston de dijan, aem iemjan ent.ste bertan en n  m \n",
      "----\n",
      "iter 2500, loss: 65.784931\n",
      "----\n",
      "  en\n",
      "aker heer wende ne daan beeriteeb \n",
      "eOn hiehgbier zoeden,.\"\n",
      "\n",
      "ess, gBe hiar isk oon ban ountunder.\n",
      "\"\n",
      "'t 't deenderde.. ween zikuwaar je  warentenker. zij aar\"nij eels rood wal van ov de vevevelij ke \n",
      "----\n",
      "iter 2600, loss: 64.894857\n",
      "----\n",
      "  Man be dattoldatroor aand ne Eg om latg, tat goopslar waar wet dad doop dalabmuden zi. letalon em de \"d\n",
      "en we ven Hin\n",
      "'tgieplde des Vgu dev\n",
      "kmen merdoor zaans diesantoo haarkeit tant dad dalgen wals\n",
      " \n",
      "----\n",
      "iter 2700, loss: 64.014053\n",
      "----\n",
      " pders ve de de inovens wti hestaangemarnij hade delen deit. hachtrem me de voelre zo\n",
      "erde te mgege de woetnait hee megelinoooge\n",
      "wdogen\n",
      "ies en, zon dud an lan orichksde er.teg, l eatrien. haavden op he \n",
      "----\n",
      "iter 2800, loss: 63.167637\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-334115629111>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-334115629111>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# encode in 1-of-k representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m \u001b[0;31m# unnormalized log probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# data I/O\n",
    "data = open('nescio.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
