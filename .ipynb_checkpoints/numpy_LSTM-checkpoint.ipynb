{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = open('shakespeare.txt', 'r').read()\n",
    "data = open('nescio.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size = len(data)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set is length 202367\n",
      "Vocab set is length 89\n"
     ]
    }
   ],
   "source": [
    "print(f'Data set is length {data_size}')\n",
    "print(f'Vocab set is length {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple character embedding\n",
    "char_to_idx = {char:i for i, char in enumerate(chars)}\n",
    "idx_to_char = {i:char for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 # I think its times 0.01 to avoid exploding gradients\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "\n",
    "# bias\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should happen:\n",
    "\n",
    "x = data[0]\n",
    "y = data[1]\n",
    "\n",
    "h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h))\n",
    "\n",
    "y_pred = np.dot(Why, h)\n",
    "\n",
    "loss = y - y_pred (Simplified, we will use Cross-entropy loss for it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(xs, targets, hidden, Wxh, Whh, Why, bh, by):\n",
    "    \"\"\"Calculate the forward pass\n",
    "    Calculate the cross-entropy loss, \n",
    "    which is based on the softmax functon and the negative log likelyhood.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_preds = {}\n",
    "    hs = {}\n",
    "    softmax_probs = {}\n",
    "        \n",
    "    hs[-1] = np.copy(hidden)\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(xs)):\n",
    "        x = xs[i]\n",
    "        x_vec = np.zeros((vocab_size, 1)) # vectorize the input\n",
    "        x_vec[x] = 1\n",
    "\n",
    "        # Calculate the new hidden, which is based on the input and the previous hidden layer\n",
    "        hs[i] = np.tanh(np.dot(Wxh, x_vec) + np.dot(Whh, hs[i - 1]) + bh)\n",
    "        # Predict y\n",
    "        y_preds[i] = np.dot(Why, hs[i]) + by\n",
    "        \n",
    "        softmax_probs[i] = np.exp(y_preds[i]) / np.sum(np.exp(y_preds[i])) # Softmax probabilty\n",
    "        loss += -np.log(softmax_probs[i][targets[i], 0]) #Negative loss likelyhood\n",
    "    \n",
    "    prev_hidden = hs[len(xs) - 1]\n",
    "\n",
    "    return hs, softmax_probs, loss, prev_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 5 required positional arguments: 'Wxh', 'Whh', 'Why', 'bh', and 'by'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-41d5146eacd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m126\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Karpathy code to test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 5 required positional arguments: 'Wxh', 'Whh', 'Why', 'bh', and 'by'"
     ]
    }
   ],
   "source": [
    "# Test for the forward pass\n",
    "xs = [char_to_idx[ch] for ch in data[100:125]]\n",
    "prev_hidden = np.zeros((hidden_size, 1))\n",
    "targets = [char_to_idx[ch] for ch in data[101:126]]\n",
    "\n",
    "hs, softmax_probs, loss, prev_hidden = forward(xs, targets, prev_hidden)\n",
    "\n",
    "# Karpathy code to test\n",
    "test_hs = {}\n",
    "test_ys = {}\n",
    "test_xs = {}\n",
    "test_loss = 0\n",
    "test_ps = {}\n",
    "test_hs[-1] = np.copy(prev_hidden)\n",
    "\n",
    "for t in range(len(xs)):\n",
    "    test_xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    test_xs[t][xs[t]] = 1\n",
    "    test_hs[t] = np.tanh(np.dot(Wxh, test_xs[t]) + np.dot(Whh, test_hs[t-1]) + bh) # hidden state\n",
    "    test_ys[t] = np.dot(Why, test_hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    test_ps[t] = np.exp(test_ys[t]) / np.sum(np.exp(test_ys[t])) # probabilities for next chars\n",
    "    test_loss += -np.log(test_ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "assert loss - test_loss < 0.01 or test_loss - loss > 0.01 #Klein verschil in loss kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_preds, target): \n",
    "    \"\"\"Calculate the cross-entropy loss, \n",
    "    which is based on the softmax functon and the negative log likelyhood.\"\"\"\n",
    "    \n",
    "    softmax_probs = {}\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(y_preds)):\n",
    "        softmax_probs[i] = np.exp(y_preds[i]) / np.sum(np.exp(y_preds[i])) # Softmax probabilty\n",
    "\n",
    "        loss += -np.log(softmax_probs[i][target[i], 0]) #Negative loss likelyhood\n",
    "    \n",
    "    return softmax_probs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-8c33e1059bf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Test for loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msoftmax_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Karpathy code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_preds' is not defined"
     ]
    }
   ],
   "source": [
    "#Test for loss function\n",
    "target = [char_to_idx[ch] for ch in data[101]]\n",
    "softmax_probs, loss = loss_function(y_preds, target)\n",
    "\n",
    "# Karpathy code\n",
    "test_loss = 0\n",
    "test_ps = {}\n",
    "test_ps[0] = np.exp(test_ys[0]) / np.sum(np.exp(test_ys[0])) # probabilities for next chars\n",
    "test_loss += -np.log(test_ps[0][target[0],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "assert softmax_probs[0][0] == test_ps[0][0]\n",
    "assert loss == test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_gradients():\n",
    "    \"\"\"Initialize the gradients to 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dby, dbh = np.zeros_like(by), np.zeros_like(bh)\n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dby, dbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(softmax_probs, hs, xs, targets):\n",
    "    \"\"\"Perform the backward pass\"\"\"\n",
    "    \n",
    "    dWxh, dWhh, dWhy, dby, dbh = initialize_gradients()\n",
    "    \n",
    "    # Initialize empty next hidden layer for the first backprop\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    \n",
    "    for i in reversed(range(len(xs))):\n",
    "        # X to vector\n",
    "        x = xs[i]    \n",
    "        x_vec = np.zeros((vocab_size, 1))\n",
    "        x_vec[x] = 1\n",
    "\n",
    "        dy = np.copy(softmax_probs[i])\n",
    "        dy[targets[i]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "\n",
    "        dby += dy   \n",
    "        dWhy += np.dot(dy, hs[i].T)\n",
    "        dh = np.dot(Why.T, dy) + dhnext\n",
    "        dhraw = (1 - hs[i] * hs[i]) * dh  \n",
    "        dWxh += np.dot(dhraw, x_vec.T)\n",
    "        dWhh += np.dot(dhraw, hs[i-1].T)\n",
    "        dbh += dhraw\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "    # Clip to prevent exploding gradients\n",
    "    for dparam in [dWhy, dWxh, dWhh, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dby, dbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for gradient initialization\n",
    "dWxh, dWhh, dWhy, dby, dbh = initialize_gradients()\n",
    "\n",
    "\n",
    "# Karpathys code\n",
    "test_dWxh, test_dWhh, test_dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "test_dbh, test_dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "\n",
    "assert dWhy.shape == test_dWhy.shape\n",
    "assert dbh.shape == test_dbh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'softmax_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-f45e03b34414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Test backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Karpathy code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'softmax_probs' is not defined"
     ]
    }
   ],
   "source": [
    "#Test backward\n",
    "x = [char_to_idx[ch] for ch in data[100]]\n",
    "dWxh, dWhh, dWhy, dby, dbh = backward(softmax_probs, hs, x, target)\n",
    "\n",
    "#Karpathy code\n",
    "test_dhnext = np.zeros_like(hs[0])\n",
    "test_dy = np.copy(test_ps[0])\n",
    "test_dy[target[0]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "test_dWhy += np.dot(test_dy, hs[0].T)\n",
    "test_dby += test_dy\n",
    "test_dh = np.dot(Why.T, test_dy) + test_dhnext # backprop into h\n",
    "test_dhraw = (1 - hs[0] * hs[0]) * test_dh # backprop through tanh nonlinearity\n",
    "test_dbh += test_dhraw\n",
    "\n",
    "x_vec = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "x_vec[x] = 1\n",
    "\n",
    "test_dWxh += np.dot(test_dhraw, x_vec.T)\n",
    "test_dWhh += np.dot(test_dhraw, hs[-1].T)\n",
    "test_dhnext = np.dot(Whh.T, test_dhraw)\n",
    "\n",
    "for dparam in [test_dWxh, test_dWhh, test_dWhy, test_dbh, test_dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    \n",
    "print(test_dWhy[0][0])\n",
    "print(dWhy[0][0])\n",
    "\n",
    "assert test_dWhy[0][0] == dWhy[0][0]\n",
    "assert test_dby[0][0] == dby[0][0]\n",
    "assert test_dWxh[0][0] == dWxh[0][0]\n",
    "assert test_dWhh[0][0] == dWhh[0][0]\n",
    "assert test_dhnext[0][0] == dhnext[0][0]\n",
    "assert test_dbh[0][0] == dbh[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_gradients(Wxh, Whh, Why, by, bh, dWxh, dWhh, dWhy, dby, dbh, learning_rate):\n",
    "    \"\"\"Update the gradients using stochastic gradient descent.\"\"\"\n",
    "    Wxh -= learning_rate * dWxh\n",
    "    Whh -= learning_rate * dWhh\n",
    "    Why -= learning_rate * dWhy\n",
    "    bh -= learning_rate * dbh\n",
    "    by -= learning_rate * dby\n",
    "    \n",
    "    return Wxh, Whh, Why, bh, by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad(Wxh, Whh, Why, by, bh, dWxh, dWhh, dWhy, dby, dbh, mWxh, mWhh, mWhy, mbh, mby, learning_rate):\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update\n",
    "        \n",
    "    return Wxh, Whh, Why, by, bh, mWxh, mWhh, mWhy, mbh, mby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n, Wxh, Whh, Why, bh, by):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters from the model\n",
    "    h is the memory state, seed_ix is seed letter for the first time step\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(seed_ix, int):\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[seed_ix] = 1\n",
    "        ixes = []\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "            y = np.dot(Why, h) + by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "            x = np.zeros((vocab_size, 1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "    else:\n",
    "        print(seed_ix)\n",
    "        xs = [char_to_idx(ch) for ch in seed_ix]\n",
    "        ixes = list(xs[:-1])\n",
    "        \n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[xs[-1]] = 1\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "            y = np.dot(Why, h) + by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "            x = np.zeros((vocab_size, 1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        \n",
    "        \n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(data, seq_length, epochs):\n",
    "    \"\"\"Perform RNN over the data\"\"\"\n",
    "    data_len = len(data)\n",
    "    \n",
    "    # Initialize weights\n",
    "    Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "    Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "    Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "    \n",
    "    # bias\n",
    "    bh = np.zeros((hidden_size, 1))\n",
    "    by = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    weights = [Wxh, Whh, Why, bh, by]\n",
    "    \n",
    "    # Store losses\n",
    "    losses = []\n",
    "    smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "    \n",
    "    # Memory voor Adagrad\n",
    "    mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    mbh, mby  = np.zeros_like(bh), np.zeros_like(by)\n",
    "    \n",
    "    # Loop over the epochs\n",
    "    for i in range(epochs):\n",
    "        n = 0\n",
    "        # Loop over the amount of sequences\n",
    "        sequences_amount = int(data_len // seq_length)\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        for j in range(sequences_amount):\n",
    "            \n",
    "            start_pos = seq_length * j\n",
    "            # Reset and go from the start of data\n",
    "            if n == 0 or start_pos + seq_length + 1 >= data_len:\n",
    "                prev_hidden = np.zeros((hidden_size, 1))\n",
    "             \n",
    "            # Embed the inputs and targets\n",
    "            xs = [char_to_idx[ch] for ch in data[start_pos:start_pos+seq_length]]\n",
    "            targets = [char_to_idx[ch] for ch in data[start_pos+1:start_pos+seq_length+1]]\n",
    "\n",
    "            # Forward pass\n",
    "            hs, softmax_probs, loss, prev_hidden = forward(xs, targets, prev_hidden, *weights)\n",
    "    \n",
    "            #Backward\n",
    "            dWxh, dWhh, dWhy, dby, dbh = backward(softmax_probs, hs, xs, targets)\n",
    "\n",
    "            # Update gradients with adagrad\n",
    "            Wxh, Whh, Why, by, bh, mWxh, mWhh, mWhy, mbh, mby = adagrad(Wxh, Whh, Why, by, bh, dWxh, dWhh, dWhy, dby, dbh, mWxh, mWhh, mWhy, mbh, mby, learning_rate)\n",
    "\n",
    "            # Update gradients with gradient descent\n",
    "            #Wxh, Whh, Why, bh, by = update_gradients(Wxh, Whh, Why, by, bh, dWxh, dWhh, dWhy, dby, dbh, learning_rate)\n",
    "            \n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "            \n",
    "            if n % 100 == 0:\n",
    "                losses.append(smooth_loss)\n",
    "                print(f'{i + 1}: {n} / {sequences_amount}: {smooth_loss}')\n",
    "                \n",
    "                \n",
    "                # Print a sample\n",
    "                sample_ix = sample(prev_hidden, xs[0], 200, *weights)\n",
    "                txt = ''.join(idx_to_char[ix] for ix in sample_ix)\n",
    "                print('----- \\n' + txt + '  \\n------')\n",
    "\n",
    "            n += 1\n",
    "            \n",
    "        print(f'Finished epoch {i + 1}.')\n",
    "        \n",
    "        \n",
    "    # Print gefeliciteerd bericht\n",
    "    gefeliciteerd = sample(prev_hidden, 'Gefeliciteerd', 50, *weights)\n",
    "    txt = ''.join(idx_to_char[ix] for ix in sample_ix)\n",
    "    print('----- \\n' + txt + '  \\n------')\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'bh' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-9333f83b83bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-ab35ef42e76b>\u001b[0m in \u001b[0;36mRNN\u001b[0;34m(data, seq_length, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mWhy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'bh' referenced before assignment"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "losses = RNN(data, seq_length, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1f71c63ba2d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(xs, targets, hidden):\n",
    "    y_preds = {}\n",
    "    hs = {}\n",
    "    softmax_probs = {}\n",
    "        \n",
    "    hs[-1] = np.copy(hidden)\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(xs)):\n",
    "        x = xs[i]\n",
    "        x_vec = np.zeros((vocab_size, 1)) # vectorize the input\n",
    "        x_vec[x] = 1\n",
    "\n",
    "        # Calculate the new hidden, which is based on the input and the previous hidden layer\n",
    "        hs[i] = np.tanh(np.dot(Wxh, x_vec) + np.dot(Whh, hs[i - 1]) + bh)\n",
    "        # Predict y\n",
    "        y_preds[i] = np.dot(Why, hs[i]) + by\n",
    "        \n",
    "        softmax_probs[i] = np.exp(y_preds[i]) / np.sum(np.exp(y_preds[i])) # Softmax probabilty\n",
    "        loss += -np.log(softmax_probs[i][targets[i], 0]) #Negative loss likelyhood\n",
    "    \n",
    "    prev_hidden = hs[len(xs) - 1]\n",
    "    \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dby, dbh = np.zeros_like(by), np.zeros_like(bh)\n",
    "    \n",
    "    # Initialize empty next hidden layer for the first backprop\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    \n",
    "    for i in reversed(range(len(xs))):\n",
    "        # X to vector\n",
    "        x = xs[i]    \n",
    "        x_vec = np.zeros((vocab_size, 1))\n",
    "        x_vec[x] = 1\n",
    "\n",
    "        dy = np.copy(softmax_probs[i])\n",
    "        dy[targets[i]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "\n",
    "        dby += dy   \n",
    "        dWhy += np.dot(dy, hs[i].T)\n",
    "        dh = np.dot(Why.T, dy) + dhnext\n",
    "        dhraw = (1 - hs[i] * hs[i]) * dh  \n",
    "        dWxh += np.dot(dhraw, x_vec.T)\n",
    "        dWhh += np.dot(dhraw, hs[i-1].T)\n",
    "        dbh += dhraw\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "    # Clip to prevent exploding gradients\n",
    "    for dparam in [dWhy, dWxh, dWhh, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "        \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, prev_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun_v2(xs, targets, hidden):\n",
    "    # Forward pass\n",
    "    hs, softmax_probs, loss, prev_hidden = forward(xs, targets, hidden)\n",
    "    \n",
    "    #Backward\n",
    "    dWxh, dWhh, dWhy, dby, dbh = backward(softmax_probs, hs, xs, targets)\n",
    "\n",
    "    # Update gradients with adagrad\n",
    "    uWxh, uWhh, uWhy, uby, ubh, umWxh, umWhh, umWhy, umbh, umby = adagrad(Wxh, Whh, Why, by, bh, dWxh, dWhh, dWhy, dby, dbh, mWxh, mWhh, mWhy, mbh, mby, learning_rate)\n",
    "    \n",
    "    return loss, uWxh, uWhh, uWhy, uby, ubh, umWxh, umWhh, umWhy, umbh, umby, prev_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " PrttsstoootGooooooss'sDsoootDot ooooooosstuosssboob GoosssotoooooDooosDootoooooooooooootososooGoooooosogstooGgoooosostoooostotobssgs'osotoostoossssbsGootoootssGoooss'touoooooooooGotosoooooG'tsssGstoss \n",
      "----\n",
      "iter 0, loss: 114.925547\n",
      "----\n",
      "  jareen zot, zlonk ot zrzlendzozbnt ztaat dtdoar zaan zap zmt kk ze kl tonek nzzd zt z'dHaande dhak zunt zin zok an elll z tun voedzzer 'm di kt zor zf hatTk zk 'nt zrat zaa  mogzkEk zal, zt zut zzbn  \n",
      "----\n",
      "iter 100, loss: 115.111534\n",
      "----\n",
      " te nekechtijrendst 'manden wak deeeron denhaeltek\n",
      "\"viddet dr lsTPoddid reskht\n",
      "tkbiesginen Ar.s dd, 'e dachdesvd.ens\n",
      "Zdchovssgd wen taar dn pechralkecd sd\n",
      "sitdendens.\n",
      "tok\n",
      "'ird. eng geind. den dlsst or  \n",
      "----\n",
      "iter 200, loss: 110.904239\n",
      "----\n",
      " jez\n",
      "matt de illen bie torin dirt  pt do ded sensorort e e hegeg venog teen niden der dat.in han me td dlg dord lidt g.\n",
      "da dn 'auen de domiwel. breer det Nooe oelter wenwoodden dee, irigtg. hijd de ber \n",
      "----\n",
      "iter 300, loss: 106.716997\n",
      "----\n",
      "  wen De dinf. zod,\n",
      "te'lli\n",
      "jaaen watdezen te be\"sett in koouteloen, tinief Aves 'lfer ge lrge eOdk ijdeel Dn lechtg.iddel dis éi tid ws  hajt Eeer huer\n",
      "\n",
      " dien je me ze tolhs wegen heer zokde det loin b \n",
      "----\n",
      "iter 400, loss: 102.746470\n",
      "----\n",
      "  hagijé wattd\n",
      "Egri.matsti, dij daar dij, Min zoerder sw\" drij aasmenderdaptfen wn.\n",
      "gen derdt, zesdmerden lim irap vu ede epMos kaardelerijuvis wam den boien et dap paar Aaar\n",
      "Maar\n",
      "Zig jm ke rat. Moees. \n",
      "----\n",
      "iter 500, loss: 99.132793\n",
      "----\n",
      "  etsCtii tacijaar nk e, tu lat\n",
      "jn in dood a s 'nuv ein ooen s\n",
      "\n",
      " is den haoe densss \n",
      " En oen ar,\n",
      "zoen oen ooen Lleofs vu bn\n",
      "en veen dn var ke daer en dor iaar man\n",
      "en orof vog scak kod gi i,\n",
      "denten ooen \n",
      "----\n",
      "iter 600, loss: 95.660731\n",
      "----\n",
      " domdg doarhaaf laed vik 't t. wezichdediez'linkek oourijd man zed dijet.fat hag tov zoion dz'mdie, lin mander. Eelee dim mie moe ien vae haeive demgeg zoreneenop. dui, zorenigmee den\n",
      "\"inbelôar1rderdrd \n",
      "----\n",
      "iter 700, loss: 92.257455\n",
      "----\n",
      " n han soened gaar en, Elan. Eoastee elaar g va en ouhe hos zugfee zee ei orej zelen . En. Erien. En ma. haand lrlen de halg. ze mareeel aenwas zou, ran zaaogde nar, haar gis eet zas magk wan her\n",
      "kezen \n",
      "----\n",
      "iter 800, loss: 89.067516\n",
      "----\n",
      " voirimin bor man gen vaar\n",
      "en en en.ejn zekebeen der nopme ben eems ze bg ee oool. in iar benden. canrder.bhaar ok daar Hog denukeelvaar Latdtdet ve 't\n",
      "bork en \n",
      "mdenin bomensfkde net en zde vor kenten  \n",
      "----\n",
      "iter 900, loss: 86.398585\n",
      "----\n",
      "  han biaan moes\n",
      "isken\n",
      "havaaenen detensjen haar inteluslaal dtaal mijet kijon k't dent\n",
      "jins Jan tende 't vauszheei ie hamd zout hee z'ziet. \" in buenbit dgaar haarjent hatdijrendbet'en 'n vaarden baar  \n",
      "----\n",
      "iter 1000, loss: 83.672857\n",
      "----\n",
      "  'td,\n",
      "dicht, elddenden en be d, oeld, dichte heg jiet ken\n",
      "dich eek denmecht det ten, 't dogreracherer ood voen igt doerdeld. dee 'lreroir, bun, we hupte, laarerdrgen zeé lte raar et ligdd ectdd et ttj \n",
      "----\n",
      "iter 1100, loss: 81.411818\n",
      "----\n",
      " rderwi nan ze zogTup s  'n mene zechd zasjenkem daor. Hicatjatgesg,\n",
      "oinjar momeichg vs zitdjjinooichaarden\n",
      "aanmu\n",
      "hijjeen zie zonig haugep van woooogue scogbejeeeniias, veand en , lachtijekar\n",
      "hijdeevin \n",
      "----\n",
      "iter 1200, loss: 79.228945\n",
      "----\n",
      "  var mate,\n",
      "doddomge, wattder mat \"Ian zakt oam hortkeelwe kortan.-od, rar dijre,\n",
      "ooe git zaomeltk rogets wot wad be kas rachtk\n",
      "che fod. \"ten kerten gdecht,\n",
      "wacee zaadmoar fer guit pal zat keed wal ies \n",
      "----\n",
      "iter 1300, loss: 77.306053\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-a13f00129be1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-15b8a78165fb>\u001b[0m in \u001b[0;36mlossFun_v2\u001b[0;34m(xs, targets, hidden)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Update gradients with adagrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-f7760ffcf2d0>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(softmax_probs, hs, xs, targets)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdby\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdWhy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdhnext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mdhraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mdWxh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_idx[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_idx[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(idx_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, Wxh, Whh, Why, by, bh, mWxh, mWhh, mWhy, mbh, mby, prev_hidden = lossFun_v2(inputs, targets, hprev)\n",
    "\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
