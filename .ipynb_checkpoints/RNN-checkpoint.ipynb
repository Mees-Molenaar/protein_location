{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN in Numpy\n",
    "\n",
    "Recurrent Neural Networks (RNN) are a class of Artificial Neural Networks that is extremely well equiped to process a sequence of inputs. Therefore, RNNs are usefull in time series. \n",
    "\n",
    "In this work, I created a RNN from scratch using Numpy. This RNN is based on Andrej Karpathy's char-rnn and will be the basis for a LSTM network. This network will be used to classify protein locations based on the amino acid sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import numpy and matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the training data and save some important variables. If you want to train on your own text, just change the .txt file in the data variable.\n",
    "#data = open('shakespeare.txt', 'r').read()\n",
    "data = open('nescio.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size = len(data)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set is length 202367\n",
      "Vocab set is length 89\n"
     ]
    }
   ],
   "source": [
    "print(f'Data set is length {data_size}')\n",
    "print(f'Vocab set is length {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple character embedding\n",
    "char_to_idx = {char:i for i, char in enumerate(chars)}\n",
    "idx_to_char = {i:char for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    r\"\"\" Simple recurrent neural network (RNN) class for an input sequence.\n",
    "    \n",
    "        This RNN initializes weight and gradients. And contains the forward\n",
    "        and backward pass. The network is optimized using Adagrad.\n",
    "        The train method is used to train the network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        seq_length : Number of layers connected to each others. \n",
    "        hidden_sz : The number of features in the hidden state h.\n",
    "        vocab_sz : The number of possible inputs and outputs.\n",
    "        \n",
    "        \n",
    "        Inputs (train)\n",
    "        --------------\n",
    "        data : Data used to train the network.\n",
    "        optimizer : The optimizer that is used to train the network.\n",
    "        lr : The learning rate used to train the network.\n",
    "        epochs : The number of epochs to train the network.\n",
    "        progress : If True, shows the progress of training the network.\n",
    "        \n",
    "        Inputs (predict)\n",
    "        ----------------\n",
    "        start : Start of a sentence that the network uses as initial sequence.\n",
    "        n : Length of the prediction.\n",
    "        \n",
    "        \n",
    "        Output (train)\n",
    "        --------------\n",
    "        smooth_loss : The loss of the current trained network.\n",
    "        Wxh, Whh, Why : Updated weights of the network due to training.\n",
    "        bh, by : Updated biases due to training.\n",
    "        \n",
    "        Output (predict)\n",
    "        ----------------\n",
    "        txt : A string that is predicted by the RNN.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seq_length, hidden_sz, vocab_sz):\n",
    "        self.hs = {} # Hidden states\n",
    "        self.sm_ps = {} # Softmax probabilities\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.vocab_sz = vocab_sz\n",
    "        \n",
    "        # Start with zero loss\n",
    "        self.loss = 0 \n",
    "        \n",
    "        # Initiate weight matrices\n",
    "        self.Wxh, self.Whh, self.Why, self.bh, self.by = self.init_weights()\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes weights and biases based on the inputs hidden_sz and vocab_sz\n",
    "        \"\"\"\n",
    "        Wxh = np.random.randn(self.hidden_sz, self.vocab_sz) * 0.01 #times 0.01 to avoid exploding gradients\n",
    "        Whh = np.random.randn(self.hidden_sz, self.hidden_sz) * 0.01\n",
    "        Why = np.random.randn(self.vocab_sz, self.hidden_sz) * 0.01\n",
    "        \n",
    "        # bias\n",
    "        bh = np.zeros((hidden_size, 1))\n",
    "        by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        return Wxh, Whh, Why, bh, by\n",
    "    \n",
    "    def init_gradients(self):\n",
    "        \"\"\"\n",
    "        Initializes gradients for biases and weights.\n",
    "        \"\"\"\n",
    "        self.dWxh, self.dWhh, self.dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        self.dby, self.dbh = np.zeros_like(self.by), np.zeros_like(self.bh)\n",
    "    \n",
    "    def forward(self, xs, targets):\n",
    "        \"\"\"\n",
    "        Forward pass of the RNN\n",
    "        \"\"\"\n",
    "        \n",
    "        y_preds = {}\n",
    "\n",
    "        self.loss = 0\n",
    "\n",
    "        for i in range(len(xs)):\n",
    "            x = xs[i]\n",
    "            x_vec = np.zeros((self.vocab_sz, 1)) # vectorize the input\n",
    "            x_vec[x] = 1\n",
    "\n",
    "            # Calculate the new hidden, which is based on the input and the previous hidden layer\n",
    "            self.hs[i] = np.tanh(np.dot(self.Wxh, x_vec) + np.dot(self.Whh, self.hs[i - 1]) + self.bh)\n",
    "            # Predict y\n",
    "            y_preds[i] = np.dot(self.Why, self.hs[i]) + self.by\n",
    "\n",
    "            self.sm_ps[i] = np.exp(y_preds[i]) / np.sum(np.exp(y_preds[i])) # Softmax probabilty\n",
    "            self.loss += -np.log(self.sm_ps[i][targets[i], 0]) #Negative loss likelyhood\n",
    "\n",
    "        self.hs[-1] = self.hs[len(xs) - 1]\n",
    "        \n",
    "    def backward(self, xs, targets):\n",
    "        \"\"\"\n",
    "        Backward pass of the RNN\n",
    "        \"\"\"\n",
    "        self.init_gradients()\n",
    "    \n",
    "        # Initialize empty next hidden layer for the first backprop\n",
    "        dhnext = np.zeros_like(self.hs[0])\n",
    "\n",
    "        for i in reversed(range(len(xs))):\n",
    "            # X to vector\n",
    "            x = xs[i]    \n",
    "            x_vec = np.zeros((vocab_size, 1))\n",
    "            x_vec[x] = 1\n",
    "\n",
    "            dy = np.copy(self.sm_ps[i])\n",
    "            dy[targets[i]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "\n",
    "            self.dby += dy   \n",
    "            self.dWhy += np.dot(dy, self.hs[i].T)\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext\n",
    "            dhraw = (1 - self.hs[i] * self.hs[i]) * dh  \n",
    "            self.dWxh += np.dot(dhraw, x_vec.T)\n",
    "            self.dWhh += np.dot(dhraw, self.hs[i-1].T)\n",
    "            self.dbh += dhraw\n",
    "            dhnext = np.dot(self.Whh.T, dhraw)\n",
    "\n",
    "        # Clip to prevent exploding gradients\n",
    "        for dparam in [self.dWhy, self.dWxh, self.dWhh, self.dbh, self.dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "            \n",
    "    def init_adagrad_memory(self):\n",
    "        \"\"\"\n",
    "        Initialize memory matrices needed for Adagrad.\n",
    "        \"\"\"\n",
    "        self.mWxh, self.mWhh, self.mWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        self.mbh, self.mby  = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "\n",
    "    def update_gradients(self, optimizer, lr):\n",
    "        \"\"\"\n",
    "        Update gradients based on the optimizer you have chosen.\n",
    "        \"\"\"\n",
    "        if optimizer == 'Adagrad':\n",
    "            if not hasattr(self, 'mWhh'):\n",
    "                self.init_adagrad_memory()\n",
    "                \n",
    "            # perform parameter update with Adagrad\n",
    "            for param, dparam, mem in zip([self.Wxh, self.Whh, self.Why, self.bh, self.by],\n",
    "                                  [self.dWxh, self.dWhh, self.dWhy, self.dbh, self.dby],\n",
    "                                  [self.mWxh, self.mWhh, self.mWhy, self.mbh, self.mby]):\n",
    "                mem += dparam * dparam\n",
    "                param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update\n",
    "                \n",
    "    def reset_hidden(self):\n",
    "        \"\"\"\n",
    "        Reset the hidden layer\n",
    "        \"\"\"\n",
    "        self.hs[-1] = np.zeros((self.hidden_sz, 1))\n",
    "        \n",
    "    def plot_losses(self):\n",
    "        \"\"\"\n",
    "        Plot the cross entropy loss against the number of sequences\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'losses'):\n",
    "            plt.plot(self.losses)\n",
    "            plt.xlabel('Number of sequences')\n",
    "            plt.ylabel('Cross entropy loss')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('Error: No losses recorded, train the model!')\n",
    "    \n",
    "    def train(self, data, optimizer, lr, epochs, progress=True):\n",
    "        \"\"\"\n",
    "        Train the model by chopping the data in sequences followed by performing\n",
    "        the forward pass, backward pass and update the gradients.\n",
    "        \"\"\"\n",
    "        self.losses = []\n",
    "        smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "        \n",
    "        # Loop over the amount of epochs\n",
    "        for epoch in range(epochs):\n",
    "            n = 0\n",
    "            \n",
    "            # Reset hidden state\n",
    "            self.reset_hidden()\n",
    "            \n",
    "            data_len = len(data)\n",
    "            \n",
    "            # Loop over the amount of sequences\n",
    "            sequences_amount = int(data_len // self.seq_length)\n",
    "            for j in range(sequences_amount):\n",
    "                \n",
    "                start_pos = self.seq_length * j\n",
    "                \n",
    "                # Embed the inputs and targets\n",
    "                xs = [char_to_idx[ch] for ch in data[start_pos:start_pos+self.seq_length]]\n",
    "                targets = [char_to_idx[ch] for ch in data[start_pos+1:start_pos+self.seq_length+1]]\n",
    "                \n",
    "                # Forward pass\n",
    "                self.forward(xs, targets)\n",
    "                \n",
    "                # Backward\n",
    "                self.backward(xs, targets)\n",
    "                \n",
    "                # Update weight matrices\n",
    "                self.update_gradients(optimizer, lr)\n",
    "                \n",
    "                smooth_loss = smooth_loss * 0.999 + self.loss * 0.001\n",
    "        \n",
    "                if progress and n % 1000 == 0:\n",
    "                    print(f'Epoch {epoch + 1}: {n} / {sequences_amount}: {smooth_loss}')\n",
    "                 \n",
    "                n += 1\n",
    "                self.losses.append(smooth_loss)\n",
    "    \n",
    "    def predict(self, start, n):\n",
    "        \"\"\"\n",
    "        Predict a sequence of text based on a starting string.\n",
    "        \"\"\"\n",
    "        seed_idx = char_to_idx[start[-1]]\n",
    "        x = np.zeros((self.vocab_sz, 1))\n",
    "        x[seed_idx] = 1\n",
    "        \n",
    "        txt = [ch for ch in start]\n",
    "        \n",
    "        idxes = []\n",
    "        \n",
    "        h = self.hs[-1]\n",
    "        \n",
    "        for i in range(n):\n",
    "            \n",
    "            # Calculate the hidden\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            # Calculate y\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "\n",
    "            sm_p = np.exp(y) / np.sum(np.exp(y)) # Softmax probabilty\n",
    "            # Determine character based on weighted probability (is using the softmax probability)\n",
    "            idx = np.random.choice(range(self.vocab_sz), p=sm_p.ravel())\n",
    "            idxes.append(idx)\n",
    "            \n",
    "            # Save X for next iteration\n",
    "            x = np.zeros((self.vocab_sz, 1))\n",
    "            x[idx] = 1\n",
    "            \n",
    "        prediction = [idx_to_char[idx] for idx in idxes]\n",
    "        \n",
    "        txt += prediction\n",
    "        \n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(seq_length, hidden_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0 / 8094: 112.21590551173051\n",
      "Epoch 1: 1000 / 8094: 85.4800822212044\n",
      "Epoch 1: 2000 / 8094: 69.15924091057279\n",
      "Epoch 1: 3000 / 8094: 61.321604034208214\n",
      "Epoch 1: 4000 / 8094: 58.351825446648135\n",
      "Epoch 1: 5000 / 8094: 57.137323535328015\n",
      "Epoch 1: 6000 / 8094: 55.441738315764404\n",
      "Epoch 1: 7000 / 8094: 54.09266979721139\n",
      "Epoch 1: 8000 / 8094: 53.53038804333329\n"
     ]
    }
   ],
   "source": [
    "model.train(data, 'Adagrad', learning_rate, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefeliciteerd  g\n",
      "\n",
      "  I mar j de d ban e  en N \" e e at wan i  I\n",
      "\n",
      "\n",
      "\n",
      "Om  Nie ee t ro at  H Zt d t. Gch n e   H aat  u \"V\n",
      " e aag ,\n",
      " e \" \n",
      "\n",
      "\n",
      ". \n",
      " Za se? aar c Taas Paan  m hge.\"\n",
      "\n",
      " Zoe an t   Hi t  \n",
      "\n",
      "\n",
      " aal Vi H \" J  W n e\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict('Gefeliciteerd', 200)\n",
    "print(''.join(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "* Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network \tElsevier \"Physica D: Nonlinear Phenomena\" journal, Volume 404, March 2020: Special Issue on Machine Learning and Dynamical Systems (DOI: \t10.1016/j.physd.2019.132306)\n",
    "* https://www.kdnuggets.com/2020/07/rnn-deep-learning-sequential-data.html\n",
    "* https://gist.github.com/karpathy/d4dee566867f8291f086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
