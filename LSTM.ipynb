{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM in Numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from RNN import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid helper function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-1 * x))\n",
    "\n",
    "# Derivative sigmoid helper function\n",
    "def dsigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the training data and save some important variables. If you want to train on your own text, just change the .txt file in the data variable.\n",
    "#data = open('shakespeare.txt', 'r').read()\n",
    "data = open('nescio.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size = len(data)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set is length 201884\n",
      "Vocab set is length 88\n"
     ]
    }
   ],
   "source": [
    "print(f'Data set is length {data_size}')\n",
    "print(f'Vocab set is length {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple character embedding\n",
    "char_to_idx = {char:i for i, char in enumerate(chars)}\n",
    "idx_to_char = {i:char for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(RNN):\n",
    "    \n",
    "    def __init__(self, seq_length, hidden_sz, vocab_sz):\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.vocab_sz = vocab_sz\n",
    "        \n",
    "        # Start with zero loss\n",
    "        self.loss = 0\n",
    "        \n",
    "        # Initiate weight matrices\n",
    "        self.model_params = self.init_weights()\n",
    "    \n",
    "        # Variables for memory\n",
    "        self.hs = {} # hidden states\n",
    "        self.c_t = {} # cell states\n",
    "        self.sm_ps = {} # Softmax probabilities\n",
    "        self.ho = {} # hidden o\n",
    "        self.hc = {} # hidden c\n",
    "        self.hi = {} # hidden i\n",
    "        self.hf = {} # hidden f\n",
    "    \n",
    "        # Initialize hidden and cell state\n",
    "        self.reset_hidden()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes weights and biased based on the inputs hidden_sz and vocab_sz\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Whf = np.random.randn(self.hidden_sz, self.hidden_sz) * 0.01\n",
    "        self.Wxf = np.random.randn(self.hidden_sz, self.vocab_sz) * 0.01\n",
    "        \n",
    "        \n",
    "        self.Whi = np.random.randn(self.hidden_sz, self.hidden_sz) * 0.01\n",
    "        self.Wxi = np.random.randn(self.hidden_sz, self.vocab_sz) * 0.01\n",
    "        \n",
    "        self.Whc = np.random.randn(self.hidden_sz, self.hidden_sz) * 0.01\n",
    "        self.Wxc = np.random.randn(self.hidden_sz, self.vocab_sz) * 0.01\n",
    "        \n",
    "        self.Who = np.random.randn(self.hidden_sz, self.hidden_sz) * 0.01\n",
    "        self.Wxo = np.random.randn(self.hidden_sz, self.vocab_sz) * 0.01\n",
    "        \n",
    "        self.Why = np.random.randn(self.vocab_sz, self.hidden_sz) * 0.01\n",
    "        \n",
    "        self.Bf = np.zeros((self.hidden_sz, 1))\n",
    "        self.Bi = np.zeros((self.hidden_sz, 1))\n",
    "        self.Bc = np.zeros((self.hidden_sz, 1))\n",
    "        self.Bo = np.zeros((self.hidden_sz, 1))\n",
    "        self.By = np.zeros((self.vocab_sz, 1))\n",
    "        \n",
    "        return (self.Whf, self.Wxf, self.Whi, self.Wxi, self.Whc, self.Wxc, \n",
    "                self.Who, self.Wxo, self.Why, self.Bf, self.Bi, self.Bc, self.Bo, self.By)\n",
    "    \n",
    "    \n",
    "    def init_gradients(self):\n",
    "        \"\"\"\n",
    "        Initialize gradients for biases and weights\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dWhf, self.dWxf = np.zeros_like(self.Whf), np.zeros_like(self.Wxf)\n",
    "        self.dWhi, self.dWxi = np.zeros_like(self.Whi), np.zeros_like(self.Wxi)\n",
    "        self.dWhc, self.dWxc = np.zeros_like(self.Whc), np.zeros_like(self.Wxc)\n",
    "        self.dWho, self.dWxo = np.zeros_like(self.Who), np.zeros_like(self.Wxo)\n",
    "        self.dWhy = np.zeros_like(self.Why)\n",
    "        \n",
    "        self.dBf = np.zeros_like(self.Bf)\n",
    "        self.dBi = np.zeros_like(self.Bi)\n",
    "        self.dBc = np.zeros_like(self.Bc)\n",
    "        self.dBo = np.zeros_like(self.Bo)\n",
    "        self.dBy = np.zeros_like(self.By)\n",
    "        \n",
    "        return (self.dWhf, self.dWxf, self.dWhi, self.dWxi, self.dWhc, self.dWxc,\n",
    "                self.dWho, self.dWxo, self.dWhy, self.dBf, self.dBi, self.dBc, self.dBo, self.dBy)\n",
    "    \n",
    "    def init_adagrad_memory(self):\n",
    "        \"\"\"\n",
    "        Initialze memory matrices needed for Adagrad.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.mWhf, self.mWxf = np.zeros_like(self.Whf), np.zeros_like(self.Wxf)\n",
    "        self.mWhi, self.mWxi = np.zeros_like(self.Whi), np.zeros_like(self.Wxi)\n",
    "        self.mWhc, self.mWxc = np.zeros_like(self.Whc), np.zeros_like(self.Wxc)\n",
    "        self.mWho, self.mWxo = np.zeros_like(self.Who), np.zeros_like(self.Wxo)\n",
    "        self.mWhy = np.zeros_like(self.Why)\n",
    "        \n",
    "        self.mBf = np.zeros_like(self.Bf)\n",
    "        self.mBi = np.zeros_like(self.Bi)\n",
    "        self.mBc = np.zeros_like(self.Bc)\n",
    "        self.mBo = np.zeros_like(self.Bo)\n",
    "        self.mBy = np.zeros_like(self.By)\n",
    "        \n",
    "        return (self.mWhf, self.mWxf, self.mWhi, self.mWxi, self.mWhc, self.mWxc,\n",
    "                self.mWho, self.mWxo, self.mWhy, self.mBf, self.mBi, self.mBc, self.mBo, self.mBy)\n",
    "        \n",
    "    \n",
    "    def update_gradients(self, optimizer, lr):\n",
    "        \"\"\"\n",
    "        Update gradients based on the optimizer you have chosen.\n",
    "        \"\"\"\n",
    "        if optimizer == 'Adagrad':\n",
    "            if not hasattr(self, 'mWhf'):\n",
    "                self.mem_params = self.init_adagrad_memory()\n",
    "                \n",
    "            # perform parameter update with Adagrad\n",
    "            for param, dparam, mem in zip(self.model_params, self.gradient_params, self.mem_params):\n",
    "                mem += dparam * dparam\n",
    "                param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update\n",
    "    \n",
    "    def forward(self, xs, targets):\n",
    "        \"\"\"\n",
    "        Forward pass LSTM\n",
    "        \"\"\"\n",
    "        \n",
    "        y_preds = {}\n",
    "        \n",
    "        self.loss = 0\n",
    "        \n",
    "        for i in range(len(xs)):\n",
    "            # Vectorize the input\n",
    "            x = xs[i]\n",
    "            x_vec = np.zeros((self.vocab_sz, 1)) \n",
    "            x_vec[x] = 1\n",
    "            \n",
    "            \n",
    "            # Calculate new hidden and cell state\n",
    "            self.hf[i] = np.dot(self.Whf, self.hs[i - 1]) + np.dot(self.Wxf, x_vec) + self.Bf\n",
    "            self.hi[i] = np.dot(self.Whi, self.hs[i - 1]) + np.dot(self.Wxi, x_vec) + self.Bi\n",
    "            self.hc[i] = np.dot(self.Whc, self.hs[i - 1]) + np.dot(self.Wxc, x_vec) + self.Bc\n",
    "            self.ho[i] = np.dot(self.Who, self.hs[i - 1]) + np.dot(self.Wxo, x_vec) + self.Bo\n",
    "            \n",
    "            \n",
    "            f_t = sigmoid(self.hf[i])\n",
    "            i_t = sigmoid(self.hi[i])\n",
    "            cwave_t = np.tanh(self.hc[i])\n",
    "            o_t = sigmoid(self.ho[i])\n",
    "            \n",
    "            self.c_t[i] = f_t * self.c_t[i - 1] + i_t * cwave_t\n",
    "            self.hs[i] = o_t * np.tanh(self.c_t[i])\n",
    "            \n",
    "            # Predict y\n",
    "            y_preds[i] = np.dot(self.Why, self.hs[i]) + self.By\n",
    "            \n",
    "            self.sm_ps[i] = np.exp(y_preds[i]) / np.sum(np.exp(y_preds[i])) # Softmax probability\n",
    "            self.loss += -np.log(self.sm_ps[i][targets[i], 0]) # Negative loss likelyhood\n",
    "            \n",
    "        self.c_t[-1] = self.c_t[len(xs) - 1]\n",
    "        self.hs[-1] = self.hs[len(xs) - 1]\n",
    "        \n",
    "    def backward(self, xs, targets):\n",
    "        \"\"\"\n",
    "        Backward pass for LSTM\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize gradients\n",
    "        self.gradient_params= self.init_gradients()\n",
    "        \n",
    "        # Start with an empty next layer for the cell state and hidden state\n",
    "        dhnext = np.zeros_like(self.hs[0])\n",
    "        dcnext = np.zeros_like(self.c_t[0])\n",
    "        \n",
    "        # Loop over inputs and calculate gradients\n",
    "        for i in reversed(range(len(xs))):\n",
    "            # One hot encoding\n",
    "            x = xs[i]\n",
    "            x_vec = np.zeros((vocab_size, 1))\n",
    "            x_vec[x] = 1\n",
    "            \n",
    "            dy = np.copy(self.sm_ps[i])\n",
    "            dy[targets[i]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "            \n",
    "            self.dBy += dy\n",
    "            self.dWhy += np.dot(dy, self.hs[i].T)\n",
    "            # h branches to ouput, and next layer. Therefore, we need the gradient of next layer is added\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext \n",
    "            \n",
    "            # Calculations for o\n",
    "            do = dh * np.tanh(self.c_t[i]) # Weet dit niet zeker\n",
    "            do = dsigmoid(self.ho[i]) * do\n",
    "            \n",
    "            self.dWxo += np.dot(do, x_vec.T)\n",
    "            self.dWho += np.dot(do, self.hs[i-1].T)\n",
    "            self.dBo += do\n",
    "            \n",
    "            # Calculations for dc\n",
    "            dc= dh * sigmoid(self.ho[i]) \n",
    "            dc = (1-np.square(np.tanh(self.c_t[i]))) * dc #Weet dit ook niet zeker\n",
    "            dc = dc + dcnext\n",
    "            \n",
    "            # Calculation dcwave\n",
    "            dcwave_t = sigmoid(self.hi[i]) * dc\n",
    "            # C branches to next layer, therefore we need the gradient of that layer added.\n",
    "            dcwave_t = dcwave_t * (1-np.square(np.tanh(self.hc[i])))\n",
    "            \n",
    "            self.dWxc += np.dot(dcwave_t, x_vec.T)\n",
    "            self.dWhc += np.dot(dcwave_t, self.hs[i-1].T)\n",
    "            self.dBc += dcwave_t\n",
    "            \n",
    "            # Calculating di\n",
    "            di = sigmoid(self.hc[i]) * dc\n",
    "            di = di * dsigmoid(self.hi[i])\n",
    "            \n",
    "            self.dWxi += np.dot(di, x_vec.T)\n",
    "            self.dWhi += np.dot(di, self.hs[i-1].T)\n",
    "            self.dBi += di\n",
    "            \n",
    "            #Calculating df\n",
    "            df = self.c_t[i-1] * dc\n",
    "            df = dsigmoid(self.hf[i]) * df\n",
    "            \n",
    "            self.Wxf += np.dot(df, x_vec.T)\n",
    "            self.Whf += np.dot(df, self.hs[i-1].T)\n",
    "            self.dBf += df\n",
    "            \n",
    "        # Clip to prevent exploding gradients\n",
    "        for dparam in [self.dWhf, self.dWxf, self.dWhi, self.dWxi, self.dWhc, self.dWxc,\n",
    "                      self.dWho, self.dWxo, self.dWhy, self.dBf, self.dBi, self.dBo,\n",
    "                      self.dBc, self.dBy]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        \n",
    "    def reset_hidden(self):\n",
    "        \"\"\"\n",
    "        Reset hidden layer and cell state\n",
    "        \"\"\"\n",
    "        self.hs[-1] = np.zeros((self.hidden_sz, 1))\n",
    "        self.c_t[-1] = np.zeros((self.hidden_sz, 1))\n",
    "        \n",
    "    def plot_losses(self):\n",
    "        \"\"\"\n",
    "        Plot the cross entropy loss against the number of sequences\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'losses'):\n",
    "            plt.plot(self.losses)\n",
    "            plt.xlabel('Number of sequences')\n",
    "            plt.ylabel('Cross entropy loss')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('Error: No losses recorded, train the model!')\n",
    "        \n",
    "    def train(self, data, optimizer, lr, epochs, progress=True):\n",
    "        \"\"\"\n",
    "        Train the model by chopping the data in sequences followed by performing\n",
    "        the forward pass, backward pass and update the gradients.\n",
    "        \"\"\"\n",
    "        self.losses = []\n",
    "        smooth_loss = -np.log(1.0 / self.vocab_sz) * self.seq_length # Loss at iteration 0\n",
    "        \n",
    "        # Loop over the amount of epochs\n",
    "        for epoch in range(epochs):\n",
    "            n = 0\n",
    "            \n",
    "            # Reset hidden state\n",
    "            self.reset_hidden()\n",
    "            \n",
    "            data_len = len(data)\n",
    "            \n",
    "            # Loop over amount of sequences in the data\n",
    "            sequences_amount = int(data_len // self.seq_length)\n",
    "            for j in range(sequences_amount):\n",
    "                \n",
    "                start_pos = self.seq_length * j\n",
    "                \n",
    "                # Embed the inputs and targets\n",
    "                xs = [char_to_idx[ch] for ch in data[start_pos:start_pos + self.seq_length]]\n",
    "                targets = [char_to_idx[ch] for ch in data[start_pos + 1:start_pos + self.seq_length + 1]]\n",
    "                \n",
    "                # Forward pass\n",
    "                self.forward(xs, targets)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.backward(xs, targets)\n",
    "                \n",
    "                # Update weight matrices\n",
    "                self.update_gradients(optimizer, lr)\n",
    "                \n",
    "                smooth_loss = smooth_loss * 0.999 + self.loss * 0.001\n",
    "                \n",
    "                if progress and n % 1000 == 0:\n",
    "                    print(f'Epoch {epoch + 1}: {n} / {sequences_amount}: {smooth_loss}')\n",
    "                    \n",
    "                n += 1\n",
    "                self.losses.append(smooth_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(seq_length, hidden_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [char_to_idx[ch] for ch in data[0:5]]\n",
    "targets = [char_to_idx[ch] for ch in data[1:6]]\n",
    "model.forward(xs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward(xs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0 / 8075: 111.93342275553746\n",
      "Epoch 1: 1000 / 8075: 78.87434786491959\n",
      "Epoch 1: 2000 / 8075: 63.60220508307718\n",
      "Epoch 1: 3000 / 8075: 56.72167717225573\n",
      "Epoch 1: 4000 / 8075: 54.42674497125727\n",
      "Epoch 1: 5000 / 8075: 53.78307716013414\n",
      "Epoch 1: 6000 / 8075: 52.611541452596065\n",
      "Epoch 1: 7000 / 8075: 51.4032111706786\n",
      "Epoch 1: 8000 / 8075: 50.98773806950163\n",
      "Epoch 2: 0 / 8075: 51.009954472118764\n",
      "Epoch 2: 1000 / 8075: 50.17602319875799\n",
      "Epoch 2: 2000 / 8075: 49.740397490242074\n",
      "Epoch 2: 3000 / 8075: 49.230796527046756\n",
      "Epoch 2: 4000 / 8075: 49.386117724154296\n",
      "Epoch 2: 5000 / 8075: 49.74395610846519\n",
      "Epoch 2: 6000 / 8075: 49.318596310832845\n",
      "Epoch 2: 7000 / 8075: 48.70886498144336\n",
      "Epoch 2: 8000 / 8075: 48.64747357972019\n"
     ]
    }
   ],
   "source": [
    "model.train(data, 'Adagrad', learning_rate, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot_losses()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
